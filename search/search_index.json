{"docs":[{"location":"paradox.json","text":"","title":""},{"location":"index.html","text":"Ecclesiastical Latin IPA: /ˈʃi.o/, [ˈʃiː.o], [ˈʃi.i̯o] Verb: I can, know, understand, have knowledge.","title":"Scio"},{"location":"index.html#scio","text":"Scio is a Scala API for Apache Beam and Google Cloud Dataflow inspired by Apache Spark and Scalding.\nGetting Started is the best place to start with Scio. If you are new to Apache Beam and distributed data processing, check out the Beam Programming Guide first for a detailed explanation of the Beam programming model and concepts. If you have experience with other Scala data processing libraries, check out this comparison between Scio, Scalding and Spark.\nExample Scio pipelines and tests can be found under scio-examples. A lot of them are direct ports from Beam’s Java examples. See this page for some of them with side-by-side explanation. Also see Big Data Rosetta Code for common data processing code snippets in Scio, Scalding and Spark.\nSee Scio Scaladocs for current API documentation.","title":"Scio"},{"location":"index.html#getting-help","text":"","title":"Getting help"},{"location":"index.html#documentation","text":"Getting Started - current API documentation Scio, Scalding and Spark - comparison of these frameworks Runners - how Scio handles Beam runners and runner specific logic Scio data guideline - guideline for common problems Releases - Detailed release notes on new Scio releases FAQ - frequently asked questions","title":"Documentation"},{"location":"index.html#io","text":"Avro - using Scio with Avro files BigQuery - using Scio with BigQuery in a type safe way Bigtable - using Scio with Bigtable Parquet - using Scio with Parquet files Protobuf - using Scio with Protobuf","title":"IO"},{"location":"index.html#extras","text":"Algebird Sort Merge Bucket","title":"Extras"},{"location":"index.html#internals","text":"ScioIO - new IO system to simplify implementation and stubbing in JobTest OverrideTypeProvider - custom mappings for type-safe BigQuery Kryo - Kryo data serialization Coders - new Magnolia based Coders derivation","title":"Internals"},{"location":"index.html#further-readings","text":"Spotify Unwrapped: How We Brought You a Decade of Data Scio 0.7: a Deep Dive Big Data Processing at Spotify: The Road to Scio (Part 1) Big Data Processing at Spotify: The Road to Scio (Part 2) The world beyond batch: Streaming 101 The world beyond batch: Streaming 102 Dataflow/Beam & Spark: A Programming Model Comparison VLDB paper on the Dataflow Model","title":"Further Readings"},{"location":"index.html#presentations","text":"Scio in Depth - Apache Beam Summit, 2022 Techbytes: Data Processing with Scio - Spotify Engineering Talk, 2019 Techbytes: Handling Big Data at Spotify - Spotify Engineering Talk, 2019 Scio - Big Data on Google Cloud with Scala and Scio - Apache Beam Summit London 2018 Talk Sorry - How Bieber broke Google Cloud at Spotify (slides) - Scala Up North 2017 Talk Scio - Moving to Google Cloud A Spotify Story (slides) - Philly ETE 2017 Talk Scio - A Scala API for Google Cloud Dataflow & Apache Beam (slides) - Scala by the Bay 2016 Talk From stream to recommendation with Cloud Pub/Sub and Cloud Dataflow - GCP NEXT 16 Talk Apache Beam Presentation Materials","title":"Presentations"},{"location":"index.html#projects-using-or-related-to-scio","text":"Featran - A Scala feature transformation library for data science and machine learning Big Data Rosetta Code - Code snippets for solving common big data problems in various platforms. Inspired by Rosetta Code Ratatool - A tool for random data sampling and generation, which includes BigDiffy, a Scio library for pairwise field-level statistical diff of data sets (slides) Elitzur - Data validation for Scala and Scio Scio Koans - A collection of Scio exercises inspired by Ruby Koans and many others. scio-deep-dive - Building Scio from scratch step by step for an internal training session Klio - Large scale audio or binary file processing with Python and Apache Beam scala-flow - A lightweight Scala wrapper for Google Cloud Dataflow from Zendesk clj-headlights - Clojure API for Apache Beam, also from Zendesk datasplash - A Clojure API for Google Cloud Dataflow","title":"Projects using or related to Scio"},{"location":"Getting-Started.html","text":"","title":"Getting Started"},{"location":"Getting-Started.html#getting-started","text":"First install the Google Cloud SDK and create a Google Cloud Storage bucket for your project, e.g. gs://my-bucket. Make sure it’s in the same region as the BigQuery datasets you want to access and where you want Dataflow to launch workers on GCE.\nScio may need Google Cloud’s application default credentials for features like BigQuery. Run the following command to set it up.\ngcloud auth application-default login","title":"Getting Started"},{"location":"Getting-Started.html#building-scio","text":"Scio is built using SBT. To build Scio and publish artifacts locally, run:\ngit clone git@github.com:spotify/scio.git\ncd scio\n# 'sbt +publishLocal' to cross build for all Scala versions\n# 'sbt ++$SCALA_VERSION publishLocal' to build for a specific Scala version\nsbt publishLocal\nMore info on cross-building here.\nYou can also specify sbt heap size with -mem, e.g. sbt -mem 8192. Some sensible defaults are stored in .sbtopts.\nTo ensure the project loads and builds successfully, run the following sbt command so that all custom tasks are executed\nsbt compile Test/compile","title":"Building Scio"},{"location":"Getting-Started.html#running-the-examples","text":"You can execute the examples locally from SBT. By default, pipelines will be executed using the DirectRunner and local filesystem will be used for input and output. Take a look at the examples to find out more.\nneville@localhost scio $ sbt\n[info] ...\n> project scio-examples\n[info] ...\n> runMain com.spotify.scio.examples.WordCount --input=<FILE PATTERN> --output=<DIRECTORY>\nNote Unlike Hadoop, Scio or Dataflow input should be file patterns and not directories, i.e. gs://bucket/path/part-*.txt and not gs://bucket/path. Output on the other hand should be directories just like Hadoop, so gs://bucket/path will produce files like gs://bucket/path/part-00000-of-00005.txt.\nUse the DataflowRunner to execute pipelines on Google Cloud Dataflow service using managed resources in the Google Cloud Platform.\nneville@localhost scio $ sbt\n[info] ...\n> project scio-examples\n[info] ...\n> set beamRunners := \"DataflowRunner\"\n[info] ...\n> runMain com.spotify.scio.examples.WordCount\n--project=<PROJECT ID>\n--region=<GCE AVAILABILITY ZONE> --runner=DataflowRunner\n--input=<FILE PATTERN> --output=<DIRECTORY>\nThe Cloud Platform project refers to its name (not number). GCE availability region should be in the same region as the BigQuery datasets and GCS bucket.\nBy default, only DirectRunner is in the library dependencies list. Use set beamRunners := \"<runners>\" to specify additional runner dependencies as a comma separated list, i.e. “DataflowRunner,FlinkRunner”.","title":"Running the Examples"},{"location":"Getting-Started.html#sbt-project-setup","text":"To create a new SBT project using Giter8 scio-template, simply:\nsbt new spotify/scio-template.g8\nOr add the following to your build.sbt. Replace the direct and Dataflow runner with ones you wish to use. The compiler plugin dependency is only needed for the type safe BigQuery API with scala 2.12.\nlibraryDependencies ++= Seq(\n  \"com.spotify\" %% \"scio-core\" % scioVersion,\n  \"com.spotify\" %% \"scio-test\" % scioVersion % Test,\n  \"org.apache.beam\" % \"beam-runners-direct-java\" % beamVersion % Runtime,\n  \"org.apache.beam\" % \"beam-runners-google-cloud-dataflow-java\" % beamVersion % Runtime\n)\n\naddCompilerPlugin(\"org.scalamacros\" % \"paradise\" % \"2.1.1\" cross CrossVersion.full)","title":"SBT project setup"},{"location":"Getting-Started.html#bigquery-settings","text":"You may need a few extra settings to use BigQuery queries as pipeline input.\nsbt -Dbigquery.project=<PROJECT-ID>\nbigquery.project: GCP project to make BigQuery requests with at compile time. bigquery.secret: By default the credential in Google Cloud SDK will be used. A JSON secret file can be used instead with -Dbigquery.secret=secret.json.","title":"BigQuery Settings"},{"location":"Getting-Started.html#options","text":"The following options should be specified when running a job on Google Cloud Dataflow service.\n--project - The project ID for your Google Cloud Project. This is required if you want to run your pipeline using the Cloud Dataflow managed service. --region - The Compute Engine regional endpoint for launching worker instances to run your pipeline.\nFor pipeline execution parameters and optimization, see the following documents.\nSpecifying Pipeline Execution Parameters Service Optimization and Execution\nThe defaults should work well for most cases but we sometimes tune the following parameters manually. - --workerMachineType - start with smaller types like n1-standard-1 and go up if you run into memory problem. n1-standard-4 works well for a lot of our memory hungry jobs. - --maxNumWorkers - avoid setting it to too high, i.e. 1000 or close to quota, since that reduces available instances for other jobs and more workers means more expensive shuffle. - --diskSizeGb - increase this if you run into disk space problem during shuffle, or alternatively optimize code by replacing groupByKey with reduceByKey or sumByKey. - --workerDiskType - specify SSD for jobs with really expensive shuffles. See a list of disk types here. Also see this page about persistent disk size and type. - --network - specify this if you use VPN to communicate with external services, e.g. HDFS on an on-premise cluster.\nMore Dataflow pipeline specific options available can be found in DataflowPipelineOptions and super interfaces. Some more useful ones are from DataflowPipelineWorkerPoolOptions.\nDataflowWorkerHarnessOptions#getWorkerCacheMb affects side input performance but needs an extra step to enable. See this FAQ item.\nThere are a few more experimental settings that might help specific scenarios: - --experiments=shuffle_mode=service - use external shuffle service instead of local disk - --experiments=enable_custom_bigquery_sink - new custom sink that works around certain limitations when writing to BigQuery - --experiments=worker_region-<REGION> - use specified Google Cloud region instead of zone for more flexible capacity","title":"Options"},{"location":"Builtin.html","text":"","title":"Built-in Functionality"},{"location":"Builtin.html#built-in-functionality","text":"Scio is a thin wrapper on top of Beam offering idiomatic Scala APIs. Check out the Beam Programming Guide first for a detailed explanation of the Beam programming model and concepts.","title":"Built-in Functionality"},{"location":"Builtin.html#basics","text":"ScioContext wraps Beam’s Pipeline SCollection wraps Beam’s PCollection ScioResult wraps Beam’s PipelineResult\nSee dedicated sections on:\nIO Joins Side Inputs","title":"Basics"},{"location":"Builtin.html#core-functionality","text":"A ScioContext represents the pipeline and is the starting point for performing reads and the means by which the pipeline is executed. Execute a pipeline by invoking run and await completion by chaining waitUntilDone:\nimport com.spotify.scio._\nval sc: ScioContext = ???\nsc.run().waitUntilDone()\nSCollection is the representation of the data in a pipeline at a particular point in the execution graph preceding or following a transform. SCollections have many of the methods you would expect on a standard Scala collection: map, filter, flatten, flatMap, reduce, collect, fold, and take.\nAny SCollection of 2-tuples is considered a keyed SCollection and the various joins and *ByKey variants of other methods become available. The first item in the tuple is considered the key and the second item the value. The keyBy method creates a keyed SCollection, where the user-defined function extracts the key from the existing values:\nimport com.spotify.scio.values.SCollection\n\nval elements: SCollection[String] = ???\nval result: SCollection[(String, String)] = elements.keyBy(_.head.toString)\nOnce keyed, elements with the same key can be grouped so that they can be processed together:\nimport com.spotify.scio.values.SCollection\n\nval elements: SCollection[(String, String)] = ???\nval result: SCollection[(String, Iterable[String])] = elements.groupByKey\nDistinct elements can be found with distinct (or the distinctBy and distinctByKey variants):\nimport com.spotify.scio.values.SCollection\n\nval elements: SCollection[String] = ???\nval distinct: SCollection[String] = elements.distinct\nElements can be split into different SCollections with partition, which can be useful for error handling. Note that the number of partitions should be small.\nimport com.spotify.scio.values.SCollection\n\nval elements: SCollection[Int] = ???\nval (lessThanFive, greaterThanFive): (SCollection[Int], SCollection[Int]) = elements.partition(_ > 5)\nSCollections of the same type can be combined with a union (or unionAll) operation.\nimport com.spotify.scio.values.SCollection\n\nval a: SCollection[Int] = ???\nval b: SCollection[Int] = ???\nval elements: SCollection[Int] = a.union(b)\nElements can be printed to the console for inspection at any point of the graph by using debug:\nimport com.spotify.scio.values.SCollection\n\nval elements: SCollection[String] = ???\nelements.debug(prefix = \"myLabel: \")","title":"Core functionality"},{"location":"Builtin.html#contextandargs","text":"Scio’s ContextAndArgs provides a convenient way to both parse command-line options and acquire a ScioContext:\nimport com.spotify.scio._\n\nval cmdlineArgs: Array[String] = ???\nval (sc, args) = ContextAndArgs(cmdlineArgs)\nIf you need custom pipeline options, subclass Beam’s PipelineOptions and use ContextAndArgs.typed:\nimport com.spotify.scio._\nimport org.apache.beam.sdk.options.PipelineOptions\n\ntrait Arguments extends PipelineOptions {\n  def getMyArg: String\n  def setMyArg(input: String): Unit\n}\n\nval cmdlineArgs: Array[String] = ???\nval (sc, args) = ContextAndArgs.typed[Arguments](cmdlineArgs)\nval myArg: String = args.getMyArg","title":"ContextAndArgs"},{"location":"Builtin.html#aggregations","text":"Scio provides a suite of built-in aggregations. All *ByKey variants do the same as the normal function, but per-key for keyed SCollections.","title":"Aggregations"},{"location":"Builtin.html#counting","text":"count (or countByKey) counts the number of elements countByValue counts the number of elements for each value in a SCollection[T] countApproxDistinct (or countApproxDistinctByKey) estimates a distinct count, with Beam’s ApproximateUnique or Scio’s HyperLogLog-based ApproxDistinctCounter\nimport com.spotify.scio.values.SCollection\nimport com.spotify.scio.extra.hll.zetasketch.ZetaSketchHllPlusPlus\n\nval elements: SCollection[String] = ???\nval sketch = ZetaSketchHllPlusPlus[String]()\nval result: SCollection[Long] = elements.countApproxDistinct(sketch)","title":"Counting"},{"location":"Builtin.html#statistics","text":"max (or maxByKey) finds the maximum element given some Ordering min (or minByKey) finds the minimum element given some Ordering mean finds the mean given some Numeric quantilesApprox (or approxQuantilesByKey) finds the distribution using Beam’s ApproximateQuantiles\nFor SCollections containing Double, Scio additionally provides a stats method that computes the count, mean, min, max, variance, standard deviation, sample variance, and sample standard deviation over the SCollection. Convenience methods are available directly on the SCollection if only a single value is required:\nimport com.spotify.scio.values.SCollection\nimport com.spotify.scio.util.StatCounter\n\nval elements: SCollection[Double] = ???\n\nval stats: SCollection[StatCounter] = elements.stats\nval variance: SCollection[Double] = stats.map { s => s.variance }\n\nval stdev: SCollection[Double] = elements.stdev","title":"Statistics"},{"location":"Builtin.html#sums-combinations","text":"combine (or combineByKey) combines elements with a set of user-defined functions:\nimport com.spotify.scio.values.SCollection\n\ncase class A(count: Long, total: Long)\nobject A {\n  def apply(i: Int): A = A(1L, i.toLong)\n  def mergeValue(a: A, i: Int): A = A(a.count + 1L, a.total + i)\n  def mergeCombiners(a: A, b: A) = A(a.count + b.count, a.total + b.total)\n}\n\nval elements: SCollection[Int] = ???\nelements.combine(A.apply)(A.mergeValue)(A.mergeCombiners)\nsum (or sumByKey) sums elements given a Semigroup, while aggregate (or aggregateByKey) aggregates elements either with a set of user-defined functions, via a Aggregator, or via a MonoidAggregator.\nBoth Semigroup and Monoid instances can be derived with magnolify, assuming the behavior for the primitive types is what you expect.\nNote Note that for String the default Semigroup[String] behavior is to append, which is usually not what you want.\nFully-automatic derivation can be very concise but relies on some implicit magic:\nimport com.spotify.scio.values.SCollection\nimport com.twitter.algebird._\nimport magnolify.cats.auto._\n\ncase class A(count: Long, total: Long)\n\nval elements: SCollection[A] = ???\n\nval summed: SCollection[A] = elements.sum\nval aggregated: SCollection[A] = elements.aggregate(Aggregator.fromMonoid[A])\nSemi-automatic derivation in a companion object may be more intelligible:\ncase class A(count: Long, total: Long)\nobject A {\n  import magnolify.cats.semiauto._\n  import cats._\n  implicit val aMonoid: Monoid[A] = MonoidDerivation[A]\n}\nSee also Algebird","title":"Sums & combinations"},{"location":"Builtin.html#metrics","text":"Scio supports Beam’s Counter Distribution and Gauge.\nSee MetricsExample.","title":"Metrics"},{"location":"Builtin.html#scioresult","text":"ScioResult can be used to access metric values, individually or as a group:\nimport com.spotify.scio._\nimport org.apache.beam.sdk.metrics.{MetricName, Counter}\n\nval sc: ScioContext = ???\nval counter: Counter = ???\n\nval sr: ScioResult = sc.run().waitUntilDone()\nval counterValue: metrics.MetricValue[Long] = sr.counter(counter)\nval counterMap: Map[MetricName, metrics.MetricValue[Long]] = sr.allCounters","title":"ScioResult"},{"location":"Builtin.html#taps-materialization","text":"Writes return a ClosedTap, which provides an interface to access the written results or pass them to a subsequent Scio job.\nimport com.spotify.scio._\nimport com.spotify.scio.io.{Tap, ClosedTap}\nimport com.spotify.scio.values.SCollection\n\nval sc: ScioContext = ???\nval elements: SCollection[String] = ???\nval writeTap: ClosedTap[String] = elements.saveAsTextFile(\"gs://output-path\")\n\nval sr: ScioResult = sc.run().waitUntilDone()\n\nval textTap: Tap[String] = sr.tap(writeTap)\nval textContexts: Iterator[String] = textTap.value\n\nval sc2: ScioContext = ???\nval results: SCollection[String] = textTap.open(sc)\nThe same mechanism underlies Scio’s materialize method, which will save the contents of an SCollection at the point of the materialize to a temporary location and make them available after the pipeline completes:\nimport com.spotify.scio._\nimport com.spotify.scio.io.{Tap, ClosedTap}\nimport com.spotify.scio.values.SCollection\n\nval sc: ScioContext = ???\nval elements: SCollection[String] = ???\nval materializeTap: ClosedTap[String] = elements.materialize\n\nval sr: ScioResult = sc.run().waitUntilDone()\nval textTap: Tap[String] = sr.tap(materializeTap)\nSee also: WordCountOrchestration example.","title":"Taps & Materialization"},{"location":"Builtin.html#use-native-beam-functionality","text":"If there is a need to use a Beam IO or transform for which Scio does not have an API, you can easily use the native Beam API for single steps in a pipeline otherwise written in Scio.\ncustomInput supports reading from a Beam source; any transform of type PTransform[PBegin, PCollection[T]]:\nimport com.spotify.scio._\nimport com.spotify.scio.values.SCollection\nimport org.apache.beam.sdk.transforms.PTransform\nimport org.apache.beam.sdk.values.{PBegin, PCollection}\nimport org.apache.beam.sdk.io.TextIO\n\nval sc: ScioContext = ???\nval filePattern: String = ???\n\nval textRead: PTransform[PBegin, PCollection[String]] = TextIO.read().from(filePattern)\nval elements: SCollection[String] = sc.customInput(\"ReadText\", textRead)\nsaveAsCustomOutput supports writing to a Beam sink; any transform of type PTransform[PCollection[T], PDone]:\nimport com.spotify.scio.values.SCollection\nimport org.apache.beam.sdk.transforms.PTransform\nimport org.apache.beam.sdk.values.{PDone, PCollection}\nimport org.apache.beam.sdk.io.TextIO\n\nval outputLocation: String = ???\nval elements: SCollection[String] = ???\nval textWrite: PTransform[PCollection[String], PDone] = TextIO.write().to(outputLocation)\nelements.saveAsCustomOutput(\"WriteText\", textWrite)\nFinally, applyTransform supports using any Beam transform of type PTransform[PCollection[T], PCollection[U]]:\nimport com.spotify.scio.values.SCollection\nimport org.apache.beam.sdk.transforms.{PTransform, Sum}\nimport org.apache.beam.sdk.values.PCollection\nimport java.lang\n\nval elements: SCollection[Double] = ???\nval transform: PTransform[PCollection[lang.Double], PCollection[lang.Double]] = Sum.doublesGlobally\nval result: SCollection[lang.Double] = elements\n  .map(Double.box)\n  .applyTransform(transform)\nSee also: BeamExample","title":"Use native Beam functionality"},{"location":"Builtin.html#windowing","text":"timestampBy allows for changing an element’s timestamp:\nimport com.spotify.scio.values.SCollection\nimport org.joda.time.Instant\n\ncase class A(timestamp: Instant, value: String)\nval elements: SCollection[A] = ???\nval timestamped: SCollection[A] = elements.timestampBy(_.timestamp)\nThe withTimestamp, withWindow, and withPaneInfo functions flatten window metadata into the SCollection:\nimport com.spotify.scio.values.SCollection\nimport org.joda.time.Instant\n\nval elements: SCollection[String] = ???\nval timestamped: SCollection[(String, Instant)] = elements.withTimestamp\ntoWindowed converts the SCollection to a WindowedSCollection whose elements are all instances of WindowedValue, which gives full access to the windowing metadata:\nimport com.spotify.scio.values._\nimport org.joda.time.Instant\n\nval elements: SCollection[String] = ???\nval windowed: WindowedSCollection[String] = elements.toWindowed\nwindowed.map { v: WindowedValue[String] =>\n  v.withTimestamp(Instant.now())\n}\nScio provides convenience functions for the common types of windowing (withFixedWindows, withSlidingWindows, withSessionWindows, withGlobalWindow) but also provides full control over the windowing with withWindowFn.\nimport com.spotify.scio.values.SCollection\nimport org.joda.time.Duration\n\nval elements: SCollection[String] = ???\nval windowedElements: SCollection[String] = elements.withFixedWindows(Duration.standardHours(1))","title":"Windowing"},{"location":"Builtin.html#batching","text":"In cases where some transform performs better on a group of items, elements can be batched by number of elements with `batch`, by the size of the elements with `batchByteSized`, or by some user-defined weight with `batchWeighted`. There are also keyed variants of each of these: `batchByKey`, `batchByteSizedByKey`, and `batchWeightedByKey`.\nimport com.spotify.scio.values.SCollection\n\nval elements: SCollection[String] = ???\nval batchedElements: SCollection[Iterable[String]] = elements.batch(10)","title":"Batching"},{"location":"Builtin.html#misc","text":"Some elements of an SCollection can be randomly sampled using sample:\nimport com.spotify.scio.values.SCollection\n\nval elements: SCollection[String] = ???\nval result: SCollection[String] = elements.sample(withReplacement = true, fraction = 0.01)\nThe SCollection can be randomly split into new SCollections given a weighting of what fraction of the input should be in each split:\nimport com.spotify.scio.values.SCollection\n\nval elements: SCollection[Int] = ???\nval weights: Array[Double] = Array(0.2, 0.6, 0.2)\nval splits: Array[SCollection[Int]] = elements.randomSplit(weights)\nThe “top” n elements of an SCollection given some Ordering can be found with top:\nimport com.spotify.scio.values.SCollection\n\nval elements: SCollection[Int] = ???\nval top10: SCollection[Iterable[Int]] = elements.top(10)\nThe common elements of two SCollections can be found with intersection:\nimport com.spotify.scio.values.SCollection\n\nval a: SCollection[String] = ???\nval b: SCollection[String] = ???\nval common: SCollection[String] = a.intersection(b)\nFor a keyed SCollection, intersectByKey will give the elements in the LHS whose keys are in the RHS:\nimport com.spotify.scio.values.SCollection\n\nval a: SCollection[(String, Int)] = ???\nval b: SCollection[String] = ???\nval common: SCollection[(String, Int)] = a.intersectByKey(b)\nSimilarly, subtract (or subtractByKey) will give the elements in the LHS that are not present in the RHS:\nimport com.spotify.scio.values.SCollection\n\nval a: SCollection[String] = ???\nval b: SCollection[String] = ???\nval notInB: SCollection[String] = a.subtract(b)","title":"Misc"},{"location":"Joins.html","text":"","title":"Joins"},{"location":"Joins.html#joins","text":"Scio provides a full suite of join functionality and a few extras that solve tricky edge-cases in large-scale data processing.\nAll joins operate over SCollections containing 2-tuples, where the first tuple item is considered the key and the second the value. The order in which SCollections are joined matters; larger datasets should be further to the left. For example in a.join(b), a should be the larger of the two datasets and by convention a is called the left-hand-side or LHS, while b is the right-hand-side or RHS.","title":"Joins"},{"location":"Joins.html#cogroup","text":"The Beam transform which underlies the standard joins below is the Cogroup. Scio also provides a cogroup operation, which returns iterables from each SCollection containing all the values which match each key:\nimport com.spotify.scio.values.SCollection\n\nval a: SCollection[(String, String)] = ???\nval b: SCollection[(String, Int)] = ???\nval elements: SCollection[(String, (Iterable[String], Iterable[Int]))] = a.cogroup(b)","title":"Cogroup"},{"location":"Joins.html#standard-joins","text":"Scio’s standard joins have SQL-like names with SQL-like semantics. In the examples below, the contents of the LHS are of type (K, V), while the RHS are of type (K, W).\njoin produces elements of (K, (V, W)), where the key K must be in both the LHS and RHS:\nimport com.spotify.scio.values.SCollection\n\nval a: SCollection[(String, String)] = ???\nval b: SCollection[(String, Int)] = ???\nval elements: SCollection[(String, (String, Int))] = a.join(b)\nleftOuterJoin produces elements of (K (V, Option[W])), where the key K is in the LHS but may not be in the RHS:\nimport com.spotify.scio.values.SCollection\n\nval a: SCollection[(String, String)] = ???\nval b: SCollection[(String, Int)] = ???\nval elements: SCollection[(String, (String, Option[Int]))] = a.leftOuterJoin(b)\nrightOuterJoin produces elements of (K, (Option[V], W])), where the key K is in the RHS but may not be in the LHS:\nimport com.spotify.scio.values.SCollection\n\nval a: SCollection[(String, String)] = ???\nval b: SCollection[(String, Int)] = ???\nval elements: SCollection[(String, (Option[String], Int))] = a.rightOuterJoin(b)\nfullOuterJoin produces elements of (K (Option[V], Option[W])), where the key K can be in either side:\nimport com.spotify.scio.values.SCollection\n\nval a: SCollection[(String, String)] = ???\nval b: SCollection[(String, Int)] = ???\nval elements: SCollection[(String, (Option[String], Option[Int]))] = a.fullOuterJoin(b)\nWhen multiple joins of the same type are chained, it is more efficient to use Scio’s MultiJoin class. Instead of a.join(b).join(c) prefer MultiJoin (or its variants, MultiJoin.left, MultiJoin.outer, MultiJoin.cogroup):\nimport com.spotify.scio.values.SCollection\nimport com.spotify.scio.util.MultiJoin\n\nval a: SCollection[(String, Int)] = ???\nval b: SCollection[(String, Boolean)] = ???\nval c: SCollection[(String, Float)] = ???\nval elements: SCollection[(String, (Int, Boolean, Float))] = MultiJoin(a, b, c)","title":"Standard joins"},{"location":"Joins.html#hash-joins","text":"Scio’s hashJoin and variants hashLeftOuterJoin, and hashFullOuterJoin provide a convenient syntax over the top of Beam’s SideInput class to avoid shuffle during the join. The RHS should fit in memory, as with normal SideInputs.\nimport com.spotify.scio.values.SCollection\n\nval a: SCollection[(String, Int)] = ???\nval b: SCollection[(String, Boolean)] = ???\nval elements: SCollection[(String, (Int, Boolean))] = a.hashJoin(b)\nIn the less-common case where the LHS contains only keys to be looked-up, hashLookup will join in all matching values from the RHS. Again, the RHS should fit in memory.\nimport com.spotify.scio.values.SCollection\n\nval a: SCollection[String] = ???\nval b: SCollection[(String, String)] = ???\nval elements: SCollection[(String, Iterable[String])] = a.hashLookup(b)\nIn addition, Scio also provides the shuffle-free intersection and subtraction operations hashIntersectByKey and hashSubtractByKey.\nimport com.spotify.scio.values.SCollection\n\nval a: SCollection[(String, Int)] = ???\nval b: SCollection[String] = ???\n\nval subtracted: SCollection[(String, Int)] = a.hashSubtractByKey(b)\nval intersected: SCollection[(String, Int)] = a.hashIntersectByKey(b)","title":"Hash joins"},{"location":"Joins.html#large-hash-join","text":"Similar to Hash Joins, Scio’s largeHashJoin and variants largeHashLeftOuterJoin and largeHashFullOuterJoin provide a convenient syntax on top of Scio’s Sparkey support to avoid shuffle during a join. Use of sparkey requires only that the RHS fit on disk.\nimport com.spotify.scio.values.SCollection\nimport com.spotify.scio.extra.sparkey._\n\nval a: SCollection[(String, Int)] = ???\nval b: SCollection[(String, Boolean)] = ???\nval elements: SCollection[(String, (Int, Boolean))] = a.largeHashJoin(b)\nLarger shuffle-free intersection and subtraction operations are also provided as largeHashIntersectByKey and largeHashSubtractByKey.\nimport com.spotify.scio.values.SCollection\nimport com.spotify.scio.extra.sparkey._\n\nval a: SCollection[(String, Int)] = ???\nval b: SCollection[String] = ???\n\nval subtracted: SCollection[(String, Int)] = a.largeHashSubtractByKey(b)\nval intersected: SCollection[(String, Int)] = a.largeHashIntersectByKey(b)","title":"Large hash join"},{"location":"Joins.html#sparse-join","text":"Scio supports a ‘sparse join’ for cases where both the LHS and RHS of a join are large, but where the keys in the RHS cover a relatively small number of rows in the LHS.\nIn this case, an optimization of the join can significantly reduce the shuffle. The keys of the RHS are inserted into a Bloom Filter, a probabilistic data structure that effectively acts as a Set but with some risk of false positives. Elements in the LHS dataset are partitioned by passing an element’s key through the filter and splitting the dataset on whether the key is found or not. All LHS keys which are found in the filter are probably in the RHS dataset, so a full join is performed on these elements. Any LHS key not found in the filter are definitely not in the RHS dataset, so these items can be handled without a join. To properly size the Bloom filter, an estimate of the number of keys in the RHS (rhsNumKeys) must be provided to the join function.\nIn addition to sparseJoin (and variants sparseLeftOuterJoin, sparseRightOuterJoin, and sparseFullOuterJoin) Scio also provides a sparseIntersectByKey implementation. Scio uses Guava’s BloomFilter. Import magnolify.guava.auto._ to get common instances of Guava Funnel:\nimport com.spotify.scio.values.SCollection\nimport magnolify.guava.auto._\n\nval a: SCollection[(String, Int)] = ???\nval b: SCollection[(String, Boolean)] = ???\nval c: SCollection[String] = ???\n\nval bNumKeys: Int = ???\nval joined = a.sparseJoin(b, bNumKeys)\n\nval cNumKeys: Int = ???\nval intersected: SCollection[(String, Int)] = a.sparseIntersectByKey(c, cNumKeys)\nFinally, Scio provides sparseLookup, a special-case for joining all items from the RHS with matching keys into the LHS items with that key. Differently than the other sparse variants, in this case an estimate of the number of keys in the LHS must be provided:\nimport com.spotify.scio.values.SCollection\nimport magnolify.guava.auto._\n\nval a: SCollection[(String, Int)] = ???\nval b: SCollection[(String, String)] = ???\n\nval aNumKeys: Int = ???\nval lookedUp: SCollection[(String, (Int, Iterable[String]))] = a.sparseLookup(b, aNumKeys)","title":"Sparse join"},{"location":"Joins.html#skewed-join","text":"Similar to sparse joins, Scio supports a ‘skewed join’ for the special case in which some keys in a dataset are very frequent, or hot.\nScio uses a Count-Min sketch (or CMS), a probabilistic data structure that is internally similar to a Bloom filter, but which provides an estimated count for a given item which is explicitly an _over_estimate. The keys of the LHS are counted and those which exceed the value of the hotKeyThreshold parameter (default: 9000) plus an error bound are considered ‘hot’, while any remaining key is ‘cold’. Both the LHS and RHS are divided into ‘hot’ and ‘chill’ partitions. The chill sides are joined normally, while the hot side of the RHS is hashJoined into the hot LHS, avoiding shuffle on this segment of the dataset.\nScio provides skewedJoin, skewedLeftOuterJoin, and skewedFullOuterJoin variants. Import com.twitter.algebird.CMSHasherImplicits._ for the implicits required for count-min sketch.\nimport com.spotify.scio.values.SCollection\nimport com.twitter.algebird.CMSHasherImplicits._\n\nval a: SCollection[(String, Int)] = ???\nval b: SCollection[(String, String)] = ???\nval elements: SCollection[(String, (Int, String))] = a.skewedJoin(b)","title":"Skewed Join"},{"location":"Joins.html#sort-merge-bucket-smb-","text":"Sort-Merge Buckets allow for shuffle-free joins of large datasets. See Sort Merge Bucket","title":"Sort Merge Bucket (SMB)"},{"location":"Joins.html#see-also","text":"Join Optimizations at Spotify How Scio can save you time and money with clever join strategies and approximate algorithms, Apache Beam Summit 2022","title":"See also"},{"location":"SideInputs.html","text":"","title":"Side Inputs"},{"location":"SideInputs.html#side-inputs","text":"Side inputs provide a way to broadcast small amounts of data to all workers.\nSide inputs are more performant if they fit entirely into memory. We therefore recommend using the singleton variants if possible, and setting the –workerCacheMb option. For a dataflow job on a standard worker we recommend a maximum size of roughly 1GB for a side input. If you have a need for a larger side input, see the section on Sparkey side inputs.\nSee also the Beam Programming Guide’s section on Side Inputs which provides some additional details.","title":"Side Inputs"},{"location":"SideInputs.html#standard-side-inputs","text":"Converting an SCollection to a side-input Seq or Iterable is supported via asListSideInput and asIterableSideInput respectively:\nimport com.spotify.scio.values.{SCollection, SideInput}\n\nval stringElements: SCollection[String] = ???\nval stringListSI: SideInput[Seq[String]] = stringElements.asListSideInput\nval stringIterSI: SideInput[Iterable[String]] = stringElements.asIterableSideInput\nFor keyed SCollections, Scio provides asMapSideInput for when there is a unique key-value relationship and asMultiMapSideInput for when a key may have multiple values:\nimport com.spotify.scio.values.{SCollection, SideInput}\n\nval keyedElements: SCollection[(String, String)] = ???\nval mapSingleSI: SideInput[Map[String, String]] = keyedElements.asMapSideInput\nval mapMultiSI: SideInput[Map[String, Iterable[String]]] = keyedElements.asMultiMapSideInput","title":"Standard side inputs"},{"location":"SideInputs.html#singleton-variants","text":"In addition to standard Beam SideInputs, Scio also provides Singleton variants that are often more performant than the Beam defaults.\nFor SCollections with a single element, asSingletonSideInput will convert it to a side input:\nimport com.spotify.scio.values.{SCollection, SideInput}\n\nval elements: SCollection[Int] = ???\nval sumSI: SideInput[Int] = elements.sum.asSingletonSideInput\nTo get an SideInput of Set[T], use asSetSingletonSideInput:\nimport com.spotify.scio.values.{SCollection, SideInput}\n\nval elements: SCollection[String] = ???\nval setSI: SideInput[Set[String]] = elements.asSetSingletonSideInput\nFor keyed SCollections, asMapSingletonSideInput for when there is a unique key-value relationship and asMultiMapSingletonSideInput for when a key may have multiple values:\nimport com.spotify.scio.values.{SCollection, SideInput}\n\nval keyedElements: SCollection[(String, String)] = ???\nval mapSingleSI: SideInput[Map[String, String]] = keyedElements.asMapSingletonSideInput\nval mapMultiSI: SideInput[Map[String, Iterable[String]]] = keyedElements.asMultiMapSingletonSideInput","title":"Singleton variants"},{"location":"SideInputs.html#side-input-context","text":"To ‘join’ a SideInput, use withSideInputs, then access it via the SideInputContext:\nimport com.spotify.scio.values.{SCollection, SideInput}\n\nval keyedElements: SCollection[(String, String)] = ???\nval mapSingleSI: SideInput[Map[String, String]] = keyedElements.asMapSingletonSideInput\n\nval elements: SCollection[String] = ???\nelements\n  .withSideInputs(mapSingleSI)\n  .map { case (element, ctx) =>\n    val mapSingle: Map[String, String] = ctx(mapSingleSI)\n    val value: Option[String] = mapSingle.get(element)\n    value\n  }","title":"Side input context"},{"location":"SideInputs.html#workercachemb-option","text":"By default, Dataflow workers allocate 100MB (see DataflowWorkerHarnessOptions#getWorkerCacheMb) of memory for caching side inputs, and falls back to disk or network. Jobs with large side inputs may therefore be slow. To override this default, register DataflowWorkerHarnessOptions before parsing command line arguments and then pass --workerCacheMb=N when submitting the job:\nimport com.spotify.scio._\nimport org.apache.beam.sdk.options.PipelineOptionsFactory\nimport org.apache.beam.runners.dataflow.options.DataflowWorkerHarnessOptions\n\ndef main(cmdlineArgs: Array[String]): Unit = {\n  PipelineOptionsFactory.register(classOf[DataflowWorkerHarnessOptions])\n  val (sc, args) = ContextAndArgs(cmdlineArgs)\n  ???\n}","title":"workerCacheMb option"},{"location":"io/index.html","text":"","title":"IO"},{"location":"io/index.html#io","text":"Avro Binary BigQuery Bigtable Cassandra CSV Datastore GRPC Elasticsearch JDBC Json Neo4J Object file Parquet Protobuf PubSub ReadFiles Redis Spanner Tensorflow Text","title":"IO"},{"location":"io/Avro.html","text":"","title":"Avro"},{"location":"io/Avro.html#avro","text":"","title":"Avro"},{"location":"io/Avro.html#read-avro-files","text":"Scio comes with support for reading Avro files. Avro supports generic or specific records, Scio supports both via the same method (avroFile), but depending on the type parameter.","title":"Read Avro files"},{"location":"io/Avro.html#read-specific-records","text":"import com.spotify.scio.ScioContext\nimport com.spotify.scio.avro._\n\nimport org.apache.avro.specific.SpecificRecord\n\nval sc: ScioContext = ???\n\n// SpecificRecordClass is compiled from Avro schema files\ndef result = sc.avroFile[SpecificRecord](\"gs://path-to-data/lake/part-*.avro\")","title":"Read Specific records"},{"location":"io/Avro.html#read-generic-records","text":"import com.spotify.scio.ScioContext\nimport com.spotify.scio.avro._\n\nimport org.apache.avro.generic.GenericRecord\nimport org.apache.avro.Schema\n\ndef yourAvroSchema: Schema = ???\n\nval sc: ScioContext = ???\n\ndef result = sc.avroFile(\"gs://path-to-data/lake/part-*.avro\", yourAvroSchema)\n// `record` is of GenericRecord type","title":"Read Generic records"},{"location":"io/Avro.html#write-avro-files","text":"Scio comes with support for writing Avro files. Avro supports generic or specific records, Scio supports both via the same method (saveAsAvroFile), but depending on the type of the content of SCollection.","title":"Write Avro files"},{"location":"io/Avro.html#write-specific-records","text":"import com.spotify.scio.values.SCollection\nimport com.spotify.scio.avro._\n\nimport org.apache.avro.specific.SpecificRecord\n\ncase class Foo(x: Int, s: String)\nval sc: SCollection[Foo] = ???\n\n// convert to avro SpecificRecord\ndef fn(f: Foo): SpecificRecord = ???\n\n// type of Avro specific records will hold information about schema,\n// therefor Scio will figure out the schema by itself\n\ndef result =  sc.map(fn).saveAsAvroFile(\"gs://path-to-data/lake/output\")","title":"Write Specific records"},{"location":"io/Avro.html#write-generic-records","text":"import com.spotify.scio.values.SCollection\nimport com.spotify.scio.coders.Coder\nimport com.spotify.scio.avro._\n\nimport org.apache.avro.generic.GenericRecord\nimport org.apache.avro.Schema\n\ncase class Foo(x: Int, s: String)\nval sc: SCollection[Foo] = ???\n\nlazy val yourAvroSchema: Schema = ???\nimplicit lazy val coder: Coder[GenericRecord] = avroGenericRecordCoder(yourAvroSchema)\n\n// convert to avro GenericRecord\ndef fn(f: Foo): GenericRecord = ???\n\n// writing Avro generic records requires additional argument `schema`\ndef result =  sc.map(fn).saveAsAvroFile(\"gs://path-to-data/lake/output\", schema = yourAvroSchema)","title":"Write Generic records"},{"location":"io/Avro.html#rules-for-schema-evolution","text":"Unless impossible, provide default values for your fields. New field must have a default value. You can only delete field which has default value. Do not change the data type of existing fields. If needed, add a new field to the schema. Do not rename existing fields. If needed, use aliases.","title":"Rules for schema evolution"},{"location":"io/Avro.html#common-issues-guidelines","text":"Follow Avro guidelines, especially the one about schema evolution Wherever possible use specific records Use Builder pattern to construct Avro records","title":"Common issues/guidelines"},{"location":"io/Binary.html","text":"","title":"Binary"},{"location":"io/Binary.html#binary","text":"","title":"Binary"},{"location":"io/Binary.html#read-binary-files","text":"See read entire file as binary for reading an entire file as a single binary record.\nBinary reads are supported via the binaryFile, with a BinaryFileReader instance provided that can parse the underlying binary file format.\nimport com.spotify.scio.ScioContext\nimport com.spotify.scio.io.BinaryIO.BinaryFileReader\n\nval sc: ScioContext = ???\nval myBinaryFileReader: BinaryFileReader = ???\nsc.binaryFile(\"gs://<input-dir>\", myBinaryFileReader)\nThe complexity of the reader is determined by the complexity of the input format. See BinaryInOut for a fully-worked example.","title":"Read Binary files"},{"location":"io/Binary.html#write-binary-files","text":"Binary writes are supported on SCollection[Array[Byte]] with the saveAsBinaryFile method:\nimport com.spotify.scio.values.SCollection\n\nval byteArrays: SCollection[Array[Byte]] = ???\nbyteArrays.saveAsBinaryFile(\"gs://<output-dir>\")\nA static header and footer argument are provided, along with the framing parameters framePrefix and frameSuffix. In this example, we record a magic number in the header along with the number of records in the file and a magic number in the footer.\nimport com.spotify.scio.values.SCollection\nimport java.nio.ByteBuffer\n\ndef intToPaddedArray(i: Int) = ByteBuffer.allocate(4).putInt(i).array()\n\nval byteArrays: SCollection[Array[Byte]] = ???\nbyteArrays.saveAsBinaryFile(\n  \"gs://<output-dir>\",\n  header = Array(1, 2, 3),\n  footer = Array(4, 5, 6),\n  framePrefix = arr => intToPaddedArray(arr.length),\n  frameSuffix = _ => Array(0)\n)\nSee also the object file format, which saves binary data in an avro container.","title":"Write Binary files"},{"location":"io/BigQuery.html","text":"","title":"BigQuery"},{"location":"io/BigQuery.html#bigquery","text":"","title":"BigQuery"},{"location":"io/BigQuery.html#background","text":"NOTE that there are currently two BigQuery dialects, the legacy query syntax and the new SQL 2011 standard. The SQL standard is highly recommended since it generates dry-run schemas consistent with actual result and eliminates a lot of edge cases when working with records in a type-safe manner. To use standard SQL, prefix your query with #standardsql.","title":"Background"},{"location":"io/BigQuery.html#tablerow","text":"BigQuery rows are represented as TableRow in the BigQuery Java API which is basically a Map<String, Object>. Fields are accessed by name strings and values must be cast or converted to the desired type, both of which are error prone process.","title":"TableRow"},{"location":"io/BigQuery.html#type-safe-bigquery","text":"The type safe BigQuery API in Scio represents rows as case classes and generates TableSchema converters automatically at compile time with the following mapping logic:\nNullable fields are mapped to Option[T]s Repeated fields are mapped to List[T]s Records are mapped to nested case classes Timestamps are mapped to Joda Time Instant\nSee documentation for BigQueryType for the complete list of supported types.","title":"Type safe BigQuery"},{"location":"io/BigQuery.html#type-annotations","text":"There are 5 annotations for type safe code generation.","title":"Type annotations"},{"location":"io/BigQuery.html#bigquerytype-fromstorage","text":"This expands a class with output fields from a BigQuery Storage API read. Note that class Row has no body definition and is expanded by the annotation at compile time based on actual table schema.\nStorage API provides fast access to BigQuery-managed storage by using an rpc-based protocol. It is preferred over @BigQueryType.fromTable and @bigQueryType.fromQuery. For comparison:\nfromTable exports the entire table to Avro files on GCS and reads from them. This incurs export cost and export quota. It can also be wasteful if only a fraction of the columns/rows are needed. fromQuery executes the query and saves result as a temporary table before reading it like fromTable. This incurs both query and export cost plus export quota. fromStorage accesses the underlying BigQuery storage directly, reading only columns and rows based on selectedFields and rowRestriction. No query, export cost or quota hit.\nimport com.spotify.scio.bigquery.types.BigQueryType\n\n@BigQueryType.fromStorage(\n    \"bigquery-public-data:samples.gsod\",\n    selectedFields = List(\"tornado\", \"month\"),\n    rowRestriction = \"tornado = true\"\n  )\n  class Row","title":"BigQueryType.fromStorage"},{"location":"io/BigQuery.html#bigquerytype-fromtable","text":"This expands a class with fields that map to a BigQuery table.\nimport com.spotify.scio.bigquery.types.BigQueryType\n\n@BigQueryType.fromTable(\"bigquery-public-data:samples.gsod\")\nclass Row","title":"BigQueryType.fromTable"},{"location":"io/BigQuery.html#bigquerytype-fromquery","text":"This expands a class with output fields from a SELECT query. A dry run is executed at compile time to get output schema and does not incur additional cost.\nimport com.spotify.scio.bigquery.types.BigQueryType\n\n@BigQueryType.fromQuery(\"SELECT tornado, month FROM [bigquery-public-data:samples.gsod]\")\nclass Row\nThe query string may also contain \"%s\"s and additional arguments for parameterized query. This could be handy for log type data.\nimport com.spotify.scio.bigquery.types.BigQueryType\n\n// generate schema at compile time from a specific date\n@BigQueryType.fromQuery(\"SELECT user, url FROM [my-project:logs.log_%s]\", \"20160101\")\nclass Row\n\n// generate new query strings at runtime\nval newQuery = Row.query(args(0))\nThere’s also a $LATEST placeholder for table partitions. The latest common partition for all tables with the placeholder will be used.\nimport com.spotify.scio.bigquery.types.BigQueryType\n\n// generate schema at compile time from the latest date available in both my-project:log1.log_* and my-project:log2.log_*\n@BigQueryType.fromQuery(\n  \"SELECT user, url, action FROM [my-project:log1.log_%s] JOIN [my-project:log2.log_%s] USING user\",\n  \"$LATEST\", \"$LATEST\")\nclass Row\n\n// generate new query strings at runtime\nval newQuery = Row.query(args(0), args(0))","title":"BigQueryType.fromQuery"},{"location":"io/BigQuery.html#bigquerytype-fromschema","text":"This annotation gets schema from a string parameter and is useful in tests.\nimport com.spotify.scio.bigquery.types.BigQueryType\n\n@BigQueryType.fromSchema(\n  \"\"\"\n    |{\n    |  \"fields\": [\n    |    {\"mode\": \"REQUIRED\", \"name\": \"f1\", \"type\": \"INTEGER\"},\n    |    {\"mode\": \"REQUIRED\", \"name\": \"f2\", \"type\": \"FLOAT\"},\n    |    {\"mode\": \"REQUIRED\", \"name\": \"f3\", \"type\": \"BOOLEAN\"},\n    |    {\"mode\": \"REQUIRED\", \"name\": \"f4\", \"type\": \"STRING\"},\n    |    {\"mode\": \"REQUIRED\", \"name\": \"f5\", \"type\": \"TIMESTAMP\"}\n    |    ]\n    |}\n  \"\"\".stripMargin)\nclass Row","title":"BigQueryType.fromSchema"},{"location":"io/BigQuery.html#bigquerytype-totable","text":"This annotation works the other way around. Instead of generating class definition from a BigQuery schema, it generates BigQuery schema from a case class definition.\nimport com.spotify.scio.bigquery.types.BigQueryType\n\n@BigQueryType.toTable\ncase class Result(user: String, url: String, time: Long)\nFields in the case class and the class itself can also be annotated with @description which propagates to BigQuery schema.\nimport com.spotify.scio.bigquery.types.BigQueryType\nimport com.spotify.scio.bigquery.description\n\n@BigQueryType.toTable\n@description(\"A list of users mapped to the urls they visited\")\ncase class Result(user: String,\n                  url: String,\n                  @description(\"Milliseconds since Unix epoch\") time: Long)","title":"BigQueryType.toTable"},{"location":"io/BigQuery.html#annotation-parameters","text":"Note that due to the nature of Scala macros, only string literals and multi-line strings with optional .stripMargin are allowed as parameters to BigQueryType.fromTable, BigQueryType.fromQuery and BigQueryType.fromSchema.\nThese are OK:\nimport com.spotify.scio.bigquery.types.BigQueryType\n\n@BigQueryType.fromTable(\"project-id:dataset-id.table-id\")\nclass Row1\n\n@BigQueryType.fromQuery(\n  \"\"\"\n    |SELECT user, url\n    |FROM [project-id:dataset-id.table-id]\n  \"\"\".stripMargin)\nclass Row2\nAnd these are not:\nval args: Array[String] = Array(\"\", \"*\", \"[project-id:dataset-id.table-id]\")\nimport com.spotify.scio.bigquery.types.BigQueryType\n\n@BigQueryType.fromQuery(\"SELECT \" + args(1) + \" FROM [\" + args(2) + \"]\")\nclass Row1\n\nval sql = \"SELECT \" + args(1) + \" FROM [\" + args(2) + \"]\"\n@BigQueryType.fromQuery(sql)\nclass Row2","title":"Annotation parameters"},{"location":"io/BigQuery.html#companion-objects","text":"Classes annotated with the type safe BigQuery API have a few more convenience methods.\nschema: TableSchema - BigQuery schema fromTableRow: (TableRow => T) - TableRow to case class converter toTableRow: (T => TableRow) - case class to TableRow converter toPrettyString(indent: Int = 0) - pretty string representation of the schema\nimport com.spotify.scio.bigquery.types.BigQueryType\n\n@BigQueryType.fromTable(\"bigquery-public-data:samples.gsod\")\nclass Row\n\nRow.toPrettyString(2)\nIn addition, BigQueryType.fromTable and BigQueryTable.fromQuery generate table: String and query: String respectively that refers to parameters in the original annotation.\nimport com.spotify.scio.bigquery.types.BigQueryTypeUser defined companion objects may interfere with macro code generation so for now do not provide one to a case class annotated with @BigQueryType.toTable, i.e. object Row.","title":"Companion objects"},{"location":"io/BigQuery.html#using-type-safe-bigquery","text":"","title":"Using type safe BigQuery"},{"location":"io/BigQuery.html#type-safe-bigquery-with-scio","text":"To enable type safe BigQuery for ScioContext:\nimport com.spotify.scio._\nimport com.spotify.scio.bigquery._\nimport com.spotify.scio.bigquery.types.BigQueryType\n\n@BigQueryType.fromQuery(\"SELECT tornado, month FROM [bigquery-public-data:samples.gsod]\")\nclass Row\n\n@BigQueryType.toTable\ncase class Result(month: Long, tornado_count: Long)\n\ndef main(cmdlineArgs: Array[String]): Unit = {\n  val (sc, args) = ContextAndArgs(cmdlineArgs)\n  sc.typedBigQuery[Row]()  // query string from Row.query\n    .flatMap(r => if (r.tornado.getOrElse(false)) Seq(r.month) else Nil)\n    .countByValue\n    .map(kv => Result(kv._1, kv._2))\n    .saveAsTypedBigQueryTable(Table.Spec(args(\"output\")))  // schema from Row.schema\n  sc.run()\n  ()\n}","title":"Type safe BigQuery with Scio"},{"location":"io/BigQuery.html#type-safe-bigqueryclient","text":"Annotated classes can be used with the BigQueryClient directly too.\nimport com.spotify.scio.bigquery.types.BigQueryType\nimport com.spotify.scio.bigquery.client.BigQuery\n\n@BigQueryType.fromQuery(\"SELECT tornado, month FROM [bigquery-public-data:samples.gsod]\")\nclass Row\n\ndef bq = BigQuery.defaultInstance()\ndef rows = bq.getTypedRows[Row]()\ndef result = bq.writeTypedRows(\"project-id:dataset-id.table-id\", rows.toList)","title":"Type safe BigQueryClient"},{"location":"io/BigQuery.html#using-type-safe-bigquery-directly-with-beams-io-library","text":"If there are any BigQuery I/O operations supported in the Apache Beam client but not exposed in Scio, you may choose to use the Beam transform directly using Scio’s .saveAsCustomOutput() option:\nimport com.spotify.scio.values.SCollection\nimport com.spotify.scio.bigquery.types.BigQueryType\nimport org.apache.beam.sdk.io.gcp.bigquery.BigQueryIO\n\n@BigQueryType.fromQuery(\"SELECT tornado, month FROM [bigquery-public-data:samples.gsod]\")\nclass Foo\n\ndef bigQueryType = BigQueryType[Foo]\ndef tableRows: SCollection[Foo] = ???\n\ndef result =\n  tableRows\n    .map(bigQueryType.toTableRow)\n    .saveAsCustomOutput(\n      \"custom bigquery IO\",\n      BigQueryIO\n        .writeTableRows()\n        .to(\"my-project:my-dataset.my-table\")\n        .withSchema(bigQueryType.schema)\n        .withCreateDisposition(???)\n        .withWriteDisposition(???)\n        .withFailedInsertRetryPolicy(???)\n      )","title":"Using type safe BigQuery directly with Beam’s IO library"},{"location":"io/BigQuery.html#bigquerytype-and-intellij-idea","text":"See the FAQ for making IntelliJ happy with type safe BigQuery.","title":"BigQueryType and IntelliJ IDEA"},{"location":"io/BigQuery.html#custom-types-and-validation","text":"See OverrideTypeProvider for details about the custom types and validation mechanism.","title":"Custom types and validation"},{"location":"io/BigQuery.html#bigquery-authentication","text":"BigQuery authentication works a bit differently than other IO types and can be hard to reason about. File-based IOs, for example, are read from directly on each remote worker node. In contrast, for BigQuery reads, Scio will actually launch a Bigquery export job from the main class process before submitting a Dataflow job request. This export job extracts the requested BQ data to a temporary GCS location, from which the job workers can read directly from. Thus, your launcher code must be credentialed with the required permissions to export data.\nThis credential will be picked up from the values of bigquery.project and bigquery.secret, if set. If they are not, Scio will attempt to find an active Application Default Credential and set the billing project to the value from DefaultProjectFactory. As of Scio 0.11.6, you can set the SBT option bigquery.debug_auth=true, which enables Scio to log the active credential used in BigQuery queries that return a 403 FORBIDDEN status.\nA service account impersonation is available using SBT option bigquery.act_as=service-account@my-project.iam.gserviceaccount.com. It requires roles/iam.serviceAccountTokenCreator to be granted to a source account.\nNote that BigQuery Storage APIs don’t require an export job as they can read from BigQuery directly.","title":"BigQuery authentication"},{"location":"io/BigQuery.html#bigquery-configurations-in-sbt","text":"Scio offers several BigQuery options that can be configured as SBT options - either in a root-level .sbtopts file or in your SBT process as sbt -D{$OPT_KEY}=${OPT_VALUE} ...:\nOption Description bigquery.project Specifies the billing project to use for queries. Defaults to the default project associated with the active GCP configuration (see DefaultProjectFactory). bigquery.secret Specifies a file path containing a BigQuery credential. Defaults to the Application Default Credential. bigquery.connect_timeout Timeout in milliseconds to establish a connection. Default is 20000 (20 seconds). 0 for an infinite timeout. bigquery.read_timeout Timeout in milliseconds to read data from an established connection. Default is 20000 (20 seconds). 0 for an infinite timeout. bigquery.priority Determines whether queries are executed in “BATCH” or “INTERACTIVE” mode. Default: BATCH. bigquery.debug_auth Enables logging active BigQuery user information on auth errors. Default: false. bigquery.types.debug Enables verbose logging of macro generation steps. Default: false. bigquery.cache.enabled Enables scio bigquery caching. Default: true. generated.class.cache.directory BigQuery generated class cache directory. Defaults to a directory in java.io.tmpdir. bigquery.cache.directory BigQuery local schema cache directory. Defaults to a directory in java.io.tmpdir. bigquery.plugin.disable.dump Disable macro class dump. Default: false. bigquery.act_as A target SA principal to impersonate current auth. Optional. bigquery.act_as_lifetime A duration in seconds of a target SA temporary credentials lifetime. Default: 3600.","title":"BigQuery configurations in SBT"},{"location":"io/Bigtable.html","text":"","title":"Bigtable"},{"location":"io/Bigtable.html#bigtable","text":"First please read Google’s official doc.","title":"Bigtable"},{"location":"io/Bigtable.html#bigtable-example","text":"This depends on APIs from scio-bigtable and imports from com.spotify.scio.bigtable._.\nLook at example here.","title":"Bigtable example"},{"location":"io/Bigtable.html#common-issues","text":"","title":"Common issues"},{"location":"io/Bigtable.html#size-of-the-cluster-vs-dataflow-cluster","text":"As a general note when writing to Bigtable from Dataflow you should at most use the # of Bigtable nodes you have * 3 cpus. Otherwise Bigtable will be overwhelmed and throttle the writes (and the reads)","title":"Size of the cluster vs Dataflow cluster"},{"location":"io/Bigtable.html#cell-compression","text":"Bigtable doesn’t compress cell values > 1Mb","title":"Cell compression"},{"location":"io/Bigtable.html#jetty-alpn-npn-has-not-been-properly-configured","text":"Check that your versions of grpc-netty, netty-handler, and netty-tcnative-boringssl-static are compatible.","title":"Jetty ALPN/NPN has not been properly configured"},{"location":"io/Bigtable.html#bigtableio","text":"The BigtableIO included in the Dataflow SDK is not recommended for use. It is not written by the Bigtable team and is significantly less performant than the HBase Bigtable Dataflow connector. Please see the example above for the recommended API.","title":"BigtableIO"},{"location":"io/Bigtable.html#key-structure","text":"Your row key should not contain common parts at the beginning of the key, doing so would overload specific Bigtable nodes. For example, if your row is identifiable by user-id and date key - do NOT use date,user-id, instead use user-id,date or even better in case of date use Bigtable version/timestamp. Read more about row key design over here.","title":"Key structure"},{"location":"io/Bigtable.html#performance","text":"Read Google doc.","title":"Performance"},{"location":"io/Bigtable.html#bigtable-vs-datastore","text":"If you require replacement for Cassandra, Bigtable is probable the most straightforward replacement in GCP. Bigtable white paper. To quote the paper - think of Bigtable as:\na sparse, distributed, persistent multidimensional sorted map. The map is indexed by a row key, column key, and a timestamp; each value in the map is an uninterpreted array of bytes.\nBigtable is replicated only within a single zone. Bigtable does not support transactions, that said all operations are atomic at the row level.\nThink of Datastore as distributed, persistent, fully managed key-value store, with support for transactions. Datastore is replicated across multiple datacenters thus making it theoretically more available than Bigtable (as of today).\nRead more about Bigtable here, and more about Datastore over here.","title":"Bigtable vs Datastore"},{"location":"io/Cassandra.html","text":"","title":"Cassandra"},{"location":"io/Cassandra.html#cassandra","text":"Scio supports writing to Cassandra\nsaveAsCassandra performs bulk writes, grouping by the table partition key before writing to the cluster.\nThe bulk writer writes to all nodes in a cluster so remote nodes in a multi-datacenter cluster may become a bottleneck.\nimport com.spotify.scio._\nimport com.spotify.scio.values.SCollection\nimport com.spotify.scio.cassandra._\n\nval host: String = ???\n\nval cql = \"INSERT INTO myKeyspace.myTable (key, value1) VALUES (?, ?)\"\nval opts = CassandraOptions(\"myKeyspace\", \"myTable\", cql, host)\nval elements: SCollection[(String, Int)] = ???\nelements.saveAsCassandra(opts) { case (key, value) => Seq(key, value) }","title":"Cassandra"},{"location":"io/Csv.html","text":"","title":"CSV"},{"location":"io/Csv.html#csv","text":"Scio supports reading and writing typed CSV via kantan\nKantan provides a CsvConfiguration that allows users to configure the CSV handling, Scio’s default config:\nimport kantan.csv._\nimport kantan.csv.CsvConfiguration.{Header, QuotePolicy}\n\nCsvConfiguration(\n  cellSeparator = ',',\n  quote = '\"',\n  quotePolicy = QuotePolicy.WhenNeeded,\n  header = Header.Implicit\n)","title":"CSV"},{"location":"io/Csv.html#read-csv","text":"FIXME this csvFile link is incorrectly getting two $$ Reading CSV is supported via csvFile. Note that the entire file must be read into memory since CSVs are not trivially splittable.","title":"Read CSV"},{"location":"io/Csv.html#read-with-a-header","text":"For CSV files with a header, reading requires an implicit HeaderDecoder for your type.\nimport com.spotify.scio.ScioContext\nimport com.spotify.scio.values.SCollection\nimport com.spotify.scio.extra.csv._\nimport kantan.csv._\n\ncase class A(i: Int, s: String)\nimplicit val decoder: HeaderDecoder[A] = HeaderDecoder.decoder(\"col1\", \"col2\")(A.apply)\n\nval sc: ScioContext = ???\nval elements: SCollection[A] = sc.csvFile(\"gs://<input-path>/*.csv\")","title":"Read with a header"},{"location":"io/Csv.html#read-without-a-header","text":"For CSV files without a header, an implicit RowDecoder must be in scope and the read must be provided with a config specifying that there is no header:\nimport com.spotify.scio.ScioContext\nimport com.spotify.scio.values.SCollection\nimport com.spotify.scio.extra.csv._\nimport kantan.csv._\n\ncase class A(i: Int, s: String)\n\nimplicit val decoder: RowDecoder[A] = RowDecoder.ordered { (col1: Int, col2: String) => A(col1, col2) }\nval config = CsvIO.DefaultCsvConfiguration.withoutHeader\n\nval sc: ScioContext = ???\nval elements: SCollection[A] = sc.csvFile(\"gs://<input-path>/*.csv\", CsvIO.ReadParam(csvConfiguration = config))","title":"Read without a header"},{"location":"io/Csv.html#write-csv","text":"Writing to CSV is supported via saveAsCsvFile.","title":"Write CSV"},{"location":"io/Csv.html#write-with-a-header","text":"Writing with a header requires an implicit HeaderEncoder to be in scope:\nimport com.spotify.scio.ScioContext\nimport com.spotify.scio.values.SCollection\nimport com.spotify.scio.extra.csv._\nimport kantan.csv._\n\ncase class A(i: Int, s: String)\n\nimplicit val encoder: HeaderEncoder[A] = HeaderEncoder.caseEncoder(\"col1\", \"col2\")(A.unapply)\n\nval elements: SCollection[A] = ???\nelements.saveAsCsvFile(\"gs://<output-path>/\")","title":"Write with a header"},{"location":"io/Csv.html#write-without-a-header","text":"Writing without a header requires an implicit RowEncoder to be in scope:\nimport com.spotify.scio.ScioContext\nimport com.spotify.scio.values.SCollection\nimport com.spotify.scio.extra.csv._\nimport kantan.csv._\n\ncase class A(i: Int, s: String)\n\nimplicit val encoder: RowEncoder[A] = RowEncoder.encoder(0, 1)((a: A) => (a.i, a.s))\n\nval elements: SCollection[A] = ???\nelements.saveAsCsvFile(\"gs://<output-path>/\")","title":"Write without a header"},{"location":"io/Datastore.html","text":"","title":"Datastore"},{"location":"io/Datastore.html#datastore","text":"Scio supports Google Datastore via Beam’s DatastoreIO.\nMagnolify’s EntityType (available as part of the magnolify-datastore artifact) provides automatically-derived mappings between Datastore’s Entity and scala case classes. See full documentation here and an example usage here.","title":"Datastore"},{"location":"io/Datastore.html#reads","text":"Read an SCollection of com.google.datastore.v1.Entity from Datastore with datastore:\nimport com.spotify.scio._\nimport com.spotify.scio.values.SCollection\nimport com.spotify.scio.datastore._\nimport com.google.datastore.v1.{Entity, Query}\n\nval sc: ScioContext = ???\n\nval projectId: String = ???\nval query: Query = Query.getDefaultInstance\nval entities: SCollection[Entity] = sc.datastore(projectId, query)","title":"Reads"},{"location":"io/Datastore.html#writes","text":"Write a collection of\nsaveAsDatastore\nimport com.spotify.scio._\nimport com.spotify.scio.values.SCollection\nimport com.spotify.scio.datastore._\nimport com.google.datastore.v1.{Entity, Query}\n\nval projectId: String = ???\nval entities: SCollection[Entity] = ???\nentities.saveAsDatastore(projectId)","title":"Writes"},{"location":"io/Grpc.html","text":"","title":"GRPC"},{"location":"io/Grpc.html#grpc","text":"Scio supports lookups via GRPC in the scio-grpc artifact.\nGiven an SCollection of GRPC request objects (ConcatRequest below), grpcLookup (or grpcLookupStream for iterable responses, grpcBatchLookup for batched calls) provides a concise syntax for handling responses:\nimport com.spotify.scio._\nimport com.spotify.scio.values.SCollection\nimport com.spotify.scio.grpc._\nimport com.spotify.concat.v1._\nimport io.grpc.netty.NettyChannelBuilder\n\nval ServiceUri: String = \"dns://localhost:50051\"\nval maxPendingRequests = 10\nval requests: SCollection[ConcatRequest] = ???\nrequests\n  .grpcLookup[ConcatResponse, ConcatServiceGrpc.ConcatServiceFutureStub](\n    () => NettyChannelBuilder.forTarget(ServiceUri).usePlaintext().build(),\n    ConcatServiceGrpc.newFutureStub,\n    maxPendingRequests\n  )(_.concat)","title":"GRPC"},{"location":"io/Elasticsearch.html","text":"","title":"Elasticsearch"},{"location":"io/Elasticsearch.html#elasticsearch","text":"Scio supports writing to Elasticsearch.","title":"Elasticsearch"},{"location":"io/Elasticsearch.html#writes","text":"An SCollection of arbitrary elements can be saved to Elasticsearch with saveAsElasticsearch. The ElasticsearchOptions-typed esOptions argument requires a mapperFactory argument capable of mapping the element type to json. saveAsElasticsearch takes a second argument list, whose single argument f can be provided as a block, and which maps the input type to Elasticsearch BulkOperations.\nimport com.spotify.scio._\nimport com.spotify.scio.values.SCollection\nimport com.spotify.scio.elasticsearch._\n\nimport co.elastic.clients.elasticsearch.core.bulk.{BulkOperation, IndexOperation}\nimport co.elastic.clients.json.jackson.JacksonJsonpMapper\nimport com.fasterxml.jackson.module.scala.DefaultScalaModule\nimport com.fasterxml.jackson.datatype.jsr310.JavaTimeModule\nimport org.apache.http.HttpHost\nimport java.time.LocalDate\n\nval host: String = ???\nval port: Int = ???\nval esIndex: String = ???\n\ncase class Document(user: String, postDate: LocalDate, word: String, count: Long)\n\nval primaryESHost = new HttpHost(host, port)\nval mapperFactory = () => {\n  val mapper = new JacksonJsonpMapper()\n  mapper.objectMapper().registerModule(DefaultScalaModule)\n  mapper.objectMapper().registerModule(new JavaTimeModule())\n  mapper\n}\nval esOptions = ElasticsearchOptions(\n  nodes = Seq(primaryESHost), \n  mapperFactory = mapperFactory\n)\n\nval elements: SCollection[Document] = ???\nelements.saveAsElasticsearch(esOptions) { d =>\n  List(\n    BulkOperation.of { bulkBuilder =>\n      bulkBuilder.index(\n        IndexOperation.of[Document] { indexBuilder =>\n          indexBuilder\n            .index(esIndex)\n            .document(d)\n        }\n      )\n    }\n  )\n}","title":"Writes"},{"location":"io/Jdbc.html","text":"","title":"JDBC"},{"location":"io/Jdbc.html#jdbc","text":"Scio supports JDBC reads and writes.","title":"JDBC"},{"location":"io/Jdbc.html#reads","text":"Reads come in two flavors: a query-based variant backed by Beam’s JdbcIO and a “sharded select” that performs a parallelizable bulk read on an entire table.","title":"Reads"},{"location":"io/Jdbc.html#read-via-query","text":"Query-based reads are supported with jdbcSelect. It expects a JdbcConnectionOptions to connect to the database. The statementPreparator argument may be used to set static parameters in the query, usually passed as arguments to the pipeline. The curried rowMapper function argument maps between a java.sql.ResultSet to the result type T.\nimport com.spotify.scio._\nimport com.spotify.scio.values.SCollection\nimport com.spotify.scio.jdbc._\nimport java.sql.Driver\n\nval (sc, args): (ScioContext, Args) = ???\nval sourceArg: String = args(\"wordCountSourceArg\")\n\nval jdbcUrl: String = ???\nval driverClass: Class[Driver] = ???\nval jdbcOptions = JdbcConnectionOptions(\"username\", Some(\"password\"), jdbcUrl, driverClass)\nval query = \"SELECT word, word_count FROM word_count WHERE source = ?\"\n\nval elements: SCollection[(String, Long)] = sc.jdbcSelect(jdbcOptions, query, _.setString(1, sourceArg)) { r =>\n  r.getString(1) -> r.getLong(2)\n}","title":"Read via query"},{"location":"io/Jdbc.html#parallelized-table-read","text":"When an entire table is to be read, the input table can be sharded based on some column value and each shard read in parallel with jdbcShardedSelect.\nJdbcShardedReadOptions requires:\nA rowMapper with the same function as in jdbcSelect The tableName of the table to be read A shardColumn, the column on which the read will be sharded. This column must be indexed and should have an index where shardColumn is not part of a composite index. A shard (Shard) implementation for the type of shardColumn. Provided implementations are Int, Long, BigDecimal, Double, Float, ShardString.HexUpperString, ShardString.HexLowerString, ShardString.UuidUpperString, ShardString.UuidLowerString, and ShardString.Base64String.\nimport com.spotify.scio._\nimport com.spotify.scio.values.SCollection\nimport com.spotify.scio.jdbc._\nimport java.sql.Driver\nimport com.spotify.scio.jdbc.sharded._\n\nval sc: ScioContext = ???\n\nval jdbcUrl: String = ???\nval driverClass: Class[Driver] = ???\nval connOpts = JdbcConnectionOptions(\"username\", Some(\"password\"), jdbcUrl, driverClass)\n\nval shardedReadOptions = JdbcShardedReadOptions[(String, Long), Long](\n  connectionOptions = connOpts,\n  tableName = \"tableName\",\n  shardColumn = \"word_count\",\n  shard = Shard.range[Long],\n  rowMapper = r => (r.getString(\"word\"), r.getLong(\"word_count\"))\n)\nval elements: SCollection[(String, Long)] = sc.jdbcShardedSelect(shardedReadOptions)","title":"Parallelized table read"},{"location":"io/Jdbc.html#writes","text":"Write to JDBC with saveAsJdbc. It expects a JdbcConnectionOptions to connect to the database. The curried preparedStatementSetter function argument receives an instance of the type-to-be-written and a PreparedStatement and appropriately sets the statement fields.\nimport com.spotify.scio._\nimport com.spotify.scio.values.SCollection\nimport com.spotify.scio.jdbc._\nimport java.sql.Driver\n\nval jdbcUrl: String = ???\nval driverClass: Class[Driver] = ???\nval jdbcOptions = JdbcConnectionOptions(\"username\", Some(\"password\"), jdbcUrl, driverClass)\nval statement = \"INSERT INTO word_count (word, count) values (?, ?)\"\n\nval elements: SCollection[(String, Long)] = ???\nelements.saveAsJdbc(jdbcOptions, statement) { case ((word, count), statement) =>\n  statement.setString(1, word)\n  statement.setLong(2, count)\n}","title":"Writes"},{"location":"io/Json.html","text":"","title":"Json"},{"location":"io/Json.html#json","text":"Scio supports reading and writing type-safe Json to a case class via circe. Scio must be able to derive Encoder and Decoder instances for the record type.\nIf you need support for custom encoders or decoders, see the circe documentation","title":"Json"},{"location":"io/Json.html#reading-json","text":"Read Json into a record type with jsonFile:\nimport com.spotify.scio._\nimport com.spotify.scio.values.SCollection\nimport com.spotify.scio.extra.json._\n\ncase class Record(i: Int, d: Double, s: String)\n\nval sc: ScioContext = ???\nval records: SCollection[Record] = sc.jsonFile[Record](\"input.json\")","title":"Reading Json"},{"location":"io/Json.html#writing-json","text":"Write to Json with saveAsJsonFile, which optionally takes a custom printer argument of type io.circe.Printer for controlling formatting.\nimport com.spotify.scio._\nimport com.spotify.scio.extra.json._\nimport com.spotify.scio.values.SCollection\n\ncase class Record(i: Int, d: Double, s: String)\n\nval elements: SCollection[Record] = ???\nelements.saveAsJsonFile(\"gs://<output-path>\")","title":"Writing Json"},{"location":"io/Neo4J.html","text":"","title":"Neo4J"},{"location":"io/Neo4J.html#neo4j","text":"Scio provides support Neo4J in the scio-neo4j artifact.\nScio uses magnolify’s magnolify-neo4j to convert to and from Neo4J types.","title":"Neo4J"},{"location":"io/Neo4J.html#static-query","text":"neo4jCypher returns an SCollection of results for a Neo4J cypher query, mapped to a specified case class type.\nimport com.spotify.scio._\nimport com.spotify.scio.values.SCollection\nimport com.spotify.scio.neo4j._\n\ncase class Entity(id: String, property: Option[String])\n\nval sc: ScioContext = ???\nval opts = Neo4jOptions(Neo4jConnectionOptions(\"neo4j://neo4j.com:7687\", \"username\", \"password\"))\nval query =\n  \"\"\"MATCH (e:Entity)\n    |WHERE e.property = 'value'\n    |RETURN e\"\"\".stripMargin\nval entities: SCollection[Entity] = sc\n  .neo4jCypher[Entity](opts, query)","title":"Static query"},{"location":"io/Neo4J.html#parameterized-query","text":"neo4jCypher can also construct queries from parameters in an existing SCollection:\nimport com.spotify.scio._\nimport com.spotify.scio.values.SCollection\nimport com.spotify.scio.neo4j._\n\ncase class MovieParam(year: Int)\ncase class Person(name: String)\ncase class Movie(title: String, year: Int)\ncase class Role(person: Person, movie: Movie, role: String)\n\nval sc: ScioContext = ???\nval input: SCollection[MovieParam] = sc.parallelize(\n  List(\n    MovieParam(1994),\n    MovieParam(0),\n    MovieParam(1995)\n  )\n)\n\nval opts = Neo4jOptions(Neo4jConnectionOptions(\"neo4j://neo4j.com:7687\", \"username\", \"password\"))\n\nval queryRoles =\n  \"\"\"MATCH (p)-[r: ACTED_IN]->(m)\n    |WHERE m.year = $year\n    |RETURN p as person, m as movie, r.role as role\n    |\"\"\".stripMargin\n\ninput.neo4jCypher[Role](opts, queryRoles)","title":"Parameterized query"},{"location":"io/Neo4J.html#writes","text":"Instances can be written via saveAsNeo4j:\nimport com.spotify.scio._\nimport com.spotify.scio.values.SCollection\nimport com.spotify.scio.neo4j._\n\ncase class Entity(id: String, property: Option[String])\n\nval sc: ScioContext = ???\nval input: SCollection[Entity] = ???\n\nval opts = Neo4jOptions(Neo4jConnectionOptions(\"neo4j://neo4j.com:7687\", \"username\", \"password\"))\nval unwindCypher =\n  \"\"\"UNWIND $rows AS row\n    |MERGE (e:Entity {id: row.id})\n    |ON CREATE SET p.id = row.id, p.property = row.property\n    |\"\"\".stripMargin\ninput.saveAsNeo4j(opts, unwindCypher)","title":"Writes"},{"location":"io/Object.html","text":"","title":"Object file"},{"location":"io/Object.html#object-file","text":"“Object files” can be used to save an SCollection of records with an arbitrary type by using Beam’s coder infrastructure. Each record is encoded to a byte array by the available Beam coder, the bytes are then wrapped in a simple Avro record containing a single byte field, then saved to disk.\nObject files are convenient for ad-hoc work, but it should be preferred to use a real schema-backed format when possible.","title":"Object file"},{"location":"io/Object.html#reading-object-files","text":"Object files can be read via objectFile:\nimport com.spotify.scio._\nimport com.spotify.scio.avro._\nimport com.spotify.scio.values.SCollection\n\ncase class A(i: Int, s: String)\n\nval sc: ScioContext = ???\nval elements: SCollection[A] = sc.objectFile(\"gs://<input-path>/*.obj.avro\")","title":"Reading object files"},{"location":"io/Object.html#writing-object-files","text":"Object files can be written via saveAsObjectFile:\nimport com.spotify.scio._\nimport com.spotify.scio.avro._\nimport com.spotify.scio.values.SCollection\n\ncase class A(i: Int, s: String)\n\nval elements: SCollection[A] = ???\nelements.saveAsObjectFile(\"gs://<output-path>\")","title":"Writing object files"},{"location":"io/Parquet.html","text":"","title":"Parquet"},{"location":"io/Parquet.html#parquet","text":"Scio supports reading and writing Parquet files as Avro records or Scala case classes. Also see Avro page on reading and writing regular Avro files.","title":"Parquet"},{"location":"io/Parquet.html#avro","text":"","title":"Avro"},{"location":"io/Parquet.html#read-parquet-files-as-avro","text":"When reading Parquet as Avro specific records, one can use parquet-extra macros for generating column projections and row predicates using idiomatic Scala syntax. To read a Parquet file as Avro specific record with column projections and row predicates:\nimport com.spotify.scio._\nimport com.spotify.scio.parquet.avro._\nimport com.spotify.scio.avro.TestRecord\n\nobject ParquetJob {\n  def main(cmdlineArgs: Array[String]): Unit = {\n\n    val (sc, args) = ContextAndArgs(cmdlineArgs)\n\n    // Macros for generating column projections and row predicates\n    val projection = Projection[TestRecord](_.getIntField, _.getLongField, _.getBooleanField)\n    val predicate = Predicate[TestRecord](x => x.getIntField > 0 && x.getBooleanField)\n\n    sc.parquetAvroFile[TestRecord](\"input.parquet\", projection, predicate)\n      // Map out projected fields right after reading\n      .map(r => (r.getIntField, r.getStringField, r.getBooleanField))\n\n    sc.run()\n    ()\n  }\n}\nNote that the result TestRecords are not complete Avro objects. Only the projected columns (intField, stringField, booleanField) are present while the rest are null. These objects may fail serialization and it’s recommended that you map them out to tuples or case classes right after reading.\nAlso note that predicate logic is only applied when reading actual Parquet files but not in JobTest. To retain the filter behavior while using mock input, it’s recommend that you do the following.\nimport com.spotify.scio._\nimport com.spotify.scio.parquet.avro._\nimport com.spotify.scio.avro.TestRecord\n\nobject ParquetJob {\n  def main(cmdlineArgs: Array[String]): Unit = {\n\n    val (sc, args) = ContextAndArgs(cmdlineArgs)\n\n    val projection = Projection[TestRecord](_.getIntField, _.getLongField, _.getBooleanField)\n    // Build both native filter function and Parquet FilterPredicate\n    // case class Predicates[T](native: T => Boolean, parquet: FilterPredicate)\n    val predicate = Predicate.build[TestRecord](x => x.getIntField > 0 && x.getBooleanField)\n\n    sc.parquetAvroFile[TestRecord](\"input.parquet\", projection, predicate.parquet)\n      // filter natively with the same logic in case of mock input in `JobTest`\n      .filter(predicate.native)\n\n    sc.run()\n    ()\n  }\n}\nYou can also read Avro generic records by specifying a reader schema.\nimport com.spotify.scio._\nimport com.spotify.scio.parquet.avro._\nimport com.spotify.scio.avro.TestRecord\nimport org.apache.avro.generic.GenericRecord\n\nobject ParquetJob {\n  def main(cmdlineArgs: Array[String]): Unit = {\n\n    val (sc, args) = ContextAndArgs(cmdlineArgs)\n\n    sc.parquetAvroFile[GenericRecord](\"input.parquet\", TestRecord.getClassSchema)\n      // Map out projected fields into something type safe\n      .map(r => (r.get(\"int_field\").asInstanceOf[Int], r.get(\"string_field\").toString))\n\n    sc.run()\n    ()\n  }\n}","title":"Read Parquet files as Avro"},{"location":"io/Parquet.html#write-avro-to-parquet-files","text":"Both Avro generic and specific records are supported when writing.\nType of Avro specific records will hold information about schema, therefore Scio will figure out the schema by itself:\nimport com.spotify.scio.values._\nimport com.spotify.scio.parquet.avro._\nimport com.spotify.scio.avro.TestRecord\n\ndef input: SCollection[TestRecord] = ???\ndef result = input.saveAsParquetAvroFile(\"gs://path-to-data/lake/output\")\nWriting Avro generic records requires additional argument schema:\nimport com.spotify.scio.values._\nimport com.spotify.scio.coders.Coder\nimport com.spotify.scio.avro._\nimport com.spotify.scio.parquet.avro._\nimport org.apache.avro.generic.GenericRecord\n\ndef input: SCollection[GenericRecord] = ???\nlazy val yourAvroSchema: org.apache.avro.Schema = ???\nimplicit lazy val coder: Coder[GenericRecord] = avroGenericRecordCoder(yourAvroSchema)\n\ndef result = input.saveAsParquetAvroFile(\"gs://path-to-data/lake/output\", schema = yourAvroSchema)","title":"Write Avro to Parquet files"},{"location":"io/Parquet.html#logical-types","text":"As of Scio 0.14.0 and above, Scio supports specific record logical types in parquet-avro out of the box.\nWhen using generic record you’ll need to supply the additional Configuration parameter AvroReadSupport.AVRO_DATA_SUPPLIER for reads or AvroWriteSupport.AVRO_DATA_SUPPLIER for writes to use logical types.\nimport com.spotify.scio._\nimport com.spotify.scio.avro._\nimport com.spotify.scio.coders.Coder\nimport com.spotify.scio.parquet.avro._\nimport com.spotify.scio.parquet.ParquetConfiguration\nimport com.spotify.scio.values.SCollection\nimport org.apache.avro.Conversions\nimport org.apache.avro.generic.GenericRecord\nimport org.apache.avro.data.TimeConversions\nimport org.apache.avro.generic.GenericData\nimport org.apache.parquet.avro.{AvroDataSupplier, AvroReadSupport, AvroWriteSupport}\n\nval sc: ScioContext = ???\nimplicit val coder: Coder[GenericRecord] = ???\nval data: SCollection[GenericRecord] = ???\n\nclass AvroLogicalTypeSupplier extends AvroDataSupplier {\n  override def get(): GenericData = {\n    val data = GenericData.get()\n\n    // Add conversions as needed\n    data.addLogicalTypeConversion(new TimeConversions.TimestampMillisConversion())\n\n    data\n  }\n}\n\n// Reads\nsc.parquetAvroFile(\n  \"somePath\",\n  conf = ParquetConfiguration.of(AvroReadSupport.AVRO_DATA_SUPPLIER -> classOf[AvroLogicalTypeSupplier])\n)\n\n// Writes\ndata.saveAsParquetAvroFile(\n  \"somePath\",\n  conf = ParquetConfiguration.of(AvroWriteSupport.AVRO_DATA_SUPPLIER -> classOf[AvroLogicalTypeSupplier])\n)","title":"Logical Types"},{"location":"io/Parquet.html#case-classes","text":"Scio uses magnolify-parquet to derive Parquet reader and writer for case classes at compile time, similar to how coders work. See this mapping table for how Scala and Parquet types map; enum type mapping is also specifically documented.","title":"Case classes"},{"location":"io/Parquet.html#read-parquet-files-as-case-classes","text":"When reading Parquet files as case classes, all fields in the case class definition are read. Therefore, it’s desirable to construct a case class type with only fields needed for processing.\nStarting in Magnolify 0.4.8 (corresponding to Scio 0.11.6 and above), predicates for case classes have Magnolify support at the field level only. You can use Parquet’s FilterApi.or and FilterApi.and to chain them:\nimport com.spotify.scio._\nimport com.spotify.scio.parquet.types._\nimport magnolify.parquet._\nimport org.apache.parquet.filter2.predicate.FilterApi\n\nobject ParquetJob {\n  case class MyRecord(int_field: Int, string_field: String)\n\n  def main(cmdlineArgs: Array[String]): Unit = {\n\n    val (sc, args) = ContextAndArgs(cmdlineArgs)\n\n    sc.typedParquetFile[MyRecord](\"input.parquet\", predicate = FilterApi.and(\n      Predicate.onField[String](\"string_field\")(_.startsWith(\"a\")),\n      Predicate.onField[Int](\"int_field\")(_ % 2 == 0))\n    )\n\n    sc.run()\n    ()\n  }\n}","title":"Read Parquet files as case classes"},{"location":"io/Parquet.html#write-case-classes-to-parquet-files","text":"When writing case classes as Parquet files, the schema is derived from the case class and all fields are written.\nimport com.spotify.scio.values._\nimport com.spotify.scio.parquet.types._\n\ncase class MyRecord(int_field: Int, string_field: String)\ndef input: SCollection[MyRecord] = ???\n\ndef result = input.saveAsTypedParquetFile(\"gs://path-to-data/lake/output\")","title":"Write case classes to Parquet files"},{"location":"io/Parquet.html#compatibility","text":"Note that Parquet writes Avro array fields differently than most other Parquet submodules. For example my_field: List[Int] would normally map to something like this:\nrepeated int32 my_field;\nWhile parquet-avro would map it to this:\nrequired group my_field {\n  repeated int32 array;\n}\nAdd the following import to handle typed Parquet in a way compatible with Parquet Avro:\nimport magnolify.parquet.ParquetArray.AvroCompat._\nThe same Avro schema evolution principles apply to Parquet, i.e. only append OPTIONAL or REPEATED fields with default null or []. See this test for some common scenarios w.r.t. Parquet schema evolution.","title":"Compatibility"},{"location":"io/Parquet.html#configuring-parquet","text":"The Parquet Java library heavily relies on Hadoop’s Job API. Therefore, in both the Parquet library and in scio-parquet, we use Hadoop’s Configuration class to manage most Parquet read and write options.\nThe Configuration class, when initialized, will load default values from the first available core-site.xml found on the classpath. Scio-parquet provides a default core-site.xml implementation: if your Scio pipeline has a dependency on scio-parquet, these default options will be picked up in your pipeline.","title":"Configuring Parquet"},{"location":"io/Parquet.html#overriding-the-default-configuration","text":"You can override the default configuration in two ways:\nDeclare a core-site.xml file of your own in your project’s src/main/resources folder. Note that Hadoop can only pick one core-site.xml to read: if you override the file in your project, Hadoop will not read Scio’s default core-site.xml at all, and none of its default options will be loaded. Create an in-memory Configuration object for use with scio-parquet’s ReadParam and WriteParam. Any options provided this way will be appended to Scio’s default configuration.\nYou can create and pass in a custom Configuration using our ParquetConfiguration helper, available in Scio 0.12.x and above:\nimport com.spotify.scio.parquet.ParquetConfiguration\n\ndata\n  .saveAsParquetAvroFile(args(\"output\"), conf = ParquetConfiguration.of(\"parquet.block.size\" -> 536870912))\nIf you’re on Scio 0.11.x or below, you’ll have to create a Configuration object directly:\nimport org.apache.hadoop.conf.Configuration\n\nval parquetConf: Configuration = {\n  val conf: Configuration = new Configuration()\n  conf.setInt(\"parquet.block.size\", 536870912)\n  conf\n}","title":"Overriding the Default Configuration"},{"location":"io/Parquet.html#common-configuration-options","text":"parquet.block.size - This determines block size for HDFS and row group size. 1 GiB is recommended over the default 128 MiB, although you’ll have to weigh the tradeoffs: a larger block size means fewer seek operations on blob storage, at the cost of having to load a larger row group into memory. fs.gs.inputstream.fadvise - “Fadvise allows applications to provide a hint to the Linux kernel with the intended I/O access pattern, indicating how it intends to read a file, whether for sequential scans or random seeks.” According to this blog post “traditional MapReduce jobs” are ideal use cases for setting this config to SEQUENTIAL, and Scio jobs fit in this category.\nHere are some other recommended settings.\nnumShards - This should be explicitly set so that the size of each output file is smaller than but close to parquet.block.size, i.e. 1 GiB. This guarantees that each file contains 1 row group only and reduces seeks. compression - Parquet defaults to ZSTD compression with a level of 3; compression level can be set to any integer from 1-22 using the configuration option parquet.compression.codec.zstd.level. SNAPPY and GZIP compression types also work out of the box; Snappy is less CPU intensive but has lower compression ratio. In our benchmarks GZIP seem to work better than Snappy on GCS.\nA full list of Parquet configuration options can be found here.","title":"Common Configuration Options"},{"location":"io/Parquet.html#parquet-reads-in-scio-0-12-0-","text":"Parquet read internals have been reworked in Scio 0.12.0. As of 0.12.0, you can opt-into the new Parquet read implementation, backed by the new Beam SplittableDoFn API, by following the instructions here.","title":"Parquet Reads in Scio 0.12.0+"},{"location":"io/Parquet.html#testing","text":"In addition to JobTest support for Avro, Typed, and Tensorflow models, Scio 0.14.5 and above include utilities for testing projections and predicates. Just import the desired module, com.spotify.scio.testing.parquet.{avro|types|tensorflow}._, from the scio-test-parquet artifact. For example, test utilities for Avro are available in com.spotify.scio.testing.parquet.avro._:\nimport com.spotify.scio.testing.parquet.avro._\n\nval projection: Schema = ???\nval predicate: FilterPredicate = ???\n\nval records: Iterable[T <: SpecificRecord] = ???\nval expected: Iterable[T <: SpecificRecord] = ???\n\nrecords withProjection projection withPredicate predicate should contain theSameElementsAs expected\nYou can also test a case class projection against an Avro writer schema to ensure writer/reader compatibility:\nimport com.spotify.scio.testing.parquet.avro._\n\ncase class MyProjection(id: Int)\nval records: Iterable[T <: SpecificRecord] = ???\nval expected: Iterable[MyRecord] = ???\n\nrecords withProjection[MyProjection] should contain theSameElementsAs expected","title":"Testing"},{"location":"io/Protobuf.html","text":"","title":"Protobuf"},{"location":"io/Protobuf.html#protobuf","text":"","title":"Protobuf"},{"location":"io/Protobuf.html#read-protobuf-files","text":"Scio comes with custom and efficient support for reading Protobuf files via protobufFile method, for example:\nimport com.spotify.scio.avro._\n\n// FooBarProto is a Protobuf generated class (must be a subclass of Protobuf's `Message`)\nsc.protobufFile[FooBarProto](\"gs://path-to-data/lake/part-*.protobuf.avro\")\n  .map( message => ??? )\n// `message` is of type FooBarProto\nImportant: in most cases the input files should have been previously written by Scio. The reason is that Scio assumes that serialized Protobuf message is stored inside bytes Avro record.\nIf you want to read serialized protobuf messages directly from a file, one solution is to use textFile followed by a map to parse your messages.","title":"Read Protobuf files"},{"location":"io/Protobuf.html#write-protobuf-files","text":"Scio comes with custom and efficient support for writing Protobuf files via saveAsProtobufFile method, for example:\n// FooBarProto is a Protobuf generated class (must be a subclass of Protobuf's `Message`)\nval data: SCollection[FooBarProto] = ???\ndata.saveAsProtobufFile(\"gs://path-to-data/lake/protos-out\")","title":"Write Protobuf files"},{"location":"io/Protobuf.html#file-format","text":"Scio’s Protobuf file is backed by Avro with the following schema:\n{\n  \"type\" : \"record\",\n  \"name\" : \"AvroBytesRecord\",\n  \"fields\" : [ {\n    \"name\" : \"bytes\",\n    \"type\" : \"bytes\"\n  } ]\n}\nAvro gives us a block based file format with compression, split and combine support. Protobuf binary is stored in the bytes field of AvroBytesRecord.\nStarting with Scio 0.2.6, the Protobuf schema also is stored as a JSON string in the Avro file metadata under the key protobuf.generic.schema. You can get the schema or JSON records using the proto-tools command line tool from gcs-tools (available in our homebrew tap). Conversion between Protobuf schema, binary and JSON is done via the protobuf-generic library.\nbrew tap spotify/public\nbrew install gcs-proto-tools\nproto-tools getschema data.protobuf.avro\nproto-tools tojson data.protobuf.avro","title":"File format"},{"location":"io/Protobuf.html#common-issues","text":"","title":"Common issues"},{"location":"io/Protobuf.html#scalapb","text":"If you end up using ScalaPB, make sure to use java based message class as input/output type, Scala based message class does not inherit from Protobuf’s Message class. To generate both Scala and Java classes add (to your build.sbt):\nimport com.trueaccord.scalapb.{ScalaPbPlugin => PB}\nPB.javaConversions in PB.protobufConfig := true","title":"ScalaPB"},{"location":"io/Pubsub.html","text":"","title":"PubSub"},{"location":"io/Pubsub.html#pubsub","text":"Scio supports Google Cloud PubSub","title":"PubSub"},{"location":"io/Pubsub.html#read-from-pubsub","text":"Use the appropriate PubsubIO method with ScioContext.read to read into strings, avro, protobuf, beam’s PubsubMessage, or into any type supported by a scio Coder. Pass a PubsubIO.ReadParam to configure whether reading from a topic or subscription.\nimport com.spotify.scio._\nimport com.spotify.scio.values._\nimport com.spotify.scio.pubsub._\n\nval a: PubsubIO[String] = PubsubIO.string(\"strings\")\n\nimport com.spotify.scio.avro.Account\nval b: PubsubIO[Account] = PubsubIO.avro[Account](\"avros\")\n\nimport com.spotify.scio.proto.Track.TrackPB\nval c: PubsubIO[TrackPB] = PubsubIO.proto[TrackPB](\"protos\")\n\nimport org.apache.beam.sdk.io.gcp.pubsub.PubsubMessage\nval d: PubsubIO[PubsubMessage] = PubsubIO.pubsub[PubsubMessage](\"messages\")\n\ncase class MyClass(s: String, i: Int)\nval e: PubsubIO[MyClass] = PubsubIO.coder[MyClass](\"myclasses\")\n\nval sc: ScioContext = ???\n\n// read strings from a subscription\nval in1: SCollection[String] = sc.read(a)(PubsubIO.ReadParam(PubsubIO.Subscription))\n\n// or from a topic\nval in2: SCollection[String] = sc.read(a)(PubsubIO.ReadParam(PubsubIO.Topic))\nThe withAttributes methods give access to the PubSub attributes within the SCollection:\nimport com.spotify.scio._\nimport com.spotify.scio.values._\nimport com.spotify.scio.pubsub._\n\nval sc: ScioContext = ???\nval in: SCollection[(String, Map[String, String])] =\n  sc.read(PubsubIO.withAttributes[String](\"strings\"))(PubsubIO.ReadParam(PubsubIO.Subscription))\n    .map { case (element, attributes) =>\n      attributes.get(\"name\")\n      ???\n    }","title":"Read from PubSub"},{"location":"io/Pubsub.html#write-to-pubsub","text":"PubSub write methods use the same PubSubIO methods as reading:\nimport com.spotify.scio._\nimport com.spotify.scio.values._\nimport com.spotify.scio.pubsub._\n\nval strings: SCollection[String] = ???\nstrings.write(PubsubIO.string(\"strings\"))(PubsubIO.WriteParam())\n\nimport com.spotify.scio.avro.Account\nval accounts: SCollection[Account] = ???\naccounts.write(PubsubIO.avro[Account](\"accounts\"))(PubsubIO.WriteParam())\n\nimport com.spotify.scio.proto.Track.TrackPB\nval tracks: SCollection[TrackPB] = ???\ntracks.write(PubsubIO.proto[TrackPB](\"tracks\"))(PubsubIO.WriteParam())\n\nimport org.apache.beam.sdk.io.gcp.pubsub.PubsubMessage\nval messages: SCollection[PubsubMessage] = ???\nmessages.write(PubsubIO.pubsub[PubsubMessage](\"messages\"))(PubsubIO.WriteParam())\n\ncase class MyClass(s: String, i: Int)\nval myClasses: SCollection[MyClass] = ???\nmyClasses.write(PubsubIO.coder[MyClass](\"myClasses\"))(PubsubIO.WriteParam())\nWriting attributes:\nimport com.spotify.scio._\nimport com.spotify.scio.values._\nimport com.spotify.scio.pubsub._\n\nval strings: SCollection[(String, Map[String, String])] = ???\nstrings.write(PubsubIO.withAttributes[String](\"strings\"))(PubsubIO.WriteParam())","title":"Write to PubSub"},{"location":"io/ReadFiles.html","text":"","title":"ReadFiles"},{"location":"io/ReadFiles.html#readfiles","text":"Scio supports reading file paths/patterns from an SCollection[String] into various formats.","title":"ReadFiles"},{"location":"io/ReadFiles.html#read-as-text-lines","text":"Reading to String text lines via readTextFiles:\nimport com.spotify.scio.ScioContext\nimport com.spotify.scio.values.SCollection\n\nval sc: ScioContext = ???\nval paths: SCollection[String] = ???\n\nval lines: SCollection[String] = paths.readTextFiles","title":"Read as text lines"},{"location":"io/ReadFiles.html#read-entire-file-as-string","text":"Reading entire files to String via readFilesAsString:\nimport com.spotify.scio.ScioContext\nimport com.spotify.scio.values.SCollection\n\nval sc: ScioContext = ???\nval paths: SCollection[String] = ???\n\nval files: SCollection[String] = paths.readFilesAsString","title":"Read entire file as String"},{"location":"io/ReadFiles.html#read-entire-file-as-binary","text":"Reading entire files to binary Array[Byte] via readFilesAsBytes:\nimport com.spotify.scio.ScioContext\nimport com.spotify.scio.values.SCollection\n\nval sc: ScioContext = ???\nval paths: SCollection[String] = ???\n\nval files: SCollection[Array[Byte]] = paths.readFilesAsBytes","title":"Read entire file as binary"},{"location":"io/ReadFiles.html#read-entire-file-as-a-custom-type","text":"Reading entire files to a custom type with a user-defined function from FileIO.ReadableFile to the output type via readFiles:\nimport com.spotify.scio.ScioContext\nimport com.spotify.scio.values.SCollection\nimport org.apache.beam.sdk.{io => beam}\n\ncase class A(i: Int, s: String)\nval sc: ScioContext = ???\nval paths: SCollection[String] = ???\n\nval userFn: beam.FileIO.ReadableFile => A = ???\nval fileBytes: SCollection[A] = paths.readFiles(userFn)","title":"Read entire file as a custom type"},{"location":"io/ReadFiles.html#read-with-a-beam-transform","text":"Reading a file can be done with a beam PTransform from a PCollection[FileIO.ReadableFile] to PCollection[T] (as an example, beam’s TextIO.readFiles()), via another variant of readFiles\nimport com.spotify.scio.ScioContext\nimport com.spotify.scio.values.SCollection\nimport org.apache.beam.sdk.{io => beam}\nimport org.apache.beam.sdk.transforms.PTransform\nimport org.apache.beam.sdk.values.PCollection\n\ncase class Record(i: Int, s: String)\n\nval sc: ScioContext = ???\nval paths: SCollection[String] = ???\n\nval userTransform: PTransform[PCollection[beam.FileIO.ReadableFile], PCollection[Record]] = ???\nval records: SCollection[Record] = paths.readFiles(userTransform)","title":"Read with a Beam transform"},{"location":"io/ReadFiles.html#read-with-a-beam-source","text":"Reading a file can be done with a beam FileBasedSource[T] (as example, beam’s TextSource) via another variant of readFiles.\nWhen using readFilesWithPath, the origin file path will be passed along with all elements emitted by the source.\nThe source will be created with the given file paths, and then split in sub-ranges depending on the desired bundle size.\nimport com.spotify.scio.ScioContext\nimport com.spotify.scio.values.SCollection\nimport org.apache.beam.sdk.{io => beam}\n\ncase class Record(i: Int, s: String)\n\nval sc: ScioContext = ???\nval paths: SCollection[String] = ???\n\nval desiredBundleSizeBytes: Long = ???\nval directoryTreatment: beam.FileIO.ReadMatches.DirectoryTreatment = ???\nval compression: beam.Compression = ???\nval createSource: String => beam.FileBasedSource[Record] = ???\n\nval records: SCollection[Record] = paths.readFiles(\n  desiredBundleSizeBytes,\n  directoryTreatment,\n  compression\n) { file => createSource(file) }\n\nval recordsWithPath: SCollection[(String, Record)] = paths.readFilesWithPath(\n  desiredBundleSizeBytes,\n  directoryTreatment,\n  compression\n) { file => createSource(file) }","title":"Read with a Beam source"},{"location":"io/Redis.html","text":"","title":"Redis"},{"location":"io/Redis.html#redis","text":"Scio provides support for Redis in the scio-redis artifact.","title":"Redis"},{"location":"io/Redis.html#batch-read","text":"Reading key-value pairs from redis for a specific key pattern is supported via redis:\nimport com.spotify.scio._\nimport com.spotify.scio.values.SCollection\nimport com.spotify.scio.redis._\n\nval sc: ScioContext = ???\nval connectionOptions = RedisConnectionOptions(\"redisHost\", 6379)\nval keyPattern = \"foo*\"\n\nval elements: SCollection[(String, String)] = sc.redis(connectionOptions, keyPattern)","title":"Batch read"},{"location":"io/Redis.html#lookups","text":"Looking up specific keys from redis can be done with RedisDoFn:\nimport com.spotify.scio._\nimport com.spotify.scio.redis._\nimport com.spotify.scio.values.SCollection\nimport org.apache.beam.sdk.transforms.ParDo\nimport scala.concurrent.{ExecutionContext, Future}\n\nval redisHost: String = ???\nval redisPort: Int = ???\nval batchSize: Int = ???\nval connectionOptions = RedisConnectionOptions(redisHost, redisPort)\n\nval keys: SCollection[String] = ???\n\nkeys\n  .applyTransform(\n    ParDo.of(\n      new RedisDoFn[String, (String, Option[String])](connectionOptions, batchSize) {\n        override def request(value: String, client: Client)(\n          implicit ec: ExecutionContext\n        ): Future[(String, Option[String])] =\n          client\n            .request(p => p.get(value) :: Nil)\n            .map { case r: List[String @unchecked] => (value, r.headOption) }\n      }\n    )\n  )","title":"Lookups"},{"location":"io/Redis.html#write","text":"Writes to Redis require an SCollection of a subclass of RedisMutation. Writes work in both batch and streaming modes via saveAsRedis:\nimport com.spotify.scio._\nimport com.spotify.scio.values.SCollection\nimport com.spotify.scio.redis._\nimport com.spotify.scio.redis.types._\n\nval connectionOptions = RedisConnectionOptions(\"redisHost\", 6379)\n\nval keys: SCollection[String] = ???\nkeys.map(IncrBy(_, 1)).saveAsRedis(connectionOptions)","title":"Write"},{"location":"io/Spanner.html","text":"","title":"Spanner"},{"location":"io/Spanner.html#spanner","text":"Scio supports reading and writing from Google Cloud Spanner.","title":"Spanner"},{"location":"io/Spanner.html#read-from-spanner","text":"Reads from Spanner occur via a query with spannerQuery or for an entire table with spannerTable. Both return an SCollection of Struct:\nTo read with a query:\nimport com.spotify.scio._\nimport com.spotify.scio.spanner._\nimport com.spotify.scio.values.SCollection\nimport org.apache.beam.sdk.io.gcp.spanner.SpannerConfig\nimport com.google.cloud.spanner.Struct\n\nval config: SpannerConfig = SpannerConfig\n    .create()\n    .withProjectId(\"someProject\")\n    .withDatabaseId(\"someDatabase\")\n    .withInstanceId(\"someInstance\")\n\nval sc: ScioContext = ???\nval queryStructs: SCollection[Struct] = sc.spannerQuery(config, \"SELECT a, b FROM table WHERE c > 5\")\nTo read an entire table:\nimport com.spotify.scio._\nimport com.spotify.scio.spanner._\nimport com.spotify.scio.values.SCollection\nimport org.apache.beam.sdk.io.gcp.spanner.SpannerConfig\nimport com.google.cloud.spanner.Struct\n\nval config: SpannerConfig = SpannerConfig\n  .create()\n  .withProjectId(\"someProject\")\n  .withDatabaseId(\"someDatabase\")\n  .withInstanceId(\"someInstance\")\n\nval sc: ScioContext = ???\nval tableStructs: SCollection[Struct] = sc.spannerTable(config, \"table\", columns=List(\"a\", \"b\"))","title":"Read from Spanner"},{"location":"io/Spanner.html#write-to-spanner","text":"An SCollection containing Mutation instances can be written to Spanner via saveAsSpanner:\nimport com.spotify.scio.spanner._\nimport com.spotify.scio.values.SCollection\nimport org.apache.beam.sdk.io.gcp.spanner.SpannerConfig\nimport com.google.cloud.spanner.Mutation\n\nval config: SpannerConfig = SpannerConfig\n  .create()\n  .withProjectId(\"someProject\")\n  .withDatabaseId(\"someDatabase\")\n  .withInstanceId(\"someInstance\")\n\nval mutations: SCollection[Mutation] = ???\nmutations.saveAsSpanner(config)","title":"Write to Spanner"},{"location":"io/Tensorflow.html","text":"","title":"Tensorflow"},{"location":"io/Tensorflow.html#tensorflow","text":"Scio supports several methods of reading and writing Tensorflow records.","title":"Tensorflow"},{"location":"io/Tensorflow.html#reading","text":"Depending on your input format, and if you need to provide a schema or not, there are various ways to read Tensorflow files.\ntfRecordFile reads entire TFRecord files into byte array elements in the pipeline, tfRecordExampleFile (or tfRecordExampleFileWithSchema) will read Example instances, and tfRecordSequenceExampleFile (or tfRecordSequenceExampleFileWithSchema) will read SequenceExample instances:\nimport com.spotify.scio.ScioContext\nimport com.spotify.scio.values.SCollection\nimport com.spotify.scio.tensorflow._\nimport org.tensorflow.proto.example.{Example, SequenceExample}\n\nval sc: ScioContext = ???\nval recordBytes: SCollection[Array[Byte]] = sc.tfRecordFile(\"gs://input-record-path\")\nval examples: SCollection[Example] = sc.tfRecordExampleFile(\"gs://input-example-path\")\nval sequenceExamples: SCollection[SequenceExample] = sc.tfRecordSequenceExampleFile(\"gs://input-sequence-example-path\")","title":"Reading"},{"location":"io/Tensorflow.html#writing","text":"Similar to reading, there are multiple ways to write Tensorflow files, depending on the format of the elements to be output. Each of these write methods is called saveAsTfRecordFile, but only one variant of the method is available based on the element type.\nFor SCollection[T] where T is a subclass of Example: saveAsTfRecordFile For SCollection[Seq[T]] where T is a subclass of Example: saveAsTfRecordFile For SCollection[T] where T is a subclass of SequenceExample: saveAsTfRecordFile For SCollection[Array[Byte]], where it is recommended that the bytes are a serialized Example: saveAsTfRecordFile\nimport com.spotify.scio.values.SCollection\nimport com.spotify.scio.tensorflow._\nimport org.tensorflow.proto.example.{Example, SequenceExample}\n\nval recordBytes: SCollection[Array[Byte]] = ???\nval examples: SCollection[Example] = ???\nval seqExamples: SCollection[Seq[Example]] = ???\nval sequenceExamples: SCollection[SequenceExample] = ???\n\nrecordBytes.saveAsTfRecordFile(\"gs://output-record-path\")\nexamples.saveAsTfRecordFile(\"gs://output-example-path\")\nseqExamples.saveAsTfRecordFile(\"gs://output-seq-example-path\")\nsequenceExamples.saveAsTfRecordFile(\"gs://output-sequence-example-path\")","title":"Writing"},{"location":"io/Tensorflow.html#prediction-inference","text":"Scio supports preforming inference on a saved Tensorflow model.\nFor an SCollection of an arbitrary user type, predictions can be made against the raw model via predict or using the model’s SignatureDefs with predictWithSigDef:\nimport com.spotify.scio.values.SCollection\nimport com.spotify.scio.tensorflow._\nimport com.spotify.zoltar.tf.TensorFlowModel\nimport org.tensorflow._\nimport org.tensorflow.proto.example.Example\n\ncase class A()\ncase class B()\n\ndef toTensors(a: A): Map[String, Tensor] = ???\ndef fromTensors(a: A, tensors: Map[String, Tensor]): B = ???\n\nval elements: SCollection[A] = ???\nval options: TensorFlowModel.Options = ???\nval fetchOpts: Seq[String] = ???\n\nval result: SCollection[B] = elements.predict[B](\"gs://model-path\", fetchOpts, options)(toTensors)(fromTensors)\nval b: SCollection[B] = elements.predictWithSigDef[B](\"gs://model-path\", options)(toTensors)(fromTensors _)\nFor an SCollection of some subclass of Example, a prediction can be made via predictTfExamples:\nimport com.spotify.scio.values.SCollection\nimport com.spotify.scio.tensorflow._\nimport com.spotify.zoltar.tf.TensorFlowModel\nimport org.tensorflow._\nimport org.tensorflow.proto.example.Example\n\nval exampleElements: SCollection[Example] = ???\nval options: TensorFlowModel.Options = ???\ndef toExample(in: Example, tensors: Map[String, Tensor]): Example = ???\n\nval c: SCollection[Example] = exampleElements.predictTfExamples[Example](\"gs://model-path\", options) {\n  case (a, tensors) => toExample(a, tensors)\n}","title":"Prediction/inference"},{"location":"io/Text.html","text":"","title":"Text"},{"location":"io/Text.html#text","text":"","title":"Text"},{"location":"io/Text.html#reading-text","text":"Scio reads newline-delimited text via textFile:\nimport com.spotify.scio._\nimport com.spotify.scio.avro._\nimport com.spotify.scio.values.SCollection\n\nval sc: ScioContext = ???\nval elements: SCollection[String] = sc.textFile(\"gs://<input-path>/*.txt\")","title":"Reading text"},{"location":"io/Text.html#writing-text","text":"An SCollection[String] or SCollection of any class implementing toString can be written out to a newline-delimited text file via saveAsTextFile. An optional header and footer parameter can be provided.\nimport com.spotify.scio._\nimport com.spotify.scio.avro._\nimport com.spotify.scio.values.SCollection\n\nval elements: SCollection[String] = ???\nelements.saveAsTextFile(\n  \"gs://<output-path>\", \n  header=Some(\"header\"), \n  footer=Some(\"footer\")\n)","title":"Writing text"},{"location":"examples.html","text":"","title":""},{"location":"Scio-Unit-Tests.html","text":"","title":"Testing"},{"location":"Scio-Unit-Tests.html#testing","text":"To write Scio unit tests you will need to add the following dependency to your build.sbt\nlibraryDependencies ++= Seq(\n  // .......\n  \"com.spotify\" %% \"scio-test-core\" % scioVersion % Test,\n  // .......\n)\nTo run the test, you can run the following commands. You can skip the first two lines if you already ran them and are still in the sbt shell.\n$ sbt\n> project scio-examples\n> test\nClick on this link for more Scala testing tasks.","title":"Testing"},{"location":"Scio-Unit-Tests.html#test-entire-pipeline","text":"We will use the WordCountTest to explain how Scio tests work. WordCount is the pipeline under test. Full example code for WordCountTest and other test examples can be found here.\nLet’s walk through the details of the test: The test class should extend the PipelineSpec which is a trait for unit testing pipelines.\nThe inData variable holds the input data for your test and the expected variable contains the expected results after your pipeline processes the inData. The WordCount pipeline counts the occurrence of each word, the given the input data , we should expect a count of a=3, b=3, c=1 etc\nval inData = Seq(\"a b c d e\", \"a b a b\", \"\")\n val expected = Seq(\"a: 3\", \"b: 3\", \"c: 1\", \"d: 1\", \"e: 1\")\nUsing JobTest, you can test the entire pipeline. Specify the type of the class under test, in this case it is com.spotify.scio.examples.WordCount.type . The args function takes the list of command line arguments passed to the main function of WordCount. The WordCount’s main function expects input and output arguments passed to it.\ncopysource\"WordCount\" should \"work\" in {\n  JobTest[com.spotify.scio.examples.WordCount.type]\n    .args(\"--input=in.txt\", \"--output=out.txt\")\n    .input(TextIO(\"in.txt\"), inData)\n    .output(TextIO(\"out.txt\"))(coll => coll should containInAnyOrder(expected))\n    .run()\n}\nThe input function injects your input test data. Note that the TestIO[T] should match the input source used in the pipeline e.g. TextIO for sc.textFile, AvroIO for sc.avro. The TextIO id (“in.txt”) should match the one specified in the args.\nThe output function evaluates the output of the pipeline using the provided assertion from the SCollectionMatchers. More info on SCollectionMatchers can be found here. In this example, we are asserting that the output of the pipeline should contain an SCollection with elements that in the expected variable in any order. Also, note that the TestIO[T] should match the output used in the pipeline e.g. TextIO for sc.saveAsTextFile\nThe run function will run the pipeline.","title":"Test entire pipeline"},{"location":"Scio-Unit-Tests.html#test-for-pipeline-with-sideinput","text":"We will use the SideInputJoinExamples test in JoinExamplesTest to illustrate how to write a test for pipelines with sideinputs. The SideInputJoinExamples pipeline has two input sources, one for eventsInfo and the other for countryInfo. CountryInfo is used as a sideinput to join with eventInfo.\nSince we have two input sources, we have to specify both in the JobTest. Note that the injected data type should match one expected by the sink.\ncopysource\"SideInputJoinExamples\" should \"work\" in {\n  JobTest[com.spotify.scio.examples.cookbook.SideInputJoinExamples.type]\n    .args(\"--output=out.txt\")\n    .input(BigQueryIO(ExampleData.EVENT_TABLE), eventData)\n    .input(BigQueryIO(ExampleData.COUNTRY_TABLE), countryData)\n    .output(TextIO(\"out.txt\"))(coll => coll should containInAnyOrder(expected))\n    .run()\n}","title":"Test for pipeline with sideinput"},{"location":"Scio-Unit-Tests.html#test-for-pipeline-with-sideoutput","text":"SideInOutExampleTest shows an example of how to test pipelines with sideoutputs. Each sideoutput is evaluated using the output function. The ids for TextIO e.g. “out1.txt” should match the ones specified in the args.\ncopysourceval inData: Seq[String] = Seq(\"The quick brown fox jumps over the lazy dog.\")\n\n\"SideInOutExample\" should \"work\" in {\n  JobTest[SideInOutExample.type]\n    .args(\n      \"--input=in.txt\",\n      \"--stopWords=stop.txt\",\n      \"--output1=out1.txt\",\n      \"--output2=out2.txt\",\n      \"--output3=out3.txt\",\n      \"--output4=out4.txt\"\n    )\n    .input(TextIO(\"in.txt\"), inData)\n    .input(TextIO(\"stop.txt\"), Seq(\"the\"))\n    .output(TextIO(\"out1.txt\"))(coll => coll should containInAnyOrder(Seq.empty[String]))\n    .output(TextIO(\"out2.txt\"))(coll => coll should containInAnyOrder(Seq.empty[String]))\n    .output(TextIO(\"out3.txt\"))(coll => coll should containInAnyOrder(Seq(\"dog: 1\", \"fox: 1\")))\n    .output(TextIO(\"out4.txt\")) {\n      _ should containInAnyOrder(Seq(\"brown: 1\", \"jumps: 1\", \"lazy: 1\", \"over: 1\", \"quick: 1\"))\n    }\n    .run()\n}","title":"Test for pipeline with sideoutput"},{"location":"Scio-Unit-Tests.html#test-partial-pipeline","text":"To test a section of a pipeline, use runWithContext. The TriggerExample.extractFlowInfo test in TriggerExampleTest tests only the extractFlowInfo part of the pipeline.\nThe data variable hold the test data and sc.parallelize will transform the input Iterable to an SCollection of strings. TriggerExample.extractFlowInfo will be executed using the ScioContext and you can then specify assertions against the result of the pipeline.\ncopysource\"TriggerExample.extractFlowInfo\" should \"work\" in {\n  val data = Seq(\n    \"01/01/2010 00:00:00,1108302,94,E,ML,36,100,29,0.0065,66,9,1,0.001,74.8,1,9,3,0.0028,71,1,9,\"\n      + \"12,0.0099,67.4,1,9,13,0.0121,99.0,1,,,,,0,,,,,0,,,,,0,,,,,0\",\n    \"01/01/2010 00:00:00,\"\n      + \"1100333,5,N,FR,9,0,39,,,9,,,,0,,,,,0,,,,,0,,,,,0,,,,,0,,,,,0,,,,,0,,,,\"\n  )\n  runWithContext { sc =>\n    val r = TriggerExample.extractFlowInfo(sc.parallelize(data))\n    r should haveSize(1)\n    r should containSingleValue((\"94\", 29))\n  }\n}\nWhen your pipeline section contains input and/or output, you can also create an anonymous JobTest to inject the test data.\nIf we have the following pipeline section:\ncopysourcedef pipeline(sc: ScioContext, input: String, output: String): Unit = {\n  sc.textFile(input)\n    .map(_ + \"X\")\n    .saveAsTextFile(output)\n}\nIt can be tested with:\ncopysourceJobTest(pipeline(_, \"in.txt\", \"out.txt\"))\n  .input(TextIO(\"in.txt\"), Seq(\"a\", \"b\", \"c\"))\n  .output(TextIO(\"out.txt\"))(_ should containInAnyOrder(Seq(\"aX\", \"bX\", \"cX\")))\n  .run()","title":"Test partial pipeline"},{"location":"Scio-Unit-Tests.html#test-for-pipeline-with-windowing","text":"We will use the LeaderBoardTest to explain how to test Windowing in Scio. The full example code is found here. LeaderBoardTest also extends PipelineSpec. The function under test is the LeaderBoard.calculateTeamScores. This function calculates teams scores within a fixed window with the following the window options:\nCalculate the scores every time the window ends Calculate an early/“speculative” result from partial data, 5 minutes after the first element in our window is processed (withEarlyFiring) Accept late entries (and recalculates based on them) only if they arrive within the allowedLateness duration.\nIn this test, we are testing calculateTeamScores for when all of the elements arrive on time, i.e. before the watermark.\nFirst, we have to create an input stream representing an unbounded SCollection of type GameActionInfo using the testStreamOf. Each element is assigned a timestamp representing when each event occurred. In the code snippet above, we start at epoch equal zero, by setting watermark to 0 in the advanceWatermarkTo.\nWe add GameActionInfo elements with varying timestamps, and we advanced the watermark to 3 minutes. At this point, all elements are on time because they came before the watermark advances to 3 minutes.\ncopysourceval stream = testStreamOf[GameActionInfo]\n  // Start at the epoch\n  .advanceWatermarkTo(baseTime)\n  // add some elements ahead of the watermark\n  .addElements(\n    event(blueOne, 3, Duration.standardSeconds(3)),\n    event(blueOne, 2, Duration.standardMinutes(1)),\n    event(redTwo, 3, Duration.standardSeconds(22)),\n    event(blueTwo, 5, Duration.standardSeconds(3))\n  )\nWe then more GameActionInfo elements and advance the watermark to infinity by calling the advanceWatermarkToInfinity. Similarly, these elements are also on time because the watermark is infinity.\ncopysource// The watermark advances slightly, but not past the end of the window\n.advanceWatermarkTo(baseTime.plus(Duration.standardMinutes(3)))\n.addElements(\n  event(redOne, 1, Duration.standardMinutes(4)),\n  event(blueOne, 2, Duration.standardSeconds(270))\n)\n// The window should close and emit an ON_TIME pane\n.advanceWatermarkToInfinity\nTo run the test, we use the runWithContext, this will run calculateTeamScores using the ScioContext. In calculateTeamScores, we pass the SCollection we created above using testStreamOf. The IntervalWindow specifies the window for which we want to assert the SCollection of elements created by calculateTeamScores. We want to assert that elements with initial window of 0 to 20 minutes were on time. Next we assert, using inOnTimePane that the SCollection elements are equal to the expected sums.\ncopysourcerunWithContext { sc =>\n  val teamScores =\n    LeaderBoard.calculateTeamScores(sc.testStream(stream), teamWindowDuration, allowedLateness)\n\n  val window = new IntervalWindow(baseTime, teamWindowDuration)\n  teamScores should inOnTimePane(window) {\n    containInAnyOrder(Seq((blueOne.team, 12), (redOne.team, 4)))\n  }\n}\nScio provides more SCollection assertions such as inWindow, inCombinedNonLatePanes, inFinalPane, and inOnlyPane. You can find the full list here. More information on testing unbounded pipelines can be found here.","title":"Test for pipeline with windowing"},{"location":"Scio-Unit-Tests.html#test-with-transform-overrides","text":"Scio provides a method to replace arbitrary named PTransforms in a test context; this is primarily useful for mocking requests to external services.\nIn this example, the GuavaLookupDoFn stands in for a transform that contacts an external service. A ParDo PTransform is created from the DoFn (ParDo.of), then applied to the pipeline (applyTransform) with a unique name (myTransform).\ncopysourcesc.textFile(args(\"input\"))\n  .map(_.toInt)\n  .applyTransform(\"myTransform\", ParDo.of(new GuavaLookupDoFn))\n  .map((i: KV[Int, BaseAsyncLookupDoFn.Try[String]]) => i.getValue.get())\n  .saveAsTextFile(args(\"output\"))\nIn a JobTest, a PTransformOverride can be passed to the transformOverride method to replace transforms in the original pipeline. Scio provides convenience methods for constructing PTransformOverrides in the com.spotify.scio.testing.TransformOverride object. Continuing the example above, TransformOverride.ofAsyncLookup can be used to map static mock data into the expected output format for the transform, here KV[Int, BaseAsyncLookupDoFn.Try[String]].\ncopysource.transformOverride(\n  TransformOverride.ofAsyncLookup[Int, String](\n    \"myTransform\",\n    Map(1 -> \"10\", 2 -> \"20\", 3 -> \"30\")\n  )\n)\nIt is also possible to provide a function rather than a static map:\ncopysourceTransformOverride.ofAsyncLookup[Int, String](\n  \"myTransform\",\n  (i: Int) => s\"${i * 10}\"\n)\nIn a scenario when the PTransform’s output is generating more elements than input, e.g. there is a flatmap inside the transform:\ncopysource.withName(\"myTransform\")\n.transform { c: SCollection[Int] =>\n  c.applyTransform(ParDo.of(new GuavaLookupDoFn))\n    .flatMap(_.getValue.get())\n    .map(_.toString)\n}\nThe transform can be mocked by one of the flavours of ofIter method to map each element to an Iterable[U]:\ncopysourceTransformOverride.ofIter[Int, String](\n  \"myTransform\",\n  Map(1 -> Seq(\"10\"), 2 -> Seq(\"20\", \"21\"), 3 -> Seq())\n)\nor similarly provide a function rather than a static map:\ncopysourceTransformOverride.ofIter[Int, String](\n  \"myTransform\",\n  // map fn equal to: Map(1 -> Seq(), 2 -> Seq(\"1\"), 3 -> Seq(\"1\", \"2\")}\n  (i: Int) => { (1 until i).map(String.valueOf(_)) }\n)\nTransformOverride.of overrides transforms of type PTransform[PCollection[T], PCollection[U]] as in the case of BaseAsyncDoFn subclasses. TransformOverride.ofKV overrides transforms of type PTransform[PCollection[T], PCollection[KV[T, U]]].\nSources can also be overridden with TransformOverride.ofSource. For example, this source:\ncopysourcesc.withName(\"ReadInput\")\n  .textFile(args(\"input\"))\nCan be overridden with static mock data:\ncopysourceTransformOverride.ofSource[String](\"ReadInput\", List(\"10\", \"11\", \"12\"))\nIt is alo possible to override a named PTransform during partial pipeline testing with runWithOverrides.\ncopysourcerunWithOverrides(\n  TransformOverride.of(\"multiply\", (v: Int) => v * v),\n  TransformOverride.ofIter(\"append\", (v: Int) => Seq(v + \"c\", v + \"d\"))\n) { sc =>\n  val result = sc\n    .parallelize(Seq(1, 2, 3))\n    .withName(\"multiply\")\n    .map(_ * 2)\n    .withName(\"append\")\n    .flatMap(v => Seq(v + \"a\", v + \"b\"))\n\n  result should containInAnyOrder(Seq(\"1c\", \"1d\", \"4c\", \"4d\", \"9c\", \"9d\"))\n}\nDue to type erasure it is possible to provide the incorrect types for the transform and the error will not be caught until runtime.\nIf you’ve specified the incorrect input type, scio will attempt to detect the error and throw an IllegalArgumentException, which will be wrapped in a PipelineExecutionException at runtime:\norg.apache.beam.sdk.Pipeline$PipelineExecutionException:\n  java.lang.IllegalArgumentException:\n    Input for override transform myTransform does not match pipeline transform. Expected: class java.lang.Integer Found: class java.lang.String\nIf you’ve specified the incorrect output type, there is little scio can do to detect the error. Typically, a coder will throw a ClassCastException whose message will contain the correct type:\norg.apache.beam.sdk.Pipeline$PipelineExecutionException:\n  java.lang.ClassCastException:\n    java.lang.String cannot be cast to java.lang.Integer","title":"Test with transform overrides"},{"location":"internals/index.html","text":"","title":"Internals"},{"location":"internals/index.html#internals","text":"Coder Typeclass Kryo OverrideTypeProvider ScioIO","title":"Internals"},{"location":"internals/Coders.html","text":"","title":"Coder Typeclass"},{"location":"internals/Coders.html#coder-typeclass","text":"","title":"Coder Typeclass"},{"location":"internals/Coders.html#coder-in-apache-beam","text":"As per Beam’s documentation\nWhen Beam runners execute your pipeline, they often need to materialize the intermediate data in your PCollections, which requires converting elements to and from byte strings. The Beam SDKs use objects called Coders to describe how the elements of a given PCollection may be encoded and decoded.\nFor the most part, coders are used when Beam transfer intermediate data between workers over the network. They may also be used by beam to test instances for equality. Anytime you create an SCollection[T], Beam needs to know how to go from an instance of T to an array of bytes, and from that array of bytes to an instance of T.\nThe Beam SDK defines a class called Coder that roughly looks like this:\npublic abstract class Coder<T> implements Serializable {\n  public abstract void encode(T value, OutputStream outStream);\n  public abstract T decode(InputStream inStream);\n}\nBeam provides built-in Coders for various basic Java types (Integer, Long, Double, etc.). But anytime you create a new class, and that class is used in an SCollection, a beam coder needs to be provided.\nimport com.spotify.scio.values.SCollection\n\ncase class Foo(x: Int, s: String)\ndef coll: SCollection[Foo] = ??? // Beam will need an org.apache.beam.sdk.coders.Coder[Foo]","title":"Coder in Apache Beam"},{"location":"internals/Coders.html#when-are-coder-used-","text":"","title":"When are Coder used ?"},{"location":"internals/Coders.html#shuffling-data","text":"Whenever intermediate data is shuffled, Beam will need to serialize and deserialize that data to transfer it between workers. In Scio any *byKey (groupByKey, reduceByKey, etc.) transform will trigger a shuffle.","title":"Shuffling data"},{"location":"internals/Coders.html#cluster-scaling-up-and-down","text":"When the runner scales up and down your cluster size, data needs to be redistributed between workers. Beam therefore needs to transfer data over the network, which means serializing and deserializing it by using Coder.","title":"Cluster scaling up and down"},{"location":"internals/Coders.html#groupbykey","text":"Grouping by key uses Coder for two reasons: First, as we have already seen, GBK triggers a shuffle, and therefore go through a serialization / deserialization cycle.\nSecond, grouping elements by key means that Beam needs to compare them and be able to decide whether two instances are equal. In Beam, and in the context of a groupByKey (or any *byKey operation), the equality of keys is tested by comparing their serialized form.\nLet’s say we have defined a class Identifier and we use it as the key in a groupByKey transform:\nimport com.spotify.scio.values.SCollection\n\nval coll: SCollection[(Identifier, Foo)] = ???\nval grouped: SCollection[(Identifier, Iterable[Foo])] = coll.groupByKey()\nTo decide whether two instances of Identifier id1 and id2 are equal, Beam will compare them after serialization. For example:\n// (pseudo-code)\ncoder.encode(id1) // 00010011\ncoder.encode(id2) // 00010010\n// -> i1 and i2 are NOT equal\nWhen they are used to test equality, coders are required to be deterministic. If a non-deterministic Coder is used to test equality, an exception is thrown:\nimport com.spotify.scio.ScioContext\nval sc = ScioContext.forTest()\nval grouped =\n  sc.parallelize(List((1.2, \"foo\"), (42.5, \"bar\"), (1.2, \"baz\")))\n    .groupByKey\nsc.run()","title":"GroupByKey"},{"location":"internals/Coders.html#scio-coder-vs-beam-coder","text":"Both Scio and Beam define a class called Coder. For the most part when writing a job, you will be interacting with com.spotify.scio.coders.Coder.\nScio Coder and its implementations simply form an ADT where each implementation is a building block that covers one of the possible cases:\nBeam: a simple wrapper around a Beam Coder Singleton: A coder for a static object. It is for example used to serialize Unit. Disjunction: Represent a Coder that makes a choice between different possible implementations. It is for example used to serialize ADTs and Either Record: A Coder for record-like structures like case classes and tuples. Transform: A Coder implemented by “transforming” the encoded/decoded value of another Coder. CoderTransform: A Coder implemented by “transforming” a Beam Coder to a new Coder. Fallback: A default Coder. Used when there is no better option.\nThere is also a “special” coder called KVCoder. It is a specific coder for Key-Value pairs. Internally Beam treats KV differently from other types so Scio needs to do the same.\nIt is important to note that Scio’s coders are only representations of those cases but do not actually implement any serialization logic. Before the job starts, those coders will be materialized, meaning they will be converted to instances of org.apache.beam.sdk.coders.Coder. Thanks to this technique, Scio can dynamically change the behavior of coders depending on the execution context. For example coders may handle nullable values differently depending on options passed to the job.\norg.apache.beam.sdk.coders.Coder instances on the other hand are the actual implementations of serialization and deserialization logic. Among other thing, each instance of org.apache.beam.sdk.coders.Coder[T] defines two methods:\nclass ExampleCoder extends org.apache.beam.sdk.coders.Coder[Example] {\n  def decode(inStream: InputStream): Example = ???\n  def encode(value: Example, outStream: OutputStream): Unit = ???\n}","title":"Scio Coder vs Beam Coder"},{"location":"internals/Coders.html#how-scio-picks-a-coder-instance","text":"Every method in Scio that may potentially need to serialize and deserialize data takes an implicit Coder argument. See for example the definition of SCollection.map:\ndef map[U: Coder](f: T => U): SCollection[U] = // implementation\n//           ↑\n//    Implicit Coder[U] lookup\nThis type signature means the following: The method map (defined in SCollection[T]), applied to a function from T to U, will return an SCollection[U]. On top of that this method has a context bound U: Coder, meaning a Coder[U] needs to be available in the implicit context.\nSo at compile time the Scala compiler will try to find an appropriate Coder. If it fails to find one, the compilation will fail.\nWhen the compiler looks for an implicit for a concrete type Foo, three cases can happen:","title":"How Scio picks a Coder instance"},{"location":"internals/Coders.html#there-exist-an-coder-foo-in-scope","text":"Scio comes with a number of implementation for common types like primitives (Int, Float, String, etc.), common Scala and Java types (Option, Either, List, etc.) and some types from the Beam API. In that case the available Coder will simply be used. It is also possible for the user (aka you) to provide an implementation. Here’s an example REPL session that demonstrate it:\nimport com.spotify.scio.coders._\nCoder[Int] // Try to find a Coder instance for Int\nHere the compiler just found a proper Coder for integers. Scio also provides Coders for commons collections types:\nCoder[List[String]] // Try to find a Coder instance for List[String]","title":"There exist an Coder[Foo] in scope"},{"location":"internals/Coders.html#no-coder-foo-is-available-but-the-compiler-can-derive-one","text":"For certain type (for example case classes with a public constructor), Scio can derive an inline Coder implementation at compile time. Note that it does not generate source code.\ncase class Demo(i: Int, s: String, xs: List[Double])\nCoder[Demo]\nsealed class hierarchy are also supported:\nsealed trait Top\nfinal case class TA(anInt: Int, aString: String) extends Top\nfinal case class TB(anDouble: Double) extends Top\nCoder[Top]","title":"No Coder[Foo] is available but the compiler can derive one"},{"location":"internals/Coders.html#no-coder-foo-is-available-and-the-compiler-can-not-derive-one","text":"Sometimes, no Coder instance can be found, and it’s impossible to automatically derive one. In that case, Scio can fallback to a Kryo coder for that type by importing com.spotify.scio.coders.kryo._. Note that it might negatively impact the performance of your job.\nIf the scalac flag -Xmacro-settings:show-coder-fallback=true is set, a warning message will be displayed at compile time. This message should help you keep track where the implicit kryo coder are used.\nWhile compiling the following example with -Xmacro-settings:show-coder-fallback=true\nimport com.spotify.scio.coders._\nimport com.spotify.scio.coders.kryo._\nval localeCoder = Coder[java.util.Locale]\nScalac will output:\nWarning: No implicit Coder found for the following type:\n\n  >> java.util.Locale\n\nusing Kryo fallback instead.\n\n\n  Scio will use a fallback Kryo coder instead.\n\n  If a type is not supported, consider implementing your own implicit Coder for this type.\n  It is recommended to declare this Coder in your class companion object:\n\n      object Locale {\n        import com.spotify.scio.coders.Coder\n        import org.apache.beam.sdk.coders.AtomicCoder\n\n        implicit def coderLocale: Coder[Locale] =\n          Coder.beam(new AtomicCoder[Locale] {\n            def decode(in: InputStream): Locale = ???\n            def encode(ts: Locale, out: OutputStream): Unit = ???\n          })\n      }\n\n  If you do want to use a Kryo coder, be explicit about it:\n\n      implicit def coderLocale: Coder[Locale] = Coder.kryo[Locale]\n\n  Additional info at:\n  - https://spotify.github.io/scio/internals/Coders\nIn this example, the compiler could not find a proper instance of Coder[Locale], and suggest you implement one yourself.\nNote that this message is not limited to direct invocation of fallback. For example, if you declare a case class that uses Locale internally, the compiler will show the same warning:\nimport com.spotify.scio.coders.Coder\nimport com.spotify.scio.coders.kryo._\ncase class Demo2(i: Int, s: String, xs: List[java.util.Locale])\nval demoCoder = Coder[Demo2]\nInt, String and List all have predefined Coder instances but Locale does not. The serialization of Locale instances is delegated to Kryo.","title":"No Coder[Foo] is available and the compiler can not derive one"},{"location":"internals/Coders.html#compiler-flags-and-warnings","text":"When Scio automatically derives a Coder for a given type, it may issue warnings about potential performance issues. For example, the default implementation of Coder[GenericRecord] is very inefficient and Scio will issue a message if it is used:\n[info] Using a fallback coder for Avro's GenericRecord is discouraged as it is VERY inefficient.\n[info] It is highly recommended to define a proper Coder[GenericRecord] using:\n[info]\n[info]   Coder.avroGenericRecordCoder(schema)\nIt is also possible to pass a flag to the compiler to issue a message anytime the fallback coder is used:\n[info]  Warning: No implicit Coder found for the following type:\n[info]\n[info]    >> com.google.common.collect.SetMultimap[String,String]\n[info]\n[info]  using Kryo fallback instead.\nTo activate this feature, pass -Xmacro-settings:show-coder-fallback=true to scalac in your build file:\nscalacOptions += \"-Xmacro-settings:show-coder-fallback=true\"","title":"Compiler flags and warnings"},{"location":"internals/Coders.html#how-to-build-a-custom-coder","text":"It is possible for the user to define their own Coder implementation. Scio provides builder functions in the Coder object. If you want to create a custom Coder, you should use one of the those three builder:\nCoder.beam: Create a Scio Coder that simply wraps a Beam implementation. For example: import com.spotify.scio.coders._\nimport org.apache.beam.sdk.coders.DoubleCoder\nimplicit def doubleCoder = Coder.beam(DoubleCoder.of())\n Coder.transform: Create a Coder for a type B by transforming the Beam implementation for a type A. Usually useful for Coder that depend on another Coder: import java.io.{InputStream, OutputStream}\nimport org.apache.beam.sdk.coders.AtomicCoder\nclass ListCoder[T](bc: org.apache.beam.sdk.coders.Coder[T]) extends AtomicCoder[List[T]] {\n  override def encode(value: List[T], outStream: OutputStream): Unit = ???\n  override def decode(inStream: InputStream): List[T] = ???\n}\nimplicit def listCoder[T: Coder]: Coder[List[T]] = Coder.transform(Coder[T])(bc => Coder.beam(new ListCoder[T](bc)))\n Coder.xmap: Create a Coder for a type B by reusing a Coder[A]. xmap simply apply the provided function to convert B to A and back. See for example a possible Coder[Char] based on an existing Coder[Byte] implicit def charCoder: Coder[Char] = Coder.xmap(Coder[Byte])(_.toChar, _.toByte)","title":"How to build a custom Coder"},{"location":"internals/Coders.html#serialization-","text":"Coder instances have to be Serializable. You do not need to extend Serializable explicitly since the Coder trait already does, but you do need to make sure that your implementation is not referencing a non-serializable object in any way.\nNote that in test mode (when you use JobTest), Scio will make sure that all the coders used in the job are serializable.","title":"⚠ Serialization ⚠"},{"location":"internals/Coders.html#testing-custom-coders","text":"Scio provides a few assertions specific to coders. See CoderAssertions.","title":"Testing custom coders"},{"location":"internals/Coders.html#null-values-support","text":"By default, and for performance reasons, Scio coders will expect the values to serialized to never be null.\nThis may cause the following exception to be thrown:\norg.apache.beam.sdk.Pipeline$PipelineExecutionException: java.lang.RuntimeException:\n  Exception while trying to `encode` an instance of scala.Tuple3:\n  Can't encode field _3 value null\nThere are 2 ways to fix this issue:\nRecommended: Replace null values by Option in you job code. NOT recommended: pass the following flag when you start the job: --nullableCoders=true If you pass this option, Scio will assume that every value are potentially null. This include every single fields in your case classes and each every elements in collections. It introduces overhead and may slow down your job execution.","title":"Null values support"},{"location":"internals/Coders.html#upgrading-to-v0-7-0-or-above-migrating-to-static-coder","text":"Migrating to Scio 0.7.x from an older version is likely to break a few things at compile time in your project. See the complete v0.7.0 Migration Guide for more information.","title":"Upgrading to v0.7.0 or above: Migrating to static coder"},{"location":"internals/Kryo.html","text":"","title":"Kryo"},{"location":"internals/Kryo.html#kryo","text":"Scio uses a framework called Kryo to serialize objects that need to be shuffled between workers. Network throughput can easily become a bottleneck for your pipeline, so optimizing serialization is an easy win. If you use Dataflow’s shuffler service, you pay per GB shuffled so you can save money even if shuffling is not a bottleneck.\nBy registering your classes at compile time, Kryo can serialize far more efficiently that doing it on the fly. The default serializer includes the full classpath of each class you serialize. By pre-registering the classes you want to serialize, Kryo can replace this with an int as identifier. For some pipelines we observed a 30-40% reduction of bytes shuffled.","title":"Kryo"},{"location":"internals/Kryo.html#how-to-enable-kryoregistrar","text":"Add the following class. You can rename it, but its name has to end in KryoRegistrar. Also make sure that the Macro Paradise plugin is enabled for your project.\nimport com.spotify.scio.coders.KryoRegistrar\nimport com.twitter.chill._\n\n@KryoRegistrar\nclass MyKryoRegistrar extends IKryoRegistrar {\n  override def apply(k: Kryo): Unit = {\n    // Take care of common Scala classes; tuples, Enumerations, ...\n    val reg = new AllScalaRegistrar\n    reg(k)\n\n    k.registerClasses(List(\n      // All classes that might be shuffled, e.g.:\n      classOf[foo.bar.MyClass],\n\n      // Class that takes type parameters:\n      classOf[_root_.java.util.ArrayList[_]],\n      // But you can also explicitly do:\n      classOf[Array[Byte]],\n\n      // Private class; cannot use classOf:\n      Class.forName(\"com.spotify.scio.extra.sparkey.LocalSparkeyUri\"),\n\n      // Some common Scala objects\n      None.getClass,\n      Nil.getClass\n    ))\n  }\n}\nNote: since Dataflow may shuffle data at any point, you not only have to include classes that are explicitly shuffled (through join or groupBy), but also those returned by map, flatMap, etc.","title":"How to enable KryoRegistrar"},{"location":"internals/Kryo.html#verifying-it-works","text":"You can add the following class to your test folder; it will enforce registration of classes during your tests. It only works if you actually run your job in tests, so be sure to include a JobTest or so for each pipeline you run.\nimport com.esotericsoftware.kryo.Kryo\nimport com.spotify.scio.coders.KryoRegistrar\nimport com.twitter.chill.IKryoRegistrar\n\n/** Makes sure we don't forget to register encoders, enabled only in tests not to crash production. */\n@KryoRegistrar\nclass TestKryoRegistrar extends IKryoRegistrar {\n  def apply(k: Kryo): Unit = {\n    k.setRegistrationRequired(true)\n  }\n}\nIf you missed registering any classes, you’ll get an error that looks like this:\n[info]   java.lang.IllegalArgumentException: Class is not registered: org.apache.avro.generic.GenericData$Record\n[info] Note: To register this class use: kryo.register(org.apache.avro.generic.GenericData$Record.class);\nWhich you solve by adding classOf[GenericData.Record] or Class.forName(\"org.apache.avro.generic.GenericData$Record\") in MyKryoRegistrar.","title":"Verifying it works"},{"location":"internals/OverrideTypeProvider.html","text":"","title":"OverrideTypeProvider"},{"location":"internals/OverrideTypeProvider.html#overridetypeprovider","text":"The OverrideTypeProvider trait allows the user to provide custom mappings from BigQuery types to custom Scala types.\nThis can be used for a number of use cases:\nUsing higher level types in Scio in order to be explicit about what your data is Custom code can be run when you create new objects to do things like data validation or simple transformation\nThe methods in the Scala trait allow you to inspect the incoming types from BigQuery and decide if you’d like to provide an alternative type mapping to your own custom type. You also must tell Scio how to convert your types back into BigQuery data types.","title":"OverrideTypeProvider"},{"location":"internals/OverrideTypeProvider.html#setup","text":"Once you implement the OverrideTypeProvider with your own custom types you can supply it to the OverrideTypeProviderFinder by specifying a JVM System property as below.\nSystem.setProperty(\n  \"override.type.provider\",\n  \"com.spotify.scio.bigquery.validation.SampleOverrideTypeProvider\")\nSince this feature uses Scala macros you must do this at initialization time. One easy way to do this is in the build.sbt file for your project. This would look like below.\ninitialize in Test ~= { _ => System.setProperty(\n  \"override.type.provider\",\n  \"com.spotify.scio.bigquery.validation.SampleOverrideTypeProvider\")\n}\nCurrently only one OverrideTypeProvider is allowed per sbt project.\nThis provider is loaded using Reflection at macro expansion time and at runtime as well.\nIf this System property isn’t specified then Scio falls back to the normal default behavior.","title":"Setup"},{"location":"internals/OverrideTypeProvider.html#implementation","text":"Custom implementations of the OverrideTypeProvider should implement the methods as described below.\ndef shouldOverrideType(tfs: TableFieldSchema)\nThis is the first point of entry and is called when we use macros to create case classes for fromQuery, fromSchema, and fromTable.\ndef getScalaType(c: blackbox.Context)(tfs: TableFieldSchema)\nThis is called when the above shouldOverrideType returns true. Expected return value is a c.Tree representing the Scala type you’d like to use for this mapping.\ndef shouldOverrideType(c: blackbox.Context)(tpe: c.Type)\nThis is called when we do conversions to and from a TableRow internally and your generated case class.\ndef createInstance(c: blackbox.Context)(tpe: c.Type, tree: c.Tree)\nThis is called when the above shouldOverrideType returns true. Expected return value is a c.Tree representing how to create a new instance of your custom Scala type.\ndef shouldOverrideType(tpe: Type)\nThis is called at runtime when we do any operations on the schema directly.\ndef getBigQueryType(tpe: Type)\nThis is called when the above shouldOverrideType returns true. It should return the String representation for the BigQuery column type for your class and field now.\ndef initializeToTable(c: blackbox.Context)(modifiers: c.universe.Modifiers,\n                                           variableName: c.universe.TermName,\n                                           tpe: c.universe.Tree)\nThis is called once per field when we extend the case classes for toTable examples.","title":"Implementation"},{"location":"internals/ScioIO.html","text":"","title":"ScioIO"},{"location":"internals/ScioIO.html#scioio","text":"Scio 0.7.0 introduces a new ScioIO[T] trait to simplify IO implementation and stubbing in JobTest. This page lists some major changes to this new API.","title":"ScioIO"},{"location":"internals/ScioIO.html#dependencies","text":"Avro and BigQuery logic was decoupled from scio-core as part of the refactor.\nBefore 0.7.0 scio-core depends on scio-avro and scio-bigquery ScioContext and SCollection[T] include Avro, object, Protobuf and BigQuery IO methods out of the box After 0.7.0 scio-core no longer depends on scio-avro and scio-bigquery Import com.spotify.scio.avro._ to get Avro, object, Protobuf IO methods on ScioContext and SCollection[T] Import com.spotify.scio.bigquery._ to get BigQuery IO methods on ScioContext and SCollection[T]","title":"Dependencies"},{"location":"internals/ScioIO.html#scioio-t-for-jobtest","text":"As part of the refactor TestIO[T] was replaced by ScioIO[T] for JobTest. Some of them were moved to different packages for consistency but most test code should work with minor import changes. Below is a list of ScioIO[T] implementations.\ncom.spotify.scio.avro AvroIO[T] ObjectFileIO[T] ProtobufIO[T] com.spotify.scio.bigquery BigQueryIO[T] TableRowJsonIO where T =:= TableRow com.spotify.scio.io DatastoreIO where T =:= Entity PubsubIO[T] TextIO where T =:= String CustomIO[T] for use with ScioContext#customInput and SCollection#customOutput com.spotify.scio.bigtable BigtableIO[T] where T =:= Row for input and T =:= Mutation for output This replaces BigtableInput and BigtableOutput com.spotify.scio.cassandra CassandraIO[T] com.spotify.scio.elasticsearch ElasticsearchIO[T] com.spotify.scio.extra.json JsonIO[T] com.spotify.scio.jdbc JdbcIO[T] com.spotify.scio.parquet.avro ParquetAvroIO[T] com.spotify.scio.spanner SpannerIO[T] com.spotify.scio.tensorflow TFRecordIO where T =:= Array[Byte] TFExampleIO where T =:= Example","title":"ScioIO[T] for JobTest"},{"location":"internals/ScioIO.html#using-scioio-t-directly","text":"2 methods, ScioContext#read and SCollection#write were added to leverage ScioIO[T] directly without needing the extra ScioContext#{textFile,AvroFile,...} and SCollection#saveAs{TextFile,AvroFile,...} syntactic sugar. See WordCountScioIO and WordCountScioIOTest for concrete examples.","title":"Using ScioIO[T] directly"},{"location":"extras/index.html","text":"","title":"Extras"},{"location":"extras/index.html#extras","text":"Algebird Annoy AsyncDoFn BigQueryAvro DistCache Fanout HyperLogLog MutableScalableBloomFilter Sorter Sort Merge Bucket Sparkey REPL Transforms Voyager","title":"Extras"},{"location":"extras/Algebird.html","text":"","title":"Algebird"},{"location":"extras/Algebird.html#algebird","text":"Algebird is Twitter’s abstract algebra library. It has a lot of reusable modules for parallel aggregation and approximation. One can use any Algebird Aggregator or Semigroup with: - aggregate and sum on SCollection[T] - aggregateByKey and sumByKey on SCollection[(K, V)]\nSee AlgebirdSpec.scala and Algebird wiki for more details. Also see these slides on semigroups.","title":"Algebird"},{"location":"extras/Algebird.html#algebird-in-repl","text":"scio> import com.twitter.algebird._\nscio> import com.twitter.algebird.CMSHasherImplicits._\nscio> val words = sc.textFile(\"README.md\").\n     | flatMap(_.split(\"[^a-zA-Z0-9]+\")).\n     | filter(_.nonEmpty).\n     | aggregate(CMS.aggregator[String](0.001, 1E-10, 1)).\n     | materialize\nscio> sc.run()\nscio> val cms = words.waitForResult().value.next\nscio> cms.frequency(\"scio\").estimate\nres2: Long = 19\n\nscio> // let's validate:\nscio> import sys.process._\nscio> \"grep -o scio README.md\"  #| \"wc -l\"!\n      19","title":"Algebird in REPL"},{"location":"extras/Annoy.html","text":"","title":"Annoy"},{"location":"extras/Annoy.html#annoy","text":"Scio integrates with Spotify’s Annoy, an approximate nearest neighbors library, via annoy-java and annoy4s.","title":"Annoy"},{"location":"extras/Annoy.html#write","text":"A keyed SCollection with Int keys and Array[Float] vector values can be saved with asAnnoy:\nimport com.spotify.scio.values.SCollection\nimport com.spotify.scio.extra.annoy._\n\nval metric: AnnoyMetric = ???\nval numDimensions: Int = ???\nval numTrees: Int = ???\nval itemVectors: SCollection[(Int, Array[Float])] = ???\nitemVectors.asAnnoy(\"gs://output-path\", metric, numDimensions, numTrees)","title":"Write"},{"location":"extras/Annoy.html#side-input","text":"An Annoy file can be read directly as a SideInput with annoySideInput:\nimport com.spotify.scio._\nimport com.spotify.scio.values.SideInput\nimport com.spotify.scio.extra.annoy._\n\nval sc: ScioContext = ???\n\nval metric: AnnoyMetric = ???\nval numDimensions: Int = ???\nval annoySI: SideInput[AnnoyReader] = sc.annoySideInput(\"gs://input-path\", metric, numDimensions)\nAlternatively, an SCollection can be converted directly to a SideInput with @scaladoc [asAnnoySideInput](com.spotify.scio.extra.annoy.package$$AnnoyPairSCollection#asAnnoySideInput(metric:com.spotify.scio.extra.annoy.package.AnnoyMetric,dim:Int):com.spotify.scio.values.SideInput[com.spotify.scio.extra.annoy.package.AnnoyReader]):\nimport com.spotify.scio.values.{SCollection, SideInput}\nimport com.spotify.scio.extra.annoy._\n\nval metric: AnnoyMetric = ???\nval numDimensions: Int = ???\nval numTrees: Int = ???\nval itemVectors: SCollection[(Int, Array[Float])] = ???\nval annoySI: SideInput[AnnoyReader] = itemVectors.asAnnoySideInput(metric, numDimensions, numTrees)\nAn AnnoyReader provides access to item vectors and their nearest neighbors:\nimport com.spotify.scio.values.{SCollection, SideInput}\nimport com.spotify.scio.extra.annoy._\n\nval annoySI: SideInput[AnnoyReader] = ???\nval elements: SCollection[Int] = ???\nelements\n  .withSideInputs(annoySI)\n  .map { case (element, ctx) =>\n    val annoyReader: AnnoyReader = ctx(annoySI)\n    val vec: Array[Float] = annoyReader.getItemVector(element)\n    element -> annoyReader.getNearest(vec, 1)\n  }","title":"Side Input"},{"location":"extras/AsyncDoFn.html","text":"","title":"AsyncDoFn"},{"location":"extras/AsyncDoFn.html#asyncdofn","text":"Scio’s BaseAsyncDoFn provides standard handling for sending asynchronous requests and capturing the responses for a bundle of pipeline elements. BaseAsyncDoFn is a subclass of DoFnWithResource which handles the creation and re-use of client classes. Scio provides several future-specific subclasses to choose from depending on the return type of the client:\nGuavaAsyncDoFn for clients that return Guava’s ListenableFuture JavaAsyncDoFn for clients that return CompletableFuture ScalaAsyncDoFn for clients that return a scala Future\nBaseAsyncDoFn will wait for all futures for all bundle elements to be returned before completing the bundle. A failure of any request for an item in the bundle will cause the entire bundle to be retried. Requests should therefore be idempotent.\nGiven this Guava-based mock client:\nimport com.google.common.util.concurrent.{ListenableFuture, Futures}\n\ncase class MyClient(value: String) {\n  def request(i: Int): ListenableFuture[String] = Futures.immediateFuture(s\"$value$i\")\n}\nFor client which returns a ListenableFuture, a custom DoFn can be defined using GuavaAsyncDoFn. Note the configured ResourceType, which will re-use the client for all threads on a worker, see ResourceType for more details.\nimport com.spotify.scio.transforms._\nimport com.spotify.scio.transforms.DoFnWithResource.ResourceType\nimport com.spotify.scio.values.SCollection\nimport org.apache.beam.sdk.transforms.ParDo\n\nclass MyDoFn() extends GuavaAsyncDoFn[Int, String, MyClient] {\n  override def getResourceType: ResourceType = ResourceType.PER_CLASS\n  override def createResource(): MyClient = MyClient(\"foo\")\n  override def processElement(input: Int): ListenableFuture[String] =\n    getResource.request(input)\n}\n\nval elements: SCollection[Int] = ???\nval result: SCollection[String] = elements.applyTransform(ParDo.of(new MyDoFn()))","title":"AsyncDoFn"},{"location":"extras/BigQueryAvro.html","text":"","title":"BigQueryAvro"},{"location":"extras/BigQueryAvro.html#bigqueryavro","text":"Scio provides support for converting Avro schemas to BigQuery TableSchemas and Avro SpecificRecords to a BigQuery TableRows.\nimport com.spotify.scio.extra.bigquery.AvroConverters\nimport org.apache.avro.specific.SpecificRecord\nimport com.google.api.services.bigquery.model.{TableFieldSchema, TableSchema, TableRow}\n\nval myAvroInstance: SpecificRecord = ???\nval bqSchema: TableSchema = AvroConverters.toTableSchema(myAvroInstance.getSchema)\nval bqRow: TableRow = AvroConverters.toTableRow(myAvroInstance)","title":"BigQueryAvro"},{"location":"extras/DistCache.html","text":"","title":"DistCache"},{"location":"extras/DistCache.html#distcache","text":"Scio supports a distributed cache, DistCache, that is similar to Hadoop’s.\nA set of one or more paths that back the DistCache are lazily downloaded by all workers, then passed through a user-defined initialization function initFn to be deserialized into an in-memory representation that can be used by all threads on that worker.\nimport com.spotify.scio._\nimport com.spotify.scio.values.SCollection\nimport org.joda.time.Instant\nimport java.io.File\n\nval sc: ScioContext = ???\nval uri: String = ???\ndef parseFn(file: File): Map[String, String] = ???\n\nval dc = sc.distCache(uri) { file => parseFn(file) }\n\nval elements: SCollection[String] = ???\nelements.flatMap { e =>\n  val optResult = dc().get(e)\n  optResult\n}\nSee DistCacheExample.scala.","title":"DistCache"},{"location":"extras/Fanout.html","text":"","title":"Fanout"},{"location":"extras/Fanout.html#fanout","text":"Scio ships with two SCollection variants that provide fanout over aggregations where an interim aggregation is performed before the final aggregation is computed. The interim step pairs the data to be aggregated with a synthetic key, then aggregates within this artificial keyspace before passing the partial aggregations on to the final aggregation step. The interim step requires an additional shuffle but can make the aggregation more parallelizable and reduces the impact of a hot key.\nThe aggregate, combine, fold, reduce, sum transforms and their keyed variants are supported.","title":"Fanout"},{"location":"extras/Fanout.html#withfanout","text":"withFanout aggregates over the number of synthetic keys specified by the fanout argument:\nimport com.spotify.scio._\nimport com.spotify.scio.values.SCollection\n\nval elements: SCollection[Int] = ???\nval result: SCollection[Int] = elements.withFanout(fanout = 10).sum","title":"WithFanout"},{"location":"extras/Fanout.html#withhotkeyfanout","text":"For hot keys, two variants allow a user to specify either a static fanout via an integer hotKeyFanout argument to withHotKeyFanout, or a dynamic per-key fanout via a function K => Int argument, also called hotKeyFanout to withHotKeyFanout:\nimport com.spotify.scio._\nimport com.spotify.scio.values.SCollection\n\nval elements: SCollection[(String, Int)] = ???\nval staticResult: SCollection[(String, Int)] = elements.withHotKeyFanout(hotKeyFanout = 10).sumByKey\nval dynamicResult: SCollection[(String, Int)] = elements\n  .withHotKeyFanout(hotKeyFanout = s => s.length % 10)\n  .sumByKey","title":"WithHotKeyFanout"},{"location":"extras/HyperLogLog.html","text":"","title":"HyperLogLog"},{"location":"extras/HyperLogLog.html#hyperloglog","text":"HyperLogLog is an algorithm to estimate the cardinality of a large dataset. Counting distinct in a large dataset requires linear space. Hll algorithm approximate the cardinality with a small memory footprint.\nHyperLogLog++ is an improved version of HyperLogLog algorithms presented by Google. It more accurately estimates distinct count in large and small data streams.\nHyperLogLog++ algorithm has been integrated with Apache Beam with help of ZetaSketch library, which is comply with Google Cloud BigQuery sketches. More about Apache beam integration can find here.","title":"HyperLogLog"},{"location":"extras/HyperLogLog.html#hyperloglog-integration-","text":"Scio distinct count API has been extended to support HyperLogLog algorithms using ApproxDistinctCount Interface. scio-extra module provide two different implementations of this interface,\ncom.spotify.scio.extra.hll.sketching.SketchHllPlusPlus com.spotify.scio.extra.hll.zetasketch.ZetaSketchHllPlusPlus\nSketchHllPlusPlus provide HyperLogLog++ implementation based on Addthis’ Stream-lib library while ZetaSketchHllPlusPlus provide implementation basedon ZetaSketch.\nFollowing is how you can use the new API.\nIn this example, we count the distinct value of a given type using SketchHllPlusPlus implementation.\nimport com.spotify.scio.values.SCollection\nimport com.spotify.scio.extra.hll.sketching.SketchHllPlusPlus\n\ndef input[T]: SCollection[T] = ???\n\ndef distinctCount: SCollection[Long] =\n        input\n        .countApproxDistinct(new SketchHllPlusPlus(15, 20))\nSame thing can be done using ZetaSketchHllPlusPlus too, but this only support Int, Long, String and Array[Byte] types only. Meaning you can only call this on SCollection of about supported types.\nimport com.spotify.scio.extra.hll.zetasketch.ZetaSketchHllPlusPlus\n\nval estimator = ZetaSketchHllPlusPlus[Int]()\n\ndef distinctCount: SCollection[Long] = input.countApproxDistinct(estimator)\nScio support the same for key-value SCollection too, in this case it will output distinct count per each unique key in the input data stream.\ndef kvInput[K, V]: SCollection[(K, V)] = ???\n\ndef distinctCount[K]: SCollection[(K, Long)] =\n    kvInput\n    .countApproxDistinctByKey(new SketchHllPlusPlus[Int](15, 20))\nSame thing with ZetaSketchHllPlusPlus\ndef distinctCount[K]: SCollection[(K, Long)] =\n    kvInput[K, Int] // K could be any type, value should be one of ZetaSketch supported types.\n    .countApproxDistinctByKey(estimator)","title":"HyperLogLog++ Integration."},{"location":"extras/HyperLogLog.html#distributed-hyperloglog-","text":"Both above implementation estimate distinct count for the whole data stream and doesn’t expose the underline sketch to the user. Scio exposed the underline sketch to the user and make it possible to run this Hll++ algorithm in distributed way using the ZetaSketch library’s internal APIs. Since, ZetaSketch is comply with Gooogle Cloud BigQuery, you can use BigQuery generated sketches with sketches exposed by this API.\nimport com.spotify.scio.extra.hll.zetasketch._\n\nimplicit def hllPlus[T]: HllPlus[T] = ???\nimport com.spotify.scio.values.SCollection\nimport com.spotify.scio.extra.hll.zetasketch._\n\ndef input[T]: SCollection[T] = ???\n// first convert each element to ZetaSketchHll.\ndef zCol[T]: SCollection[ZetaSketchHll[T]] = input.asZetaSketchHll\n// then combine all ZeatSketchHll and count the distinct.\ndef approxDistCount: SCollection[Long] = zCol.sumHll.approxDistinctCount\nor in simpler way you can do:\ndef approxDistCount: SCollection[Long] = input.approxDistinctCountWithZetaHll\nNote: This supports Int, Long, String and ByteString input types only.\nSimilarly, for key-value SCollections.\ndef kvInput[K, V]: SCollection[(K, V)] = ??? // Here type V should be one of supported type. `Int`, `Long`, `String` or `ByteString`\n\ndef zCol[K, V]: SCollection[(K, ZetaSketchHll[V])] = kvInput.asZetaSketchHllByKey\n\ndef approxDistCount[K]: SCollection[(K, Long)] = zCol.sumHllByKey.approxDistinctCountByKey\nor\ndef approxDistCount[K]: SCollection[(K, Long)] = kvInput.approxDistinctCountWithZetaHllByKey\nNOTE: ZetaSketchLL[T] has algebird’s Monoid and Aggregator implementations. Use it by importing com.spotify.scio.extra.hll.zetasketch.ZetaSketchHll._ to the scope.","title":"Distributed HyperLogLog++"},{"location":"extras/MutableScalableBloomFilter.html","text":"","title":"MutableScalableBloomFilter"},{"location":"extras/MutableScalableBloomFilter.html#mutablescalablebloomfilter","text":"Scio ships with an implementation of a scalable Bloom Filter, as described in “Scalable Bloom Filters”, Almeida, Baquero, et al..\nA Bloom filter is an approximate data structure that behaves in a similar way to a Set and can answer the question “does this set probably contain this value?”.\nAs an example, if you want to be able to answer the question “Does this user listen to Beyonce?” you could construct a Set containing all the ids of all the users that listened to Beyonce, persist it, the do a lookup into the set every time you need to know. The issue is that the Set will quickly become very large, especially for such a popular artist, and if you want to maintain many such sets, you will run into scaling issues. Bloom filters solve this problem by accepting some false positives for significant compression.\nA scalable Bloom filter is a sequence of Bloom filters that are iteratively added-to once each constituent filter nears its capacity (the point at which false positive guarantees break down). This is useful because inputs to a Bloom filter are lost, and it is not possible to resize a filter once constructed. The MutableScalableBloomFilter implementation shipping with scio maintains some additional metadata which allows it to scale automatically when necessary.\nSee the MutableScalableBloomFilter for details on how to properly size a scalable Bloom filter.\nimport com.spotify.scio._\nimport com.spotify.scio.values.SCollection\nimport com.spotify.scio.hash.MutableScalableBloomFilter\nimport magnolify.guava.auto._\n\ncase class TrackListen(trackId: String, userId: String)\n\nval elements: SCollection[TrackListen] = ???\nval msbfs: SCollection[MutableScalableBloomFilter[String]] = elements\n  .map { t => t.trackId -> t.userId }\n  .groupByKey\n  .map { case (trackId, userIds) =>\n    val msbf = MutableScalableBloomFilter[String](1_000_000)\n    msbf ++= userIds\n    msbf\n  }","title":"MutableScalableBloomFilter"},{"location":"extras/Sorter.html","text":"","title":"Sorter"},{"location":"extras/Sorter.html#sorter","text":"The sortValues transform sorts values by a secondary key following a groupByKey on the primary key, spilling sorting to disk if required. The memoryMB controls the allowable in-memory overhead before the sorter spills data to disk. Keys are compared based on the byte-array representations produced by their Beam coder.\nimport com.spotify.scio.values.SCollection\nimport com.spotify.scio.extra.sorter._\n\nval elements: SCollection[(String, (String, Int))] = ???\nval sorted: SCollection[(String, Iterable[(String, Int)])] = elements\n  .groupByKey\n  .sortValues(memoryMB = 100)","title":"Sorter"},{"location":"extras/Sort-Merge-Bucket.html","text":"","title":"Sort Merge Bucket"},{"location":"extras/Sort-Merge-Bucket.html#sort-merge-bucket","text":"Sort Merge Bucket (SMB) is a technique for writing data to file system in deterministic file locations, sorted according to some pre-determined key, so that it can later be read in as key groups with no shuffle required. Since each element is assigned a file destination (bucket) based on a hash of its join key, we can use the same technique to cogroup multiple Sources as long as they’re written using the same key and hashing scheme.\nFor example, given these input records, and SMB write will first extract the key, assign the record to a bucket, sort values within the bucket, and write these values to a corresponding file.\nInput Key Bucket File Assignment {key:“b”, value: 1} “b” 0 bucket-00000-of-00002.avro {key:“b”, value: 2} “b” 0 bucket-00000-of-00002.avro {key:“a”, value: 3} “a” 1 bucket-00001-of-00002.avro\nTwo sources can be joined by opening file readers on corresponding buckets of each source and merging key-groups as we go.","title":"Sort Merge Bucket"},{"location":"extras/Sort-Merge-Bucket.html#what-are-smb-transforms-","text":"scio-smb provides three PTransforms, as well as corresponding Scala API bindings, for SMB operations:\nSortedBucketSink writes data to file system in SMB format. Scala APIs (see: SortedBucketSCollection): SCollection[T: Coder]#saveAsSortedBucket copysourceval accountWriteTap = sc\n  .parallelize(250 until 750)\n  .map { i =>\n    Account\n      .newBuilder()\n      .setId(i % 100)\n      .setName(s\"name$i\")\n      .setType(s\"type${i % 5}\")\n      .setAmount(Random.nextDouble() * 1000)\n      .build()\n  }\n  .saveAsSortedBucket(\n    ParquetAvroSortedBucketIO\n      .write[Integer, Account](classOf[Integer], \"id\", classOf[Account])\n      .to(args(\"accounts\"))\n      .withSorterMemoryMb(128)\n      .withTempDirectory(sc.options.getTempLocation)\n      .withConfiguration(\n        ParquetConfiguration.of(ParquetOutputFormat.BLOCK_SIZE -> 512 * 1024 * 1024)\n      )\n      .withHashType(HashType.MURMUR3_32)\n      .withFilenamePrefix(\"part\") // Default is \"bucket\"\n      .withNumBuckets(1)\n      .withNumShards(1)\n  ) Note the use of Integer for parameterized key type instead of a Scala Int. The key class must have a Coder available in the default Beam (Java) coder registry. Also note that the number of buckets specified must be a power of 2. This allows sources of different bucket sizes to still be joinable. SortedBucketSource reads data that has been written to file system using SortedBucketSink into a collection of CoGbkResults. Scala APIs (see: SortedBucketScioContext): ScioContext#sortMergeGroupByKey (1 source) ScioContext#sortMergeJoin (2 sources) ScioContext#sortMergeCoGroup (1-22 sources) Note that each TupleTag used to create the SortedBucketIO.Reads needs to have a unique Id. copysourcesc.sortMergeJoin(\n  classOf[Integer],\n  ParquetAvroSortedBucketIO\n    .read(new TupleTag[GenericRecord](\"users\"), SortMergeBucketExample.UserDataSchema)\n    .withProjection(\n      SchemaBuilder\n        .record(\"UserProjection\")\n        .fields\n        .requiredInt(\"userId\")\n        .requiredInt(\"age\")\n        .endRecord\n    )\n    // Filter at the Parquet IO level to users under 50\n    // Filtering at the IO level whenever possible, as it reduces total bytes read\n    .withFilterPredicate(FilterApi.lt(FilterApi.intColumn(\"age\"), Int.box(50)))\n    // Filter at the SMB Cogrouping level to a single record per user\n    // Filter at the Cogroup level if your filter depends on the materializing key group\n    .withPredicate((xs, _) => xs.size() == 0)\n    .from(args(\"users\")),\n  ParquetTypeSortedBucketIO\n    .read(new TupleTag[AccountProjection](\"accounts\"))\n    .from(args(\"accounts\")),\n  TargetParallelism.max()\n).map { case (_, (userData, account)) =>\n  (userData.get(\"age\").asInstanceOf[Int], account.amount)\n}.groupByKey\n  .mapValues(amounts => amounts.sum / amounts.size)\n  .saveAsTextFile(args(\"output\")) SortedBucketTransform reads data that has been written to file system using SortedBucketSink, transforms each CoGbkResult using a user-supplied function, and immediately rewrites them using the same bucketing scheme. Scala APIs (see: SortedBucketScioContext): ScioContext#sortMergeTransform (1-22 sources) Note that each TupleTag used to create the SortedBucketIO.Reads needs to have a unique Id. copysourcesc.sortMergeTransform(\n  classOf[Integer],\n  ParquetAvroSortedBucketIO\n    .read(new TupleTag[GenericRecord](\"users\"), SortMergeBucketExample.UserDataSchema)\n    // Filter at the Parquet IO level to users under 50\n    .withFilterPredicate(FilterApi.lt(FilterApi.intColumn(\"age\"), Int.box(50)))\n    .from(args(\"users\")),\n  ParquetTypeSortedBucketIO\n    .read(new TupleTag[AccountProjection](\"accounts\"))\n    .from(args(\"accounts\")),\n  TargetParallelism.auto()\n).to(\n  ParquetTypeSortedBucketIO\n    .transformOutput[Integer, CombinedAccount](\"id\")\n    .to(args(\"output\"))\n).via { case (key, (users, accounts), outputCollector) =>\n  val sum = accounts.map(_.amount).sum\n  users.foreach { user =>\n    outputCollector.accept(\n      CombinedAccount(key, user.get(\"age\").asInstanceOf[Integer], sum)\n    )\n  }\n}","title":"What are SMB transforms?"},{"location":"extras/Sort-Merge-Bucket.html#what-kind-of-data-can-i-write-using-smb-","text":"SMB writes are supported for multiple formats:\nAvro (GenericRecord and SpecificRecord) when also depending on scio-avro. AvroSortedBucketIO JSON JsonSortedBucketIO Parquet when also depending on scio-parquet ParquetAvroSortedBucketIO ParquetTypesSortedBucketIO Tensorflow when also depending on scio-tensorflow TensorFlowBucketIO","title":"What kind of data can I write using SMB?"},{"location":"extras/Sort-Merge-Bucket.html#secondary-keys","text":"Since Scio 0.12.0.\nA single key group may be very large and the implementation of SMB requires either handling the elements of the key group iteratively or loading the entire key group into memory. In the case where a secondary grouping or sorting is required, this can be prohibitive in terms of memory and/or wasteful when multiple downstream pipelines do the same grouping. For example, a SMB dataset might be keyed by user_id but many downstreams want to group by the tuple of (user_id, artist_id).\nSecondary SMB keys enable this use-case by sorting pipeline output by the hashed primary SMB key as described above, then additionally sorting the output for each key by the secondary SMB key. When key groups are read by a downstream pipeline it may read either the entire (primary) key group or the subset of elements belonging to the (primary key, secondary key) tuple.\nA dataset may therefore add a secondary key and remain compatible with any downstream readers which expect only a primary key.\nTo write with a secondary key, the additional key class and path must be provided:\ncopysource.saveAsSortedBucket(\n  ParquetAvroSortedBucketIO\n    .write[Integer, String, Account](\n      // primary key class and field\n      classOf[Integer],\n      \"id\",\n      // secondary key class and field\n      classOf[String],\n      \"type\",\n      classOf[Account]\n    )\n    .to(args(\"accounts\"))\n)\nTo read with a secondary key, the additional key class must be provided:\ncopysourcesc.sortMergeGroupByKey(\n  classOf[String], // primary key class\n  classOf[String], // secondary key class\n  ParquetAvroSortedBucketIO\n    .read(new TupleTag[Account](\"account\"), classOf[Account])\n    .from(args(\"accounts\"))\n).map { case ((primaryKey, secondaryKey), elements) =>\n// ...\n}\nCorresponding secondary-key-enabled variants of sortMergeJoin, sortMergeCogroup, and sortMergeTransform are also included.","title":"Secondary keys"},{"location":"extras/Sort-Merge-Bucket.html#null-keys","text":"If the key field of one or more PCollection elements is null, those elements will be diverted into a special bucket file, bucket-null-keys.avro. This file will be ignored in SMB reads and transforms and must be manually read by a downstream user.","title":"Null keys"},{"location":"extras/Sort-Merge-Bucket.html#avro-string-keys","text":"As of Scio 0.14.0, Avro CharSequence are backed by String instead of default Utf8. With previous versions you may encounter the following when using Avro CharSequence keys:\nCause: java.lang.ClassCastException: class org.apache.avro.util.Utf8 cannot be cast to class java.lang.String\n[info]   at org.apache.beam.sdk.coders.StringUtf8Coder.encode(StringUtf8Coder.java:37)\n[info]   at org.apache.beam.sdk.extensions.smb.BucketMetadata.encodeKeyBytes(BucketMetadata.java:222)\nYou’ll have to either recompile your avro schema using String type, or add the GenericData.StringType.String property to your Avro schema with setStringType","title":"Avro String keys"},{"location":"extras/Sort-Merge-Bucket.html#parquet","text":"SMB supports Parquet reads and writes in both Avro and case class formats.\nAs of Scio 0.14.0 and above, Scio supports specific record logical types in parquet-avro out of the box.\nWhen using generic record, you have to manually supply a data supplier in your Parquet Configuration parameter. See Logical Types in Parquet for more information.","title":"Parquet"},{"location":"extras/Sort-Merge-Bucket.html#tuning-parameters-for-smb-transforms","text":"","title":"Tuning parameters for SMB transforms"},{"location":"extras/Sort-Merge-Bucket.html#numbuckets-numshards","text":"SMB reads should be more performant and less resource-intensive than regular joins or groupBys. However, SMB writes are more expensive than their regular counterparts, since they involve an extra group-by (bucketing) and sorting step. Additionally, non-SMB writes (i.e. implementations of FileBasedSink) use hints from the runner to determine an optimal number of output files. Unfortunately, SMB doesn’t have access to those runtime hints; you must specify the number of buckets and shards as static values up front.\nIn SMB, buckets correspond to the hashed value of the SMB key % a given power of 2. A record with a given key will always be hashed into the same bucket. On the file system, buckets consist of one or more sharded files in which records are randomly assigned per-bundle. Two records with the same key may end up in different shard files within a bucket.\nA good starting point is to look at your output data as it has been written by a non-SMB sink, and pick the closest power of 2 as your initial numBuckets and set numShards to 1. If you anticipate having hot keys, try increasing numShards to randomly split data within a bucket. numBuckets * numShards = total # of files written to disk.","title":"numBuckets/numShards"},{"location":"extras/Sort-Merge-Bucket.html#sortermemorymb","text":"If your job gets stuck in the sorting phase (since the GroupByKey and SortValues transforms may get fused–you can reference the Counters SortedBucketSink-bucketsInitiatedSorting and SortedBucketSink-bucketsCompletedSorting to get an idea of where your job fails), you can increase sorter memory (default is 1024MB, or 128MB for Scio <= 0.9.0):\ndata.saveAsSortedBucket(\n  AvroSortedBucketIO\n    .write[K, V](classOf[K], \"keyField\", classOf[V])\n    .to(...)\n    .withSorterMemoryMb(256)\n)\nThe amount of data each external sorter instance needs to handle is total output size / numBuckets / numShards, and when this exceeds sorter memory, the sorter will spill to disk. n1-standard workers has 3.75GB RAM per CPU, so 1GB sorter memory is a decent default, especially if the output files are kept under that size. If you have to spill to disk, note that worker disk IO depends on disk type, size, and worker number of CPUs.\nSee specifying pipeline execution parameters for more details, e.g. --workerMachineType, --workerDiskType, and --diskSizeGb. Also read more about machine types and block storage performance","title":"sorterMemoryMb"},{"location":"extras/Sort-Merge-Bucket.html#parallelism","text":"The SortedBucketSource API accepts an optional TargetParallelism parameter to set the desired parallelism of the SMB read operation. For a given set of sources, targetParallelism can be set to any number between the least and greatest numbers of buckets among sources. This can be dynamically configured using TargetParallelism.min() or TargetParallelism.max(), which at graph construction time will determine the least or greatest amount of parallelism based on sources.\nAlternately, TargetParallelism.of(Integer value) can be used to statically configure a custom value, or {@link TargetParallelism#auto()} can be used to let the runner decide how to split the SMB read at runtime based on the combined byte size of the inputs–this is also the default behavior if TargetPallelism is left unspecified.\nIf no value is specified, SMB read operations will use Auto parallelism.\nWhen selecting a target parallelism for your SMB operation, there are tradeoffs to consider:\nMinimal parallelism means a fewer number of workers merging data from potentially many buckets. For example, if source A has 4 buckets and source B has 64, a minimally parallel SMB read would have 4 workers, each one merging 1 bucket from source A and 16 buckets from source B. This read may have low throughput. Maximal parallelism means that each bucket is read by at least one worker. For example, if source A has 4 buckets and source B has 64, a maximally parallel SMB read would have 64 workers, each one merging 1 bucket from source B and 1 bucket from source A, replicated 16 times. This may have better throughput than the minimal example, but more expensive because every key group from the replicated sources must be re-hashed to avoid emitting duplicate records. A custom parallelism in the middle of these bounds may be the best balance of speed and computing cost. Auto parallelism is more likely to pick an ideal value for most use cases. If its performance is worse than expected, you can look up the parallelism value it has computed and try a manual adjustment. Unfortunately, since it’s determined at runtime, the computed parallelism value can’t be added to the pipeline graph through DisplayData. Instead, you’ll have to check the worker logs to find out which value was selected. When using Dataflow, you can do this in the UI by clicking on the SMB transform box, and searching the associated logs for the text Parallelism was adjusted. For example, in this case the value is 1024: From there, you can try increasing or decreasing the parallelism by specifying a different TargetParallelism parameter to your SMB read. Often auto-parallelism will select a low value and using TargetParallelism.max() can help.","title":"Parallelism"},{"location":"extras/Sort-Merge-Bucket.html#read-buffering","text":"Performance can suffer when reading an SMB source across many partitions if the total number of files (numBuckets * numShards * numPartitions) is too large (on the order of hundreds of thousands to millions of files). We’ve observed errors and timeouts as a result of too many simultaneous filesystem connections. To that end, we’ve added two PipelineOptions to Scio 0.10.3, settable either via command-line args or using SortedBucketOptions directly.\n--sortedBucketReadBufferSize (default: 10000): an Integer that determines the number of elements to read and buffer from each file at a time. For example, by default, each file will have 10,000 elements read and buffered into an in-memory array at worker startup. Then, the sort-merge algorithm will request them one at a time as needed. Once 10,000 elements have been requested, the file will buffer the next 10,000. Note: this can be quite memory-intensive and require bumping the worker memory. If you have a small number of files, or don’t need this optimization, you can turn it off by setting --sortedBucketReadBufferSize=0. --sortedBucketReadDiskBufferMb (default: unset): an Integer that, if set, will force each worker to actually copy the specified # of megabytes from the remote filesystem into the worker’s local temp directory, rather than streaming directly from FS. This caching is done eagerly: each worker will read as much as it can of each file in the order they’re requested, and more space will be freed up once a file is fully read. Note that this is a per worker limit.","title":"Read buffering"},{"location":"extras/Sort-Merge-Bucket.html#testing","text":"As of Scio 0.14, mocking data for SMB transforms is supported in the com.spotify.scio.testing.JobTest framework. Prior to Scio 0.14, you can test using real data written to local temp files.","title":"Testing"},{"location":"extras/Sort-Merge-Bucket.html#testing-smb-in-jobtest","text":"Scio 0.14 and above support testing SMB reads, writes, and transforms using SmbIO.\nConsider the following sample job that contains an SMB read and write:\nimport org.apache.beam.sdk.extensions.smb.ParquetAvroSortedBucketIO\nimport org.apache.beam.sdk.values.TupleTag\nimport com.spotify.scio._\nimport com.spotify.scio.avro.Account\nimport com.spotify.scio.values.SCollection\nimport com.spotify.scio.smb._\n\nobject SmbJob {\n    def main(cmdLineArgs: Array[String]): Unit = {\n        val (sc, args) = ContextAndArgs(cmdLineArgs)\n        \n        // Read\n        sc.sortMergeGroupByKey(\n            classOf[Integer],\n            ParquetAvroSortedBucketIO\n                .read(new TupleTag[Account](), classOf[Account])\n                .from(args(\"input\"))\n        )\n        \n        // Write\n        val writeData: SCollection[Account] = ???\n        writeData.saveAsSortedBucket(\n            ParquetAvroSortedBucketIO\n              .write(classOf[Integer], \"id\", classOf[Account])\n              .to(args(\"output\"))\n        )\n        \n        sc.run().waitUntilDone()\n    }\n}\nA JobTest can be wired in using SmbIO inputs and outputs. SmbIO is typed according to the record type and the SMB key type, and the SMB key function is required to construct it.\nimport com.spotify.scio.smb.SmbIO\nimport com.spotify.scio.testing.PipelineSpec\n\nclass SmbJobTest extends PipelineSpec {\n    \"SmbJob\" should \"work\" in {\n        val smbInput: Seq[Account] = ???\n        \n        JobTest[SmbJob.type]\n              .args(\"--input=gs://input\", \"--output=gs://output\")\n             \n              // Mock .sortMergeGroupByKey\n              .input(SmbIO[Int, Account](\"gs://input\", _.getId), smbInput)\n              \n              // Mock .saveAsSortedBucket\n              .output(SmbIO[Int, Account](\"gs://output\", _.getId)) { output =>\n                // Assert on output\n              }\n              .run()\n    }\n}\nSMB Transforms can be mocked by combining input and output SmbIOs:\n// Scio job\nobject SmbTransformJob {\n    def main(cmdLineArgs: Array[String]): Unit = {\n        val (sc, args) = ContextAndArgs(cmdLineArgs)\n        sc.sortMergeTransform(\n            classOf[Integer],\n            ParquetAvroSortedBucketIO\n                .read(new TupleTag[Account](), classOf[Account])\n                .from(args(\"input\"))\n        ).to(\n            ParquetAvroSortedBucketIO\n                .transformOutput[Integer, Account](classOf[Integer], \"id\", classOf[Account])\n                .to(args(\"output\"))\n        ).via { case (key, grouped, outputCollector) =>\n          val output: Account = ???\n          outputCollector.accept(output)\n        }\n        sc.run().waitUntilDone()\n  }\n}\n\n// Job test\nclass SmbTransformJobTest extends PipelineSpec {\n    \"SmbTransformJob\" should \"work\" in {\n        val smbinput: Seq[Account] = ???\n        \n        JobTest[SmbTransformJob.type]\n              .args(\"--input=gs://input\", \"--output=gs://output\")\n             \n              // Mock SMB Transform input\n              .input(SmbIO[Int, Account](\"gs://input\", _.getId), smbinput)\n              \n              // Mock SMB Transform output\n              .output(SmbIO[Int, Account](\"gs://output\", _.getId)) { output =>\n                // Assert on output\n              }\n              .run()\n    }\n}\nSee SortMergeBucketExampleTest for complete JobTest examples.","title":"Testing SMB in JobTest"},{"location":"extras/Sort-Merge-Bucket.html#testing-smb-using-local-file-system","text":"Using the JobTest framework for SMB reads, writes, and transforms is recommended, as it eliminates the need to manage local files and Taps. However, there are a few cases where performing real reads and writes is advantageous:\nIf you want to assert on SMB Predicates/Parquet FilterPredicates in reads, as these are skipped in JobTest If you want to assert on written metadata If you want to test schema evolution compatibility (i.e. writing using an updated record schema and reading using the original schema), or on projected schema compatability (i.e. using a case class projection to read Parquet data written with an Avro schema)\nScio 0.14.0 and above automatically return Taps for SMB writes and transforms, and can materialize SMB reads into Taps:\nimport com.spotify.scio.io.ClosedTap\n\n// Scio job\nobject SmbRealFilesJob {\n    def write(sc: ScioContext, output: String): ClosedTap[Account] = {\n        val writeData: SCollection[Account] = ???\n        writeData.saveAsSortedBucket(\n            ParquetAvroSortedBucketIO\n              .write(classOf[Integer], \"id\", classOf[Account])\n              .to(output)\n        )\n    }\n    \n    def read(sc: ScioContext, input: String): SCollection[(Integer, Iterable[Account])] = {\n        sc.sortMergeGroupByKey(\n            classOf[Integer],\n            ParquetAvroSortedBucketIO\n                .read(new TupleTag[Account](), classOf[Account])\n                .from(input)\n        )\n    }\n}\n\n// Unit test\nimport java.nio.file.Files\n\nclass SmbLocalFilesTest extends PipelineSpec {\n    \"SmbRealFilesJob\" should \"write and read data\" in {\n        val dir = Files.createTempDirectory(\"smb\").toString\n        \n        // Test write\n        val (_, writtenData) = runWithOutput { sc =>\n            SmbRealFilesJob.write(sc, dir)\n        }\n\n        // Assert on actual written output\n        writtenData.value should have size 100\n        \n        // Test read in separate ScioContext\n        val (_, groupedData) = runWithLocalOutput { sc =>\n            SmbRealFilesJob.read(sc, dir)\n        }\n\n        // Assert on actual read result\n        groupedData should have size 50\n    }\n}\nIn addition to JobTest examples, see SortMergeBucketExampleTest for complete SMB Tap examples.","title":"Testing SMB using local file system"},{"location":"extras/Sparkey.html","text":"","title":"Sparkey"},{"location":"extras/Sparkey.html#sparkey","text":"Scio supports Spotify’s Sparkey, which provides a simple disk-backed key-value store.\nAt Spotify, sparkeys are typically used in pipelines as side-inputs where the size of the side-input would be too large to reasonably fit into memory but can still fit on disk. Scio’s suite of largeHash functions are backed by sparkeys.\nScio supports writing any type with a coder to a sparkey by first converting","title":"Sparkey"},{"location":"extras/Sparkey.html#as-a-side-input","text":"A sparkey side-input is a good choice when you have a very large dataset that needs to be joined with a relatively small dataset, but one which is still too large to fit into memory. In this case, the asSparkeySideInput method can be used to broadcast the smaller dataset to all workers and avoid shuffle.\nimport com.spotify.scio.values.{SCollection, SideInput}\nimport com.spotify.scio.extra.sparkey._\nimport com.spotify.sparkey._\n\ncase class Track(title: String, artistId: String)\ncase class ArtistMetadata(artistId: String, name: String)\n\nval tracks: SCollection[Track] = ???\nval metadata: SCollection[ArtistMetadata] = ???\n\nval artistNameSI: SideInput[SparkeyReader] = metadata\n  .map { am => am.artistId -> am.name }\n  .asSparkeySideInput\n\ntracks.withSideInputs(artistNameSI)\n  .map { case (track, context) =>\n    val optArtistName = context(artistNameSI).get(track.artistId)\n    track -> optArtistName\n  }\nSee also Large Hash Joins, which do the same thing as this simple example but with a more compact syntax.","title":"As a Side-Input"},{"location":"extras/Sparkey.html#writing","text":"If a sparkey can be reused by multiple pipelines, it can be saved permanently with saveAsSparkey\nimport com.spotify.scio.values.SCollection\nimport com.spotify.scio.extra.sparkey._\n\nval elements: SCollection[(String, String)] = ???\nelements.saveAsSparkey(\"gs://output-path\")","title":"Writing"},{"location":"extras/Sparkey.html#reading","text":"Previously-written sparkeys can be loaded directly as side-inputs:\nimport com.spotify.scio.ScioContext\nimport com.spotify.scio.values.{SCollection, SideInput}\nimport com.spotify.scio.extra.sparkey._\nimport com.spotify.sparkey._\n\nval sc: ScioContext = ???\nval sparkeySI: SideInput[SparkeyReader] = sc.sparkeySideInput(\"gs://input-path\")","title":"Reading"},{"location":"extras/Scio-REPL.html","text":"","title":"REPL"},{"location":"extras/Scio-REPL.html#repl","text":"The Scio REPL is an extension of the Scala REPL, with added functionality that allows you to interactively experiment with Scio. Think of it as a playground to try out things.","title":"REPL"},{"location":"extras/Scio-REPL.html#quick-start","text":"You can either install Scio REPL via our Homebrew tap on a Mac or download the pre-built jar on other platforms.","title":"Quick start"},{"location":"extras/Scio-REPL.html#homebrew","text":"brew tap spotify/public\nbrew install scio\nscio-repl","title":"Homebrew"},{"location":"extras/Scio-REPL.html#pre-built-jar","text":"To download pre-built jar of Scio REPL, find version you are interested in on the release page, and download the REPL jar from Downloads section.\n$ java -jar scio-repl-<version>.jar\nWelcome to\n                 _____\n    ________________(_)_____\n    __  ___/  ___/_  /_  __ \\\n    _(__  )/ /__ _  / / /_/ /\n    /____/ \\___/ /_/  \\____/   version 0.7.0\n\nUsing Scala version 2.12.8 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_144)\nType in expressions to have them evaluated.\nType :help for more information.\n\nBigQuery client available as 'bq'\nScio context available as 'sc'\n\nscio>\nA ScioContext is created on REPL startup as sc and a starting point to most operations. Use tab completion, history and other REPL goodies to play around.","title":"Pre-built jar"},{"location":"extras/Scio-REPL.html#start-from-sbt-console","text":"$ git clone git@github.com:spotify/scio.git\nCloning into 'scio'...\nremote: Counting objects: 9336, done.\nremote: Compressing objects: 100% (275/275), done.\nremote: Total 9336 (delta 139), reused 0 (delta 0), pack-reused 8830\nReceiving objects: 100% (9336/9336), 1.76 MiB | 0 bytes/s, done.\nResolving deltas: 100% (3509/3509), done.\nChecking connectivity... done.\n$ cd scio\n$ sbt scio-repl/run","title":"Start from SBT console"},{"location":"extras/Scio-REPL.html#build-repl-jar-manually","text":"You can also build REPL jar from source.\n$ git clone git@github.com:spotify/scio.git\nCloning into 'scio'...\nremote: Counting objects: 9336, done.\nremote: Compressing objects: 100% (275/275), done.\nremote: Total 9336 (delta 139), reused 0 (delta 0), pack-reused 8830\nReceiving objects: 100% (9336/9336), 1.76 MiB | 0 bytes/s, done.\nResolving deltas: 100% (3509/3509), done.\nChecking connectivity... done.\n$ cd scio\n$ sbt scio-repl/assembly","title":"Build REPL jar manually"},{"location":"extras/Scio-REPL.html#sbt-project-from-scio-template","text":"Projects generated from scio-template.g8 have built-in REPL. Run sbt repl/run from the project root.","title":"sbt project from scio-template"},{"location":"extras/Scio-REPL.html#tutorial","text":"","title":"Tutorial"},{"location":"extras/Scio-REPL.html#local-pipeline","text":"Let’s start with simple local-mode word count example:\n// The REPL loads the following for you.\nimport com.spotify.scio._\n\ndef sc: ScioContext = ???\nval wordCount = sc\n    .textFile(\"README.md\")\n    .flatMap(_.split(\"[^a-zA-Z']+\").filter(_.nonEmpty))\n    .countByValue\n    .map(_.toString)\n    .saveAsTextFile(\"/tmp/local_wordcount\")\n\nval scioResult = sc.run().waitUntilDone()\n\nval values = scioResult.tap(wordCount).value.take(3)\nMake sure README.md is in the current directory. This example counts words in local file using a local runner (DirectRunner and writes result in a local file. The pipeline and actual computation starts on sc.run(). The last command take 3 lines from results and prints them.","title":"Local pipeline"},{"location":"extras/Scio-REPL.html#local-pipeline-","text":"In the next example we will spice things up a bit and read data from GCS:\n:newScio\nimport com.spotify.scio._\n\ndef sc: ScioContext = ???\nval shakespeare = sc.textFile(\"gs://dataflow-samples/shakespeare/hamlet.txt\")\n\nval wordCount = shakespeare\n    .flatMap(_.split(\"[^a-zA-Z']+\").filter(_.nonEmpty))\n    .countByValue\n    .map(_.toString)\n    .saveAsTextFile(\"/tmp/gcs-wordcount\")\n\nval result = sc\n    .run()\n    .waitUntilDone()\n    .tap(wordCount)\n    .value\n    .take(3)\nEach Scio context is associated with one and only one pipeline. The previous instance of sc was used for the local pipeline example and cannot be reused anymore. The first magic command, :newScio creates a new context as sc. The pipeline still performs computation locally, but reads data from Google Cloud Storage (it could also be BigQuery, Datastore, etc). This example may take a bit longer due to additional network overhead.","title":"Local pipeline ++"},{"location":"extras/Scio-REPL.html#dataflow-service-pipeline","text":"To create a Scio context for Google Cloud Dataflow service, add Dataflow pipeline options when starting the REPL. The same options will also be used by :newScio when creating new context. For example:\n$ java -jar scio-repl-0.7.0.jar \\\n> --project=<project-id> \\\n> --stagingLocation=<staging-dir> \\\n> --tempLocation=<temp-dir> \\\n> --runner=DataflowRunner\nWelcome to\n                 _____\n    ________________(_)_____\n    __  ___/  ___/_  /_  __ \\\n    _(__  )/ /__ _  / / /_/ /\n    /____/ \\___/ /_/  \\____/   version 0.7.0\n\nUsing Scala version 2.12.8 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_144)\nType in expressions to have them evaluated.\nType :help for more information.\n\nBigQuery client available as 'bq'\nScio context available as 'sc'\nimport com.spotify.scio._\n\ndef sc: ScioContext = ???\nval shakespeare = sc.textFile(\"gs://dataflow-samples/shakespeare/*\")\n\nval wordCount = shakespeare\n    .flatMap(_.split(\"[^a-zA-Z']+\").filter(_.nonEmpty))\n    .countByValue\n    .map(_.toString)\n    .saveAsTextFile(\"gs://<gcs-output-dir>\")\n\nval result = sc\n    .run()\n    .waitUntilDone()\n    .tap(wordCount)\n    .value\n    .take(3)\nIn this case we are reading data from GCS and performing computation in GCE virtual machines managed by Dataflow service. The last line is an example of reading data from GCS files to local memory after a context is closed. Most write operations in Scio return Future[Tap[T]] where a Tap[T] encapsulates some dataset that can be re-opened in another context or directly.\nUse :scioOpts to view or update Dataflow options inside the REPL. New options will be applied the next time you create a context.","title":"Dataflow service pipeline"},{"location":"extras/Scio-REPL.html#ad-hoc-local-mode","text":"You may start the REPL in distributed mode and run pipelines to aggregate from large datasets, and play around the results in local mode. You can create a local Scio context any time with :newLocalScio <name> and use it for local computations.\nscio> :newLocalScio lsc\nLocal Scio context available as 'lsc'","title":"Ad-hoc local mode"},{"location":"extras/Scio-REPL.html#bigquery-example","text":"In this example we will read some data from BigQuery and process it in Dataflow. We shall count number of tornadoes per month from a public sample dataset. Scio will do its best to find your configured Google Cloud project, but you can also specify it explicitly via -Dbigquery.project option.\n$ java -jar -Dbigquery.project=<project-id> scio-repl-0.7.0.jar \\\n> --project=<project-id> \\\n> --stagingLocation=<staging-dir> \\\n> --tempLocation=<temp-dir> \\\n> --runner=DataflowRunner\nWelcome to\n                 _____\n    ________________(_)_____\n    __  ___/  ___/_  /_  __ \\\n    _(__  )/ /__ _  / / /_/ /\n    /____/ \\___/ /_/  \\____/   version 0.7.0\n\nUsing Scala version 2.12.8 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_144)\nType in expressions to have them evaluated.\nType :help for more information.\n\nBigQuery client available as 'bq'\nScio context available as 'sc'\nimport com.spotify.scio._\nimport com.spotify.scio.bigquery._\n\ndef sc: ScioContext = ???\nval tornadoes = sc.bigQuerySelect(Query(\"SELECT tornado, month FROM [apache-beam-testing:samples.weather_stations]\"))\n \nval counts = tornadoes\n    .flatMap(r => if (r.getBoolean(\"tornado\")) Seq(r.getLong(\"month\")) else Nil)\n    .countByValue\n    .map(kv => TableRow(\"month\" -> kv._1, \"tornado_count\" -> kv._2))\n    .take(3)\n    .materialize\n\nval result = sc\n    .run()\n    .waitUntilDone()\n    .tap(counts)\n    .value\nIn this example we combine power of BigQuery and flexibility of Dataflow. We first query BigQuery table, perform a couple of transformations and take (take(3)) some data back locally (materialize) to view the results.","title":"BigQuery example"},{"location":"extras/Scio-REPL.html#bigquery-project-id","text":"Scio REPL will do its best to find your configured Google Cloud project, without the need to explicitly specifying bigquery.project property. It will search for project-id in this specific order:\nbigquery.project java system property GCLOUD_PROJECT java system property GCLOUD_PROJECT environmental variable gcloud config files: scio named configuration default configuration\nThis means that you can always set bigquery.project and it will take precedence over other configurations. Read more about gcloud configuration here.","title":"BigQuery project id"},{"location":"extras/Scio-REPL.html#i-o-commands","text":"There are few built-in commands for simple file I/O.\nimport scala.reflect._\nimport kantan.csv._\n\n// Read from an Avro, text, CSV or TSV file on local filesystem or GCS.\ndef readAvro[T : ClassTag](path: String): Iterator[T] = ???\ndef readText(path: String): Iterator[String] = ???\ndef readCsv[T: RowDecoder](path: String,\n                           sep: Char = ',',\n                           header: Boolean = false): Iterator[T] = ???\ndef readTsv[T: RowDecoder](path: String,\n                           sep: Char = '\\t',\n                           header: Boolean = false): Iterator[T] = ???\n\n// Write to an Avro, text, CSV or TSV file on local filesystem or GCS.\ndef writeAvro[T: ClassTag](path: String, data: Seq[T]): Unit = ???\ndef writeText(path: String, data: Seq[String]): Unit = ???\ndef writeCsv[T: RowEncoder](path: String, data: Seq[T],\n                            sep: Char = ',',\n                            header: Seq[String] = Seq.empty): Unit = ???\ndef writeTsv[T: RowEncoder](path: String, data: Seq[T],\n                            sep: Char = '\\t',\n                            header: Seq[String] = Seq.empty): Unit = ???","title":"I/O Commands"},{"location":"extras/Scio-REPL.html#tips","text":"","title":"Tips"},{"location":"extras/Scio-REPL.html#multi-line-code","text":"While in the REPL, use :paste magic command to paste or write multi-line code\n:paste\nimport com.spotify.scio._\n\ndef sc: ScioContext = ???\n// Entering paste mode (ctrl-D to finish)\nimport com.spotify.scio.io.ClosedTap\nimport com.spotify.scio.values.SCollection\n\ndef evenNumber(x: Int): Boolean = x % 2 == 0\ndef evenNumbers: SCollection[Int] = sc.parallelize(1 to 100).filter(evenNumber)\n\n// Exiting paste mode, now interpreting.\n\ndef tap: ClosedTap[String] = evenNumbers.saveAsTextFile(\"/tmp/even\")\n\ndef result = sc.run()","title":"Multi-line code"},{"location":"extras/Scio-REPL.html#running-jobs-asynchronously","text":"When using REPL and Dataflow service consider using the non-blocking DataflowRunner for a more interactive experience. To start:\njava -jar scio-repl-0.7.0.jar \\\n> --project=<project-id> \\\n> --stagingLocation=<staging-dir> \\\n> --tempLocation=<temp-dir> \\\n> --runner=DataflowRunner\nWelcome to\n                 _____\n    ________________(_)_____\n    __  ___/  ___/_  /_  __ \\\n    _(__  )/ /__ _  / / /_/ /\n    /____/ \\___/ /_/  \\____/   version 0.7.0\n\nUsing Scala version 2.12.8 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_144)\nType in expressions to have them evaluated.\nType :help for more information.\n\nBigQuery client available as 'bq'\nScio context available as 'sc'\nimport com.spotify.scio._\n\ndef sc: ScioContext = ???\nimport com.spotify.scio.io.ClosedTap\n\ndef closedTap: ClosedTap[String] = sc\n    .parallelize(1 to 100)\n    .map( _.toString )\n    .saveAsTextFile(\"gs://<output>\")\n\ndef result = sc.run()\n// [main] INFO org.apache.beam.runners.dataflow.DataflowRunner - Executing pipeline on the Dataflow Service, which will have billing implications related to Google Compute Engine usage and other Google Cloud Services.\n// [main] INFO org.apache.beam.runners.dataflow.util.PackageUtil - Uploading 3 files from PipelineOptions.filesToStage to staging location to prepare for execution.\n// [main] INFO org.apache.beam.runners.dataflow.util.PackageUtil - Uploading PipelineOptions.filesToStage complete: 2 files newly uploaded, 1 files cached\n// Dataflow SDK version: 2.9.0\n\ndef state = result.state\nNote that now sc.run() doesn’t block and wait until job completes and gives back control of the REPL right away. Use ScioExecutionContext to check for progress, results and orchestrate jobs.","title":"Running jobs asynchronously"},{"location":"extras/Scio-REPL.html#multiple-scio-contexts","text":"You can use multiple Scio context objects to work with several pipelines at the same time, simply use magic :newScio <context name>, for example:\nscio> :newScio c1\nScio context available as 'c1'\nscio> :newScio c2\nScio context available as 'c2'\nscio> :newLocalScio lc\nScio context available as 'lc'\nYou can use those in combination with DataflowRunner to run multiple pipelines in the same session or wire them with for comprehension over futures.","title":"Multiple Scio contexts"},{"location":"extras/Scio-REPL.html#bigquery-client","text":"Whenever possible leverage BigQuery! @BigQueryType annotations enable type safe and civilized integration with BigQuery inside Scio. Here is example of using the annotations and BigQuery client to read and write typed data directly without Scio context.\n$ java -jar -Dbigquery.project=<project-id> scio-repl-0.7.0.jar\nWelcome to\n                 _____\n    ________________(_)_____\n    __  ___/  ___/_  /_  __ \\\n    _(__  )/ /__ _  / / /_/ /\n    /____/ \\___/ /_/  \\____/   version 0.7.0\n\nUsing Scala version 2.12.8 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_144)\nType in expressions to have them evaluated.\nType :help for more information.\n\nBigQuery client available as 'bq'\nScio context available as 'sc'\nimport com.spotify.scio.bigquery._\nimport com.spotify.scio.bigquery.client._\n\ndef bq: BigQuery = ???\n@BigQueryType.fromQuery(\"SELECT tornado, month FROM [apache-beam-testing:samples.weather_stations]\") class Row\n\ndef tornadoes = bq.getTypedRows[Row]()\n\ndef result = tornadoes.next().month\n\ndef write = bq.writeTypedRows(\"project-id:dataset-id.table-id\", tornadoes.take(100).toList)","title":"BigQuery client"},{"location":"extras/Scio-REPL.html#out-of-memory-exception","text":"In case of OOM exceptions, like for example:\nimport com.spotify.scio._\nimport com.spotify.scio.io._\n\ndef sc: ScioContext = ???\ndef closedTap: ClosedTap[String] = ???\n\ndef result = sc.run().waitUntilDone().tap(closedTap).value.next()\n// Exception in thread \"main\"\n// Exception: java.lang.OutOfMemoryError thrown from the UncaughtExceptionHandler in thread \"main\"\nsimply increase the size of the heap - be reasonable about the amount of data and heap size though.\nExample of REPL startup with 2GiB of heap size:\n$ java -Xmx2g -jar scio-repl-0.7.0.jar\nWelcome to\n                 _____\n    ________________(_)_____\n    __  ___/  ___/_  /_  __ \\\n    _(__  )/ /__ _  / / /_/ /\n    /____/ \\___/ /_/  \\____/   version 0.7.0\n\nUsing Scala version 2.12.8 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_144)\n\nType in expressions to have them evaluated.\nType :help for more information.\n\nBigQuery client available as 'bq'\nScio context available as 'sc'\nRuntime.getRuntime().maxMemory()\n// res1: Long = 1908932608","title":"Out of memory exception"},{"location":"extras/Scio-REPL.html#what-is-the-type-of-an-expression-","text":"Use the built in :t! :t displays the type of an expression without evaluating it. Example:\nscio> :t sc.textFile(\"README\").flatMap(_.split(\"[^a-zA-Z']+\")).filter(_.nonEmpty).map(_.length)\ncom.spotify.scio.values.SCollection[Int]\nLearn more about magic keywords via scio> :help","title":"What is the type of an expression?"},{"location":"extras/Transforms.html","text":"","title":"Transforms"},{"location":"extras/Transforms.html#transforms","text":"The com.spotify.scio.transforms package provides a selection of transforms with additional functionality.","title":"Transforms"},{"location":"extras/Transforms.html#withresource","text":"The WithResource syntax provides a convenient wrapper around DoFnWithResource that allows reuse of some resource class, for example an API client, according to the specified ResourceType behavior for variants of map, filter, flatMap, and collect:\nimport com.spotify.scio.values.SCollection\nimport com.spotify.scio.transforms._\nimport com.spotify.scio.transforms.DoFnWithResource.ResourceType\n\nclass Client(val name: String)\nclass ClientNotThreadSafe() {\n  private var state: Int = 0\n  def name(): String = {\n    val out = s\"c$state\"\n    state = state + 1\n    out\n  }\n}\n\nval elements: SCollection[String] = ???\n\nelements.mapWithResource(new Client(\"c1\"), ResourceType.PER_CLASS) { \n  case (client, s) => s + client.name\n}\nelements.filterWithResource(new Client(\"c2\"), ResourceType.PER_INSTANCE) { \n  case (client, s) => s.nonEmpty\n}\nelements.collectWithResource(new Client(\"c3\"), ResourceType.PER_INSTANCE) {\n  case (client, s) if s.nonEmpty => s + client.name\n}\nelements.flatMapWithResource(new ClientNotThreadSafe(), ResourceType.PER_CLONE) {\n  case (client, s) => s + client.name()\n}","title":"WithResource"},{"location":"extras/Transforms.html#custom-parallelism","text":"By default, a worker on dataflow batch pipeline will have a number of threads equal to the number of vCPUs. In dataflow streaming, the default number of threads is 300.\nTo limit the number of concurrent items being processed a worker, CustomParallelism syntax allows setting a parallelism argument on variants of map, filter, flatMap, and collect:\nimport com.spotify.scio.values.SCollection\nimport com.spotify.scio.transforms._\n\nval elements: SCollection[String] = ???\nelements.mapWithParallelism(5) { s => s + \"_append\" }\nelements.filterWithParallelism(5) { s => s.nonEmpty }\nelements.flatMapWithParallelism(5) { s => s.split(\",\") }\nelements.collectWithParallelism(5) { case s if s.nonEmpty => s + \"_append\" }","title":"Custom Parallelism"},{"location":"extras/Transforms.html#filedownload","text":"The FileDownload syntax provides support for downloading arbitrary URIs to a local file, then handling the results:\nimport com.spotify.scio.values.SCollection\nimport com.spotify.scio.transforms._\nimport scala.jdk.CollectionConverters._\nimport java.net.URI\nimport java.nio.file.Files\nimport java.nio.charset.StandardCharsets\n  \nval uris: SCollection[URI] = ???\nval fileContents: SCollection[String] = uris.mapFile { path =>\n  new String(Files.readAllBytes(path), StandardCharsets.UTF_8) \n}\nval lines: SCollection[String] = uris.flatMapFile { path => \n  Files.readAllLines(path).asScala\n}","title":"FileDownload"},{"location":"extras/Transforms.html#safe-flatmap","text":"The Safe syntax provides a safeFlatMap function that captures any exceptions thrown by the body of the transform and partitions its output into an SCollection of successfully-output elements and an SCollection of the exception-throwing input elements and the Throwable they produced.\nimport com.spotify.scio.values.SCollection\nimport com.spotify.scio.transforms._\n\nval elements: SCollection[String] = ???\nval (ok: SCollection[Int], bad: SCollection[(String, Throwable)]) = elements\n  .safeFlatMap { in =>\n    in.split(\",\").map { s => s.toInt }\n  }","title":"Safe flatMap"},{"location":"extras/Transforms.html#pipe","text":"The Pipe syntax provides a method to pass elements of an SCollection[String] to a specified command-line program. Additional arguments allow configuration of the working directory, application environment, and setup & teardown commands.\nimport com.spotify.scio.values.SCollection\nimport com.spotify.scio.transforms._\n\nval elements: SCollection[String] = ???\nval upperElements: SCollection[String] = elements.pipe(\"tr [:lower:] [:upper:]\")","title":"Pipe"},{"location":"extras/Voyager.html","text":"","title":"Voyager"},{"location":"extras/Voyager.html#voyager","text":"Scio supports Spotify’s Voyager, which provides an easy to use API on top of hnswlib that that works in python and java.","title":"Voyager"},{"location":"extras/Voyager.html#write","text":"A keyed SCollection with String keys and Array[Float] vector values can be saved with asVoyager:\nimport com.spotify.scio.values.SCollection\nimport com.spotify.scio.extra.voyager._\nimport com.spotify.voyager.jni.Index.{SpaceType, StorageDataType}\n\nval voyagerUri = VoyagerUri(\"gs://output-path\")\nval space: SpaceType = ???\nval numDimensions: Int = ???\nval itemVectors: SCollection[(String, Array[Float])] = ???\nitemVectors.asVoyager(uri, space, numDimensions)","title":"Write"},{"location":"extras/Voyager.html#side-input","text":"A Voyager index can be read directly as a SideInput with asVoyagerSideInput:\nimport com.spotify.scio._\nimport com.spotify.scio.values.SCollection\nimport com.spotify.scio.extra.voyager._\nimport com.spotify.voyager.jni.Index.{SpaceType, StorageDataType}\n\nval sc: ScioContext = ???\n\nval voyagerUri = VoyagerUri(\"gs://output-path\")\n\n// index saved with voyager v1 requires settings\nval space: SpaceType = ???\nval numDimensions: Int = ???\nval storageDataType: StorageDataType = ???\nval voyagerV1: SideInput[VoyagerReader] = sc.voyagerSideInput(voyagerUri, space, numDimensions, storageDataType)\n// index saved with voyager v2 extracts settings from its metadata\nval voyagerV2: SideInput[VoyagerReader] = sc.voyagerSideInput(voyagerUri)\nAlternatively, an SCollection can be converted directly to a SideInput with asVoyagerSideInput:\nimport com.spotify.scio.values.SCollection\nimport com.spotify.scio.extra.voyager._\nimport com.spotify.voyager.jni.Index.{SpaceType, StorageDataType}\n\nval space: SpaceType = ???\nval numDimensions: Int = ???\nval itemVectors: SCollection[(String, Array[Float])] = ???\nval voyagerSI: SideInput[VoyagerReader] = itemVectors.asVoyagerSideInput(space, numDimensions)\nAn VoyagerReader provides access to querying the Voyager index to get their nearest neighbors.\nimport com.spotify.scio.values.{SCollection, SideInput}\nimport com.spotify.scio.extra.voyager._\n\nval voyagerSI: SideInput[VoyagerReader] = ???\nval elements: SCollection[(String, Array[Float])] = ???\nval maxNumResults: Int = ???\nval ef: Int = ???\n\nval queryResults: SCollection[(String, Array[VoyagerResult])] = elements\n  .withSideInputs(voyagerSI)\n  .map { case ((label, vector), ctx) =>\n    val voyagerReader: VoyagerReader = ctx(voyagerSI)\n    (label, voyagerReader.getNearest(vector, maxNumResults, ef))\n  }","title":"Side Input"},{"location":"dev/index.html","text":"","title":"Development"},{"location":"dev/index.html#development","text":"Build Style Guide How to Release Design Philosophy","title":"Development"},{"location":"dev/build.html","text":"","title":"Build"},{"location":"dev/build.html#build","text":"","title":"Build"},{"location":"dev/build.html#getting-the-source","text":"git clone https://github.com/spotify/scio.git","title":"Getting the source"},{"location":"dev/build.html#compiling","text":"Build and test the code.\ncd scio\nsbt test\nSome examples depend on Google Cloud Platform and are excluded by default if GCP credentials are missing. To enable them, authenticate yourself for GCP, set up default credentials and restart sbt.\ngcloud auth application-default login\nsbt test\nAlternatively you can populate pre-generated cache for BigQuery schemas to bypass GCP access. Define bigquery.project as a system property. The value can by anything since we’ll hit cache instead.\n./scripts/gen_schemas.sh\nsbt -Dbigquery.project=dummy-project test\nTasks in the ‘integration’ module integration/{compile,test} currently require access to datasets hosted in an internal Spotify project. External users must authenticate against their own GCP project, through the steps outlined in Getting Started.","title":"Compiling"},{"location":"dev/build.html#intellij-idea","text":"When opening the project in IntelliJ IDEA, tick “Use sbt shell:” both “for imports” and “for builds”.","title":"IntelliJ IDEA"},{"location":"dev/Style-Guide.html","text":"","title":"Style Guide"},{"location":"dev/Style-Guide.html#style-guide","text":"","title":"Style Guide"},{"location":"dev/Style-Guide.html#general-guidelines","text":"","title":"General Guidelines"},{"location":"dev/Style-Guide.html#code-formatters","text":"We use scalafmt and sbt-java-formatter to format code automatically and keep the code style consistent, and sbt-header to generate copyright file header.\nSee scalafmt configuration file.","title":"Code formatters"},{"location":"dev/Style-Guide.html#sbt-plugin","text":"Run the following command to format the entire codebase.\nsbt scalafmtAll javafmtAll headerCreateAll","title":"Sbt plugin"},{"location":"dev/Style-Guide.html#intellij-idea","text":"Most of us write Scala code in IntelliJ IDEA and it’s wise to let the IDE do most of the work including managing imports and formatting code. To use scalafmt you need an additional plugin. Follow this link to install it.\nWe also want to avoid custom settings as much as possible to make on-boarding new developers easier. Hence, we use IntelliJ IDEA’s default settings with the following exceptions:\nSet Code Style → Scala → Formatter → scalafmt","title":"IntelliJ IDEA"},{"location":"dev/Style-Guide.html#scalafix","text":"Scalafix is a refactoring and linting tool that we rely on as well to keep our code tidy. Currently, we use the following rules:\nrules = [\n  RemoveUnused,\n  LeakingImplicitClassVal\n]\nRun the following command to let scalafix refactor your code.\nsbt scalafixEnable scalafixAll","title":"Scalafix"},{"location":"dev/Style-Guide.html#references","text":"We want to adhere to the styles of well known Scala projects and use the following documents as references when scalafmt needs a little bit of help. We follow the Databricks Scala Guide mainly with a few differences described in the next section.\nDatabricks Scala Guide The Official Scala Style Guide Twitter’s Effective Scala","title":"References"},{"location":"dev/Style-Guide.html#differences-from-databricks-scala-guide","text":"","title":"Differences from Databricks Scala Guide"},{"location":"dev/Style-Guide.html#spacing-and-indentation","text":"For method declarations, align parameters when they don’t fit in a single line. Return types on the same line as the last parameter.\ndef saveAsBigQuery(\n    table: TableReference,\n    schema: TableSchema,\n    writeDisposition: WriteDisposition,\n    createDisposition: CreateDisposition,\n    tableDescription: String,\n    timePartitioning: TimePartitioning)(implicit ev: T <:< TableRow): ClosedTap[TableRow] = {\n   // method body\n  }\n\n  def saveAsTypedBigQuery(\n    tableSpec: String,\n    writeDisposition: WriteDisposition = TableWriteParam.DefaultWriteDisposition,\n    createDisposition: CreateDisposition = TableWriteParam.DefaultCreateDisposition,\n    timePartitioning: TimePartitioning = TableWriteParam.DefaultTimePartitioning)(\n    implicit tt: TypeTag[T],\n    ev: T <:< HasAnnotation,\n    coder: Coder[T]): ClosedTap[T] = {\n      // method body\n  }\nFor classes whose header doesn’t fit in a single line and exceed the line, align the next line and add a blank line after class header.\nclass Foo(val param1: String,\n          val param2: String,\n          val param3: Array[Byte])\n  extends FooInterface  // 2 space indent here\n  with Logging {\n\n  def firstMethod(): Unit = { /* body */ }  // blank line above\n}","title":"Spacing and Indentation"},{"location":"dev/Style-Guide.html#blank-lines-vertical-whitespace-","text":"A single blank line appears: Between consecutive members (or initializers) of a class: fields, constructors, methods, nested classes, static initializers, instance initializers. Within method bodies, as needed to create logical groupings of statements. Before the first member and after the last member of the class. A blank line is optional: Between consecutive one-liner fields or methods of a class that have no ScalaDoc. Before the first member and after the last member of a short class with one-liner members only. Use one blank line to separate class definitions. Excessive number of blank lines is discouraged.\nclass Foo {\n\n  val x: Int  // blank line before the first member\n  val y: Int\n  val z: Int  // no blank line between one-liners that have no ScalaDoc\n\n  def hello(): {\n    // body\n  }  // blank line after the last member\n\n}\n\n// no blank line before the first member and after the last member\nclass Bar {\n  def x = { /* body */ }\n  def y = { /* body */ }\n}","title":"Blank Lines (Vertical Whitespace)"},{"location":"dev/Style-Guide.html#curly-braces","text":"Put curly braces even around one-line conditional or loop statements. The only exception is if you are using if/else as an one-line ternary operator that is also side-effect free.\n// the only exception for omitting braces\nval x = if (true) expression1 else expression2","title":"Curly Braces"},{"location":"dev/Style-Guide.html#documentation-style","text":"Use Java docs style instead of Scala docs style. One-liner ScalaDoc is acceptable. Annotations like @tparam, @param, @return are optional if they are obvious to the user.\nScalaDoc /** */ should only be used for documenting API to end users. Use regular comments e.g. /* */ and // for explaining code to developers.\n/** This is a correct one-liner, short description. */\n\n/**\n * This is correct multi-line JavaDoc comment. And\n * this is my second line, and if I keep typing, this would be\n * my third line.\n */\n\n/** In Spark, we don't use the ScalaDoc style so this\n  * is not correct.\n  */\n\n// @param xs, @tparam T and @return are obvious and no need to document\n/** Sum up a sequence with an Algebird Semigroup. */\ndef sum[T: Semigroup](xs: Seq[T]): T = xs.reduce(implicitly[Semigroup[T]].plus)","title":"Documentation Style"},{"location":"dev/Style-Guide.html#ordering-within-a-class","text":"If a class is long and has many methods, group them logically into different sections, and use comment headers to organize them.\nclass ScioContext {\n\n  // =======================================================================\n  // Read operations\n  // =======================================================================\n\n  // =======================================================================\n  // Accumulators\n  // =======================================================================\n\n}\nOf course, the situation in which a class grows this long is strongly discouraged, and is generally reserved only for building certain public APIs.","title":"Ordering within a Class"},{"location":"dev/Style-Guide.html#imports","text":"Mutable collections Always prefix a mutable collection type with M when importing, e.g. import scala.collection.mutable.{Map => MMap} Or import scala.collection.mutable package and use mutable.Map Sort imports in IntelliJ IDEA’s default order: java.* All other imports scala.* It should look like this in Imports → Import Layout\njava\n_______ blank line _______\nall other imports\n_______ blank line _______\nscala","title":"Imports"},{"location":"dev/How-to-Release.html","text":"","title":"How to Release"},{"location":"dev/How-to-Release.html#how-to-release","text":"","title":"How to Release"},{"location":"dev/How-to-Release.html#prerequisites","text":"Sign up for a Sonatype account here Ask for permissions to push to com.spotify domain like in this ticket Add Sonatype credentials to ~/.sbt/1.0/credentials.sbt\ncredentials ++= Seq(\nCredentials(\n    \"Sonatype Nexus Repository Manager\",\n    \"oss.sonatype.org\",\n    \"$USERNAME\",\n    \"$PASSWORD\"))\nCreate a PGP key, for example on keybase.io, and distribute it to a public keyserver","title":"Prerequisites"},{"location":"dev/How-to-Release.html#update-version-matrix","text":"If the release includes a Beam version bump, update the version matrix","title":"Update version matrix"},{"location":"dev/How-to-Release.html#automatic-ci-","text":"Checkout and update the main branch.\ngit checkout main\n\ngit pull\nCreate and push a new version tag\ngit tag -a vX.Y.Z -m \"vX.Y.Z\"\n\ngit push origin vX.Y.Z","title":"Automatic (CI)"},{"location":"dev/How-to-Release.html#manual","text":"Run release skip-tests in sbt console and follow the instructions Go to oss.sonatype.org, find the staging repository, “close” and “release” When the tag build completes, update release notes with name and change log Run ./scripts/make-site.sh to update documentation","title":"Manual"},{"location":"dev/How-to-Release.html#after-successfully-published-artifacts","text":"Clean the mimaBinaryIssueFilters in build.sbt if needed Create a GitHub release Run scripts/bump_scio.sh to update homebrew formula Update scioVersion in downstream repos (scio.g8, etc.) Send announcement to scio-users@googlegroups.com","title":"After successfully published artifacts"},{"location":"dev/Design-Philosophy.html","text":"","title":"Design Philosophy"},{"location":"dev/Design-Philosophy.html#design-philosophy","text":"We learned a lot building and improving Scio. The project was inspired by Spark and Scalding from the beginning, and we improved it over time working with customers of diverse background, including backend, data and ML. The design philosophy behind Scio can be summarized in a few points.\nMake it easy to do the right thing Scala made this possible for the most part. We have a fluent API and it’s easy to find the right transformation without going through lengthy documentation or source code. The most obvious thing is usually the best. .countByValue is clearer and more efficient than .map((_, 1L)).sumByKey than .map((_, 1L)).reduceByKey(_+_). Case classes and Options are much safer and easier than JSON-based TableRows with Objects and nulls, despite the effort we went through to make it work. One can .sum types with built-in Semigroups easily and correctly. Conversely there is no .groupAll since it could incur huge performance penalty and is essentially .groupBy(_ => ()). It’s easier to ask than making the wrong assumption and use it wrong (_“because it’s there”_). Make common use cases simple We have syntactic sugar for most common IO modules e.g. ScioContext#textFile, SCollection#saveAsBigQuery but don’t cover all possible parameters. There’s a trade-off between covering more use cases and keeping the API simple. We opted for a more flexible boilerplate free Args instead of the more type-safe PipelineOptions for command line arguments parsing. Mistakes in these parts of the code are easier to catch and less damaging than those in the computation logic. Another trade-off we made. We have syntactic sugars for various types of joins (hash, inner, outer, sketch) and side input operations (cross, lookup) that can be easily swapped to fine tune a pipeline. Make complex use cases possible We wrap complex internal APIs but don’t hide them away from users. Most low level Beam APIs (Pipeline, PCollection, PTransform) are still easily accessible. There are shorthands for integrating native Beam API, e.g. ScioContext#customInput, SCollection#saveAsCustomOutput, SCollection#applyTransform. Pipelines can be submitted from main, another process, a backend service, or chained with Futures.","title":"Design Philosophy"},{"location":"scaladoc.html","text":"","title":""},{"location":"Scio,-Scalding-and-Spark.html","text":"","title":"Scio, Spark and Scalding"},{"location":"Scio,-Scalding-and-Spark.html#scio-spark-and-scalding","text":"Check out the Beam Programming Guide first for a detailed explanation of the Beam programming model and concepts.\nScio’s API is heavily influenced by Spark with a lot of ideas from Scalding.","title":"Scio, Spark and Scalding"},{"location":"Scio,-Scalding-and-Spark.html#scio-and-spark","text":"The Dataflow programming model is fundamentally different from that of Spark. Read this Google blog article for more details.\nThe Scio API is heavily influenced by Spark but there are some minor differences.\nSCollection is equivalent to Spark’s RDD. PairSCollectionFunctions and DoubleSCollectionFunctions are specialized versions of SCollection and equivalent to Spark’s PairRDDFunctions and DoubleRDDFunctions. Execution planning is static and happens before the job is submitted. There is no driver node in a Dataflow cluster and one can only perform the equivalent of Spark transformations (RDD → RDD) but not actions (RDD → driver local memory). There is no broadcast either but the pattern of RDD → driver via action and driver → RDD via broadcast can be replaced with SCollection.asSingletonSideInput and SCollection.withSideInputs. There is no DStream (continuous series of RDDs) like in Spark Streaming. Values in an SCollection are windowed based on timestamp and windowing operation. The same API works regardless of batch (single global window by default) or streaming mode. Aggregation type transformations that produce SCollections of a single value under global window will produce one value each window when a non-global window is defined. SCollection has extra methods for side input, side output, and windowing.","title":"Scio and Spark"},{"location":"Scio,-Scalding-and-Spark.html#scio-and-scalding","text":"Scio has a much simpler abstract data types compared to Scalding.\nScalding has many abstract data types like TypedPipe, Grouped, CoGrouped, SortedGrouped. Many of them are intermediate and enable some optimizations or wrap around Cascading’s data model. As a result many Scalding operations are lazily evaluated, for example in pipe.groupBy(keyFn).reduce(mergeFn), mergeFn is lifted into groupBy to operate on the map side as well. Scio on the other hand, has only one main data type SCollection[T] and SCollection[(K, V)] is a specialized variation when the elements are key-value pairs. All Scio operations are strictly evaluated, for example p.groupBy(keyFn) returns (K, Iterable[T]) where the values are immediately grouped, whereas p.reduceByKey(_ + _) groups (K, V) pairs on K and reduces values.\nSome features may look familiar to Scalding users.\nArgs is a simple command line argument parser similar to the one in Scalding. Powerful transforms are possible with sum, sumByKey, aggregate, aggregrateByKey using Algebird Semigroups and Aggregators. MultiJoin and coGroup of up to 22 sources. JobTest for end to end pipeline testing.","title":"Scio and Scalding"},{"location":"Scio,-Scalding-and-Spark.html#scollection","text":"SCollection has a few variations.\nSCollectionWithSideInput for replicating small SCollections to all left-hand side values in a large SCollection. SCollectionWithSideOutput for output to multiple SCollections. WindowedSCollection for accessing window information. SCollectionWithFanout and SCollectionWithHotKeyFanout for fanout of skewed data.","title":"SCollection"},{"location":"Scio,-Scalding-and-Spark.html#additional-features","text":"Scio also offers some additional features.\nEach worker can pull files from Google Cloud Storage via DistCache to be used in transforms locally, similar to Hadoop distributed cache. See DistCacheExample.scala. Type safe BigQuery IO via Scala macros. Case classes and converters are generated at compile time based on BQ schema. This eliminates the error prone process of handling generic JSON objects. See TypedBigQueryTornadoes.scala. Sinks (saveAs* methods) return ClosedTap[T] that can be opened either in another pipeline as SCollection[T] or directly as Iterator[T] once the current pipeline completes. This enables complex pipeline orchestration. See WordCountOrchestration.scala.","title":"Additional features"},{"location":"Runners.html","text":"","title":"Runners"},{"location":"Runners.html#runners","text":"Starting Scio 0.4.4, Beam runner is completely decoupled from scio-core, which no longer depend on any Beam runner now. Add runner dependencies to enable execution on specific backends. For example, when using Scio 0.4.7 which depends on Beam 2.2.0, you should add the following dependencies to run pipelines locally and on Google Cloud Dataflow.\nlibraryDependencies ++= Seq(\n  \"org.apache.beam\" % \"beam-runners-direct-java\" % \"2.2.0\",\n  \"org.apache.beam\" % \"beam-runners-google-cloud-dataflow-java\" % \"2.2.0\"\n)","title":"Runners"},{"location":"Runners.html#runner-specific-logic","text":"Dataflow specific logic, e.g. job ID, metrics, were also removed from ScioResult. You can convert between the generic ScioResult and runner specific result types like the example below. Note that currently only DataflowResult is implemented.\nimport com.spotify.scio.{ScioContext, ScioExecutionContext, ScioResult}\n\nobject SuperAwesomeJob {\n  def main(cmdlineArgs: Array[String]): Unit = {\n\n    val sc: ScioContext = ???\n\n    // Job code\n    // ...\n\n    // Generic result only\n    val closedContext: ScioExecutionContext = sc.run()\n    val scioResult: ScioResult = closedContext.waitUntilFinish()\n\n    // Convert to Dataflow specific result\n    import com.spotify.scio.runners.dataflow.DataflowResult\n    val dfResult: DataflowResult = scioResult.as[DataflowResult]\n\n    // Convert back to generic result\n    val scioResult2: ScioResult = dfResult.asScioResult\n\n    ()\n  }\n}\nGiven the Google Cloud project ID and Dataflow job ID, one can also create DataflowResult and ScioResult without running a pipeline. This could be when submitting jobs asynchronously and retrieving metrics later.\nimport com.spotify.scio.runners.dataflow.DataflowResult\n\nobject AnotherAwesomeJob {\n  def main(cmdlineArgs: Array[String]): Unit = {\n    val dfResult = DataflowResult(\"<PROJECT_ID>\", \"<REGION>\", \"<JOB_ID>\")\n    val scioResult = dfResult.asScioResult\n    // Some code\n  }\n}","title":"Runner specific logic"},{"location":"Scio-data-guideline.html","text":"","title":"Data Guidelines"},{"location":"Scio-data-guideline.html#data-guidelines","text":"Here are some common guidelines for building efficient, cost-effective, and maintainable pipelines. They apply to most use cases but you might have to tweak based on your needs. Also see the FAQ page for some common performance issues and remedies.","title":"Data Guidelines"},{"location":"Scio-data-guideline.html#development","text":"Use Scio REPL to get familiar with Scio and perform ad-hoc experiments, but don’t use it as a replacement for unit tests.","title":"Development"},{"location":"Scio-data-guideline.html#storage","text":"Leverage BigQuery, especially BigQuery SELECT query as input whenever possible. BigQuery has a very efficient columnar storage engine, can scale independently from Scio/Dataflow clusters and probably cheaper and easier to write than handcrafted Scala/Java pipeline code. Use BigQuery as an intermediate storage, especially if downstream jobs do a lot of slicing and dicing on rows and columns. Feel free to de-normalize data and use wide rows. Use Bigtable or Datastore depending on requirements for serving pipeline output to production services.","title":"Storage"},{"location":"Scio-data-guideline.html#computation","text":"Prefer combine/aggregate/reduce transforms over groupByKey. Keep in mind that a reduce operation must be associative and commutative. Prefer sum/sumByKey over reduce/reduceByKey for basic data types. They use Algebird Semigroups and are often more optimized than hard coded reduce functions. See AlgebirdSpec.scala for more examples. Understand the performance characteristics of different types of joins and the role of side input cache, see the FAQ for more. Understand the Kryo serialization tuning options and use custom Kryo serializers for objects in the critical pass if necessary.","title":"Computation"},{"location":"Scio-data-guideline.html#execution-parameters","text":"When tuning pipeline execution parameters, start with smaller workerMachineType e.g. default n1-standard-1 to n1-standard-4, and reasonable maxNumWorkers that reflect your input size. Keep in mind that there might be limited availability of large GCE instances and more workers means higher shuffle cost.","title":"Execution parameters"},{"location":"Scio-data-guideline.html#streaming","text":"For streaming jobs with periodically updated state, i.e. log decoration with metadata, keep (and update) states in Bigtable, and do look ups from the streaming job (read more about Bigtable key structure). Also see BigtableDoFn for an abstraction that handles asynchronous Bigtable requests. For streaming, larger worker machine types and SSD for workerDiskType might be more suitable. A typical job with 5 x n1-standard-4 and 100GB SSDs can handle ~30k peak events per second. Also see this article on disk performance.","title":"Streaming"},{"location":"releases/index.html","text":"","title":"Releases"},{"location":"releases/index.html#releases","text":"Apache Beam Compatibility Migration Guides Breaking Changelog v0.12.0 Release Blog","title":"Releases"},{"location":"releases/Apache-Beam.html","text":"","title":"Apache Beam Compatibility"},{"location":"releases/Apache-Beam.html#apache-beam-compatibility","text":"Starting from version 0.3.0, Scio moved from Google Cloud Dataflow Java SDK to Apache Beam as its core dependencies and introduced a few breaking changes.\nDataflow Java SDK 1.x.x uses com.google.cloud.dataflow.sdk namespace. Apache Beam uses org.apache.beam.sdk namespace. Dataflow Java SDK 2.x.x is also based on Apache Beam 2.x.x and uses org.apache.beam.sdk.\nScio 0.3.x depends on Beam 0.6.0 (last pre-stable release) and Scio 0.4.x depends on Beam 2.0.0 (first stable release). Breaking changes in these releases are documented below.","title":"Apache Beam Compatibility"},{"location":"releases/Apache-Beam.html#version-matrices","text":"Early Scio releases depend on Google Cloud Dataflow Java SDK while later ones depend on Apache Beam. Check out the Changelog page for migration guides.\nAlso check out the SDK Version Support Status page. Since a Beam release depends on specific version of Dataflow API, a deprecated Beam version is not guaranteed to work correctly or at all. We strongly recommend upgrading before the deprecation date.\nScio SDK Dependency Description 0.14.x Apache Beam 2.x.x avro removed from scio-core, explicit kryo coder fallback, official tensorflow metadata, hadoop 3 and parquet 1.13 0.13.x Apache Beam 2.x.x scio-elasticsearch6 removed. scio-elasticsearch7 migrated to new client. File based ScioIO param changes. 0.12.x Apache Beam 2.x.x com.spotify.scio.extra.bigquery, com.spotify.scio.pubsub removed. scio-elasticsearch6 deprecated. 0.11.x Apache Beam 2.x.x scio-sql and case-app removed, com.spotify.scio.extra.bigquery deprecated, shaded Beam Avro coder, tensorflow-core-platform 0.3.3 0.10.x Apache Beam 2.x.x Coder implicits, scio-google-cloud-platform 0.9.x Apache Beam 2.x.x Drop Scala 2.11, add Scala 2.13, Guava based Bloom Filter 0.8.x Apache Beam 2.x.x Beam SQL, BigQuery storage API, ScioExecutionContext, Async DoFns 0.7.x Apache Beam 2.x.x Static coders, new ScioIO 0.6.x Apache Beam 2.x.x Cassandra 2.2 0.5.x Apache Beam 2.x.x Better type-safe Avro and BigQuery IO 0.4.x Apache Beam 2.0.0 Stable release Beam 0.3.x Apache Beam 0.6.0 Pre-release Beam 0.2.x Dataflow Java SDK SQL-2011 support 0.1.x Dataflow Java SDK First releases\nScio Version Beam Version Details 0.14.8 2.59.0 This version will be deprecated on August 24, 2025. 0.14.7 2.58.1 This version will be deprecated on August 16, 2025. 0.14.6 2.57.0 This version will be deprecated on June 26, 2025. 0.14.5 2.56.0 This version will be deprecated on May 1, 2025. 0.14.4 2.55.1 This version will be deprecated on April 8, 2025. 0.14.3 2.54.0 This version will be deprecated on February 14, 2025. 0.14.2 2.54.0 This version will be deprecated on February 14, 2025. 0.14.1 2.54.0 This version will be deprecated on February 14, 2025. 0.14.0 2.53.0 This version will be deprecated on January 4, 2025. 0.13.6 2.52.0 This version will be deprecated on November 17, 2024. 0.13.5 2.51.0 This version will be deprecated on October 12, 2024. 0.13.4 2.51.0 This version will be deprecated on October 12, 2024. 0.13.3 2.50.0 Deprecated on August 30, 2024. 0.13.2 2.49.0 Deprecated on July 17, 2024. 0.13.1 2.49.0 Deprecated onJuly 17, 2024. 0.13.0 2.48.0 Deprecated on May 31, 2024. 0.12.8 2.46.0 Deprecated on March 10, 2024. 0.12.7 2.46.0 Deprecated on March 10, 2024. 0.12.6 2.46.0 Deprecated on March 10, 2024. 0.12.5 2.45.0 Deprecated on February 15, 2024. 0.12.4 2.44.0 Deprecated on January 13, 2024. 0.12.3 2.44.0 Deprecated on January 13, 2024. 0.12.2 2.44.0 Deprecated on January 13, 2024. 0.12.1 2.43.0 Deprecated on November 17, 2023. 0.12.0 2.41.0 Deprecated on August 23rd, 2023.","title":"Version matrices"},{"location":"releases/Apache-Beam.html#beam-dependencies","text":"Scio’s other library dependencies are kept in sync with Beam’s to avoid compatibility issues. Scio will typically not bump dependency versions beyond what is supported in Beam due to the large test surface and the potential for data loss.\nYou can find Beam’s dependency list in its Groovy config (substitute the version tag in the URL with the desired Beam version). Additionally, Beam keeps many of its Google dependencies in sync with a central BOM (substitute the version tag in the URL with the value of google_cloud_platform_libraries_bom from Beam). Scio users who suspect incompatibility issues in their pipelines (common issues are GRPC, Netty, or Guava) can run sbt evicted and sbt dependencyTree to ensure their direct and transitive dependencies don’t conflict with Scio or Beam.","title":"Beam dependencies"},{"location":"releases/Apache-Beam.html#release-cycle-and-backport-procedures","text":"Scio has a frequent release cycle, roughly every 2-4 weeks, as compared to months for the upstream Apache Beam. We also aim to stay a step ahead by pulling changes from upstream and contributing new ones back.\nLet’s call the Beam version that Scio depends on current, and upstream master latest. Here are the procedures for backporting changes:\nFor changes available in latest but not in current: - Copy Java files from latest to Scio repo - Rename classes and modify as necessary - Release Scio - Update checklist for the next current version like #633 - Remove change once current is updated\nFor changes we want to make to latest: - Submit pull request to latest - Follow the steps above once merged","title":"Release cycle and backport procedures"},{"location":"releases/migrations/index.html","text":"","title":"Migration Guides"},{"location":"releases/migrations/index.html#migration-guides","text":"Scio v0.7.0 Scio v0.8.0 Scio v0.9.0 Scio v0.10.0 Scio v0.12.0 Scio v0.13.0 Scio v0.14.0","title":"Migration Guides"},{"location":"releases/migrations/v0.7.0-Migration-Guide.html","text":"","title":"Scio v0.7.0"},{"location":"releases/migrations/v0.7.0-Migration-Guide.html#scio-v0-7-0","text":"Scio 0.7.0 comes with major improvements over previous versions. The overall goal is to make Scio safer, faster, more consistent and easier to extend by leveraging Scala’s type system more and refactoring its internals.\nThe new milestone has been profiled more than ever and will improve the performances of your pipeline. In some cases, the improvement can be very significant.\nScio 0.7.0 also includes, like every release, a number of bugfixes.","title":"Scio v0.7.0"},{"location":"releases/migrations/v0.7.0-Migration-Guide.html#whats-new-","text":"","title":"What’s new ?"},{"location":"releases/migrations/v0.7.0-Migration-Guide.html#new-ios-api","text":"In this version, we’ve refactored the implementation of IOs. Scio now provides a new class ScioIO that you can extend to support new types of IOs. ScioContext now has a new method called read and SCollection now has a new method write. Both take an instance of a class extending ScioIO as a parameter and may read from any source, or write to any target.\nAll existing IOs (GCS, BigQuery, BigTable, etc.) have been rewritten to use the new IO API.\nRead more: ScioIO","title":"New IOs api"},{"location":"releases/migrations/v0.7.0-Migration-Guide.html#new-coders","text":"Scio 0.7.0 also ship with a new Coder implementation that statically resolve the correct Coder for a given type at compile time. In previous versions, Scio would infer the correct coder implementation at runtime, which could lead to poor performances and occasionally, exceptions at runtime.\nRead more: Coders.","title":"New “static” coders"},{"location":"releases/migrations/v0.7.0-Migration-Guide.html#performances-improvements-benchmarks","text":"Thanks to the new static coders implementation, and because of the time we spend on profiling, Scio 0.7.0 should overall be more efficient than previous versions.","title":"Performances improvements & benchmarks"},{"location":"releases/migrations/v0.7.0-Migration-Guide.html#automated-migration","text":"Scio 0.7 comes with a set of scalafix rules that will do most of the heavy lifting automatically. Before you go through any manual step, we recommend you start by applying those rules.\nStart by adding the scalafix sbt plugin to your project/plugins.sbt file\naddSbtPlugin(\"ch.epfl.scala\" % \"sbt-scalafix\" % \"0.9.0\")\nlaunch sbt and run scalafixEnable\n> scalafixEnable\n[info] Set current project to my-amazing-scio-pipeline (in build file:/Users/julient/Documents/my-amazing-scio-pipeline/)","title":"Automated migration"},{"location":"releases/migrations/v0.7.0-Migration-Guide.html#prepare-your-tests","text":"Warning RUN THIS BEFORE UPGRADING SCIO\nYou’ll need to prepare your tests code for migration. For this to run properly, you code needs to compile.\nRun the following command in the sbt shell:\n> test:scalafix https://raw.githubusercontent.com/spotify/scio/main/scalafix/rules/src/main/scala/fix/v0_7_0/FixAvroIO.scala\n[info] Running scalafix on 78 Scala sources\n[success] Total time: 7 s, completed Oct 17, 2018 12:49:31 PM\nOnce FixAvroIO has been applied, you can go ahead and upgrade Scio to 0.7.x in your build file. After you have set Scio’s version in your build.sbt, make sure to either restart or reload sbt.\nYou can now run the automated migration rules. At the moment, we support 4 rules:\nName Description AddMissingImports Add the required imports to access sources / sinks on ScioContext and SCollection RewriteSysProp Replace sys.call(...) by the new syntax FixAvroIO Fix uses of AvroIO in tests BQClientRefactoring Automatically migrate from BigQueryClient to the new BigQuery client\nYou can see all the rules here.\nIn your sbt shell, you can now apply the 3 other rules:\n> scalafix https://raw.githubusercontent.com/spotify/scio/main/scalafix/rules/src/main/scala/fix/v0_7_0/AddMissingImports.scala\n[info] Running scalafix on 173 Scala sources\n[success] Total time: 16 s, completed Oct 17, 2018 12:01:31 PM\n\n> scalafix https://raw.githubusercontent.com/spotify/scio/main/scalafix/rules/src/main/scala/fix/v0_7_0/RewriteSysProp.scala\n[info] Running scalafix on 173 Scala sources\n[success] Total time: 6 s, completed Oct 17, 2018 12:34:00 PM\n\n> scalafix https://raw.githubusercontent.com/spotify/scio/main/scalafix/rules/src/main/scala/fix/v0_7_0/BQClientRefactoring.scala\n[info] Running scalafix on 173 Scala sources\n[success] Total time: 3 s, completed Oct 17, 2018 12:34:20 PM\nAt that point you can try to compile your code and fix the few compilation errors left. The next sections of this guide should contain all the information you need to fix everything.","title":"Prepare your tests"},{"location":"releases/migrations/v0.7.0-Migration-Guide.html#migration-guide","text":"The following section will detail errors you may encounter while migrating from scio 0.6.x to Scio 0.7.x, and help you fix them. If you’ve run the automated migration fixes, you can jump directly to the Add missing context bounds section.","title":"Migration guide"},{"location":"releases/migrations/v0.7.0-Migration-Guide.html#method-xxx-is-not-a-member-of-com-spotify-scio-sciocontext","text":"When using read methods from ScioContext the compiler may issue an error of type method xxx is not a member of com.spotify.scio.ScioContext.\nIOs have been refactored in Scio 0.7.0. Each IO type now lives in the appropriate project and package. It means 2 things:","title":"method xxx is not a member of com.spotify.scio.ScioContext"},{"location":"releases/migrations/v0.7.0-Migration-Guide.html#you-may-need-to-explicitly-add-a-dependency-one-of-scios-subprojects-in-your-build-file","text":"For example, Scio used to pull dependencies on BigQuery IOs even if your pipeline did not use BigQuery at all. With the new IOs, Scio will limit its dependencies to packages you actually use.\nIf your pipeline is using BigQuery, you now need to add scio-bigquery as a dependency of your project:\nlibraryDependencies += \"com.spotify\" %% \"scio-bigquery\" % scioVersion","title":"You may need to explicitly add a dependency one of Scio’s subprojects in your build file"},{"location":"releases/migrations/v0.7.0-Migration-Guide.html#youll-need-to-import-the-appropriate-package-to-gain-access-to-the-io-methods-","text":"For example while migrating a job that reads data from an Avro file, you may see the following compiler error:\n[error] value avroFile is not a member of com.spotify.scio.ScioContext\n[error]     val coll = sc.avroFile[SomeType](uri)\n[error]                            ^\nAll you have to do to fix it is to import IOs from the correct package:\nimport com.spotify.scio.avro._","title":"You’ll need to import the appropriate package to gain access to the IO methods."},{"location":"releases/migrations/v0.7.0-Migration-Guide.html#avroio-or-other-type-of-io-not-found","text":"IOs have been moved out of the com.spotify.scio.testing package. To use them in unit tests (or elsewhere), you’ll need to change the import:\ncom.spotify.scio.testing.BigQueryIO -> com.spotify.scio.bigquery.BigQueryIO com.spotify.scio.testing.{AvroIO, ProtobufIO} -> com.spotify.scio.avro.{AvroIO, ProtobufIO} com.spotify.scio.testing.TextIO -> com.spotify.scio.io.TextIO\nA complete list of IO packages can be found here.\nAdditionally, some IOs are now parameterized. For example, AvroIO must now be parameterized with the Avro record type (either GenericRecord or an extension of SpecificRecordBase). In previous versions of Scio, it was possible in some cases to omit that type. For example:\nimport org.apache.avro.generic.GenericRecord\n\nobject MyScioJob {}\n\nval inputAUri = \"\"\nval inputBUri = \"\"\nval output = \"output\"\n\nobject Schemas {\n  def record1: GenericRecord = ???\n  def record2: GenericRecord = ???\n}\n\nimplicit def bo: com.spotify.scio.testing.JobTest.BeamOptions = ??? // XXX: why do I need this ?\nimport com.spotify.scio.io._\nimport com.spotify.scio.avro._\nimport com.spotify.scio.testing._\n\nclass MyScioJobTest extends PipelineSpec {\n\n  \"MyScioJob\" should \"work\" in {\n    JobTest[MyScioJob.type]\n      .args(s\"--inputAUri=${inputAUri}\")\n      .args(s\"--inputBUri=${inputBUri}\")\n      .input(AvroIO[GenericRecord](inputAUri), Seq(Schemas.record1))\n      .input(AvroIO[GenericRecord](inputBUri), Seq(Schemas.record2))\n      .output(TextIO(output)){ coll =>\n        coll should haveSize (1)\n        ()\n      }\n      .run()\n  }\n\n  // more tests\n}","title":"AvroIO (or other type of IO) not found"},{"location":"releases/migrations/v0.7.0-Migration-Guide.html#avro-type-inference-issue-","text":"If you use AvroIO you may see the compilation of your tests failing with an error looking like\n[error] <path>/SomeTest.scala:42:20: diverging implicit expansion for type com.spotify.scio.coders.Coder[K]\n[error] starting with macro method gen in trait LowPriorityCoderDerivation\n[error]       .input(AvroIO(inputUri), inputs)\n[error]\nThe problem is that line does not explicitly set the type of the IO:\n.input(AvroIO(inputUri), inputs)\nIn Scio <= 0.6.x this works, but in Scio 0.7.x, you’ll need to be explicit about the types. For example in that case:\n.input(AvroIO[GenericRecord](inputUri), inputs)","title":"Avro type inference issue: “diverging implicit expansion”"},{"location":"releases/migrations/v0.7.0-Migration-Guide.html#bigquery-client","text":"Client was renamed from BigQueryClient to BigQueryand relocated! Now you need to:\nimport com.spotify.scio.bigquery.client.BigQuery\nThe new client offers now methods namespaced according to their resposabilities.\nMethods typically fall into 4 categories of operations query, table, export and load. i.e:\n-client.extractLocation\n+client.query.extracLocation\n\n-client.getQuerySchema(...)\n+client.query.schema(...)\n\n-client.getTableRows(...)\n+client.table.rows(...)\n\n// the same thing applies for the other formats\n-client.loadTableFromCsv(...)\n+client.load.csv(...)\n\n// the same thing applies for the other formats\n-client.exportTableAsCsv(...)\n+client.export.asCsv(...)","title":"BigQuery client"},{"location":"releases/migrations/v0.7.0-Migration-Guide.html#not-enough-arguments-for-method-top-topbykey-approxquantilesbykey-implicit-ord-ordering-","text":"Explicit Ordering functions for SCollection reducers are no longer curried. Methods that used to look like:\n.top(10)(Ordering.by(...)\nshould be changed to:\n.top(10, Ordering.by(...))","title":"Not enough arguments for method (top|topByKey|approxQuantilesByKey): (implicit ord: Ordering[…])"},{"location":"releases/migrations/v0.7.0-Migration-Guide.html#add-missing-context-bounds","text":"In the process of upgrading Scio, you may encounter the following error:\nCannot find a Coder instance for type T\nIf you’ve defined a generic function that uses an SCollection, this function is likely to need a Coder[T]. Scio will require you to provide an implicit Coder[T]. You can read about Scala implicit parameters here\nLet’s see a simple example. Say I created the following method doSomething:\ndef doSomething[T](coll: SCollection[T]): SCollection[T] =\n  coll.map { t =>\n    // do something that returns a T\n    val result: T = ???\n    result\n  }\nIf I try to compile this method the compiler will return the following error:\nCannot find a Coder instance for type:\n\n  >> T\n\n  This can happen for a few reasons, but the most common case is that a data\n  member somewhere within this type doesn't have a Coder instance in scope. Here are\n  some debugging hints:\n    - For Option types, ensure that a Coder instance is in scope for the non-Option version.\n    - For List and Seq types, ensure that a Coder instance is in scope for a single element.\n    - You can check that an instance exists for Coder in the REPL or in your code:\n        scala> Coder[Foo]\n    And find the missing instance and construct it as needed.\n\n  coll.map { t =>\n           ^\nWhat this message says is that calling .map { ... } requires a Coder[T]. You can fix this very easily by adding a new implicit parameter to your method:\nimport com.spotify.scio.coders.Coder\nimport com.spotify.scio.values.SCollection\n\ndef doSomething[T](coll: SCollection[T])(implicit coder: Coder[T]): SCollection[T] =\n  coll.map { t =>\n    // do something that returns a T\n    val result: T = ???\n    result\n  }\nAlternatively, the same result can be achieved using Scala’s context bound syntax:\nimport com.spotify.scio.coders.Coder\nimport com.spotify.scio.values.SCollection\n\ndef doSomething[T: Coder](coll: SCollection[T]): SCollection[T] =\n  coll.map { t =>\n    // do something that returns a T\n    val result: T = ???\n    result\n  }","title":"Add missing context bounds"},{"location":"releases/migrations/v0.7.0-Migration-Guide.html#replacing-kryo-fallbacks-with-your-own-coders-","text":"Most of the time, the compiler will be able to find or derive an appropriate Coder automatically. Sometimes, it may not be able to find one automatically. This will typically happen for:\nClasses defined in Java Scala classes that are not case classes Classes with a private constructor\nIn those cases, Scio will fallback to using Kryo.","title":"Replacing Kryo fallbacks with your own coders."},{"location":"releases/migrations/v0.7.0-Migration-Guide.html#showing-all-fallback-at-compile-time","text":"The compiler can show a message each time a fallback is used. To activate that feature, just the the following scalac option: -Xmacro-settings:show-coder-fallback=true.\nYou can fix this warning in two ways:\nImplement a proper Coder for this type Make it explicit that the Kryo coder is in fact the one you want to use.\nIn both cases you want to define a Coder in your own code. The only difference is how you’ll implement it.\nLet’s say you are using an SCollection[java.util.Locale]:\nimport com.spotify.scio.values.SCollection\n\ndef doSomething(coll: SCollection[String]): SCollection[java.util.Locale] =\n  coll.map { t =>\n    // do something that returns a Locale\n    val result: java.util.Locale = ???\n    result\n  }","title":"Showing all fallback at compile time"},{"location":"releases/migrations/v0.7.0-Migration-Guide.html#using-kryo-explicitly","text":"If you want to explicitly use Kryo (which will probably be the case) you can do the following:\nimport java.util.Locale\nimport com.spotify.scio.coders.Coder\n\nobject Coders {\n  implicit val coderLocale: Coder[Locale] = Coder.kryo[Locale]\n}\nNow all you have to do is make that available at call site:\nimport com.spotify.scio.values.SCollection\nimport Coders._\n\ndef doSomething(coll: SCollection[String]): SCollection[java.util.Locale] =\n  coll.map { t =>\n    // do something that returns a Locale\n    val result: java.util.Locale = ???\n    result\n  }","title":"Using Kryo explicitly"},{"location":"releases/migrations/v0.7.0-Migration-Guide.html#defining-a-custom-coder","text":"If you want to implement custom coders, see Scio’s source code for examples.\nWarning Before implementing custom coders, we recommend that you test your pipeline with the default coders. Implementing custom coders can be tricky, so make sure there’s a clear benefit in doing it. If you implement custom Coders, you need to make sure they are Serializable.","title":"Defining a custom Coder"},{"location":"releases/migrations/v0.7.0-Migration-Guide.html#wartremover-compatibility-","text":"Automatically derived coders are generated by a macro. Unfortunately, if you use WartRemover in your project, the macro will trigger warnings. There’s not much we can do in the macro to fix the issue right now, so you’ll have to disable a few warts. Here are the warts you’ll need to disable in your project:\n- Any\n- IsInstanceOf\n- Throw\nIf you use sbt-wartremover, you can disable them in your build like this:\nwartremoverErrors in (Compile, compile) := {\n  val disableWarts = Set(\n    Wart.Any,\n    Wart.IsInstanceOf,\n    Wart.Throw\n  )\n  Warts.unsafe.filterNot(disableWarts.contains)\n},\n\nwartremoverErrors in (Test, compile) := {\n  val disableWarts = Set(\n    Wart.Any,\n    Wart.IsInstanceOf,\n    Wart.Throw\n  )\n  Warts.unsafe.filterNot(disableWarts.contains)\n}","title":"WartRemover compatibility."},{"location":"releases/migrations/v0.8.0-Migration-Guide.html","text":"","title":"Scio v0.8.0"},{"location":"releases/migrations/v0.8.0-Migration-Guide.html#scio-v0-8-0","text":"","title":"Scio v0.8.0"},{"location":"releases/migrations/v0.8.0-Migration-Guide.html#tl-dr","text":"BeamSQL integration BigQuery Storage api support Generic case class conversion Remove the usage of Future New BigQuery method signatures New async DoFns Remove tensorflow methods related to schema inference Remove support for lisp-case CLI arguments","title":"TL;DR"},{"location":"releases/migrations/v0.8.0-Migration-Guide.html#new-features","text":"","title":"New features"},{"location":"releases/migrations/v0.8.0-Migration-Guide.html#beamsql","text":"Beam SQL integration is added in this release! This integration comes in many flavors, from fluent api to string interpolation with both offering the possibility to typecheck the provided query at compile time.\nA simple use case of this api is reflected in the example below. This example uses the fluent api to query the SCollection[User] and extract username and age. query return’s Row which is Beam’s underlying type that contains the values and the Schema of the extracted data.\nimport com.spotify.scio.sql._\nimport com.spotify.scio.values._\nimport org.apache.beam.sdk.values.Row\n\ncase class User(username: String, email: String, age: Int)\n\ndef users: SCollection[User] = ???\n\ndef result: SCollection[Row] = users.query(\"select username, age from SCOLLECTION\")\nIn the following example we go a little bit further. Using queryAs we can express the expected return type (String, Int) instead of Row. Query typecheck only happens at runtime\nThis will fail at runtime if the expected return type doesn’t match the inferred schema for the given query.\nimport com.spotify.scio.sql._\nimport com.spotify.scio.values._\n\ncase class User(username: String, email: String, age: Int)\n\ndef users: SCollection[User] = ???\n\ndef result: SCollection[(String, Int)] = users.queryAs(\"select username, age from SCOLLECTION\")\nString interpolation is another way to express SQL queries. As we can see from the following example it behaves exactly as any other string interpolation with the added expression of the expected return type. As in the previous example the return type is not typechecked at compile time with the inferred query schema. Any mismatch will result in runtime exception.\nimport com.spotify.scio.sql._\nimport com.spotify.scio.values._\n\ncase class User(username: String, email: String, age: Int)\n\ndef users: SCollection[User] = ???\n\ndef result: SCollection[(String, Int)] = sql\"select username, age from $users\".as[(String, Int)]\nTypecheck at compile time is provided by tsql. Here’s the same example but with this new method:\nimport com.spotify.scio.sql._\nimport com.spotify.scio.values._\n\ncase class User(username: String, email: String, age: Int)\n\ndef users: SCollection[User] = ???\n\ndef result: SCollection[(String, Int)] = tsql\"select username, age from $users\".as[(String, Int)]","title":"BeamSQL"},{"location":"releases/migrations/v0.8.0-Migration-Guide.html#handling-sql-query-errors","text":"SQL query errors can happen! They might have syntax errors, wrong fields and even wrong expected types. To help you out, we have some gorgeous error messages for you!\nUsing the users collection from previous example, here are some error messages you might encounter.\nSelecting wrong field from users:\n- tsql\"select username, age from $users\".as[(String, Int)]\n+ tsql\"select username, foo from $users\".as[(String, Int)]\nSqlValidatorException: Column 'foo' not found in any table\n\nQuery:\nselect username, foo from SCOLLECTION\n\n\nschema of SCOLLECTION:\n┌──────────────────────────────────────────┬──────────────────────┬──────────┐\n│ NAME                                     │ TYPE                 │ NULLABLE │\n├──────────────────────────────────────────┼──────────────────────┼──────────┤\n│ username                                 │ STRING               │ NO       │\n│ email                                    │ STRING               │ NO       │\n│ age                                      │ INT32                │ NO       │\n└──────────────────────────────────────────┴──────────────────────┴──────────┘\n\nQuery result schema (inferred) is unknown.\nExpected schema:\n┌──────────────────────────────────────────┬──────────────────────┬──────────┐\n│ NAME                                     │ TYPE                 │ NULLABLE │\n├──────────────────────────────────────────┼──────────────────────┼──────────┤\n│ _1                                       │ STRING               │ NO       │\n│ _2                                       │ INT32                │ NO       │\n└──────────────────────────────────────────┴──────────────────────┴──────────┘\nProviding a SQL query with syntax error:\n- tsql\"select username, age from $users\".as[(String, Int)]\n+ tsql\"select username, age fom $users\".as[(String, Int)]\nParseException: Encountered \"users\" at line 1, column 27.\nWas expecting one of:\n    <EOF>\n    \"ORDER\" ...\n    \"LIMIT\" ...\n    \"OFFSET\" ...\n    \"FETCH\" ...\n    \"FROM\" ...\n    \",\" ...\n    \"UNION\" ...\n    \"INTERSECT\" ...\n    \"EXCEPT\" ...\n    \"MINUS\" ...\n\n\nQuery:\nselect username, age fom  users\n\n\nschema of users:\n┌──────────────────────────────────────────┬──────────────────────┬──────────┐\n│ NAME                                     │ TYPE                 │ NULLABLE │\n├──────────────────────────────────────────┼──────────────────────┼──────────┤\n│ username                                 │ STRING               │ NO       │\n│ email                                    │ STRING               │ NO       │\n│ age                                      │ INT32                │ NO       │\n└──────────────────────────────────────────┴──────────────────────┴──────────┘\n\nQuery result schema (inferred) is unknown.\nExpected schema:\n┌──────────────────────────────────────────┬──────────────────────┬──────────┐\n│ NAME                                     │ TYPE                 │ NULLABLE │\n├──────────────────────────────────────────┼──────────────────────┼──────────┤\n│ _1                                       │ STRING               │ NO       │\n│ _2                                       │ INT32                │ NO       │\n└──────────────────────────────────────────┴──────────────────────┴──────────┘\nReturn type different from the inferred one:\n- tsql\"select username, age from $users\".as[(String, Int)]\n+ tsql\"select username, age fom $users\".as[String]\nInferred schema for query is not compatible with the expected schema.\n\nQuery:\nselect username, age from  users\n\n\nschema of users:\n┌──────────────────────────────────────────┬──────────────────────┬──────────┐\n│ NAME                                     │ TYPE                 │ NULLABLE │\n├──────────────────────────────────────────┼──────────────────────┼──────────┤\n│ username                                 │ STRING               │ NO       │\n│ email                                    │ STRING               │ NO       │\n│ age                                      │ INT32                │ NO       │\n└──────────────────────────────────────────┴──────────────────────┴──────────┘\n\nQuery result schema (inferred):\n┌──────────────────────────────────────────┬──────────────────────┬──────────┐\n│ NAME                                     │ TYPE                 │ NULLABLE │\n├──────────────────────────────────────────┼──────────────────────┼──────────┤\n│ username                                 │ STRING               │ NO       │\n│ age                                      │ INT32                │ NO       │\n└──────────────────────────────────────────┴──────────────────────┴──────────┘\n\nExpected schema:\n┌──────────────────────────────────────────┬──────────────────────┬──────────┐\n│ NAME                                     │ TYPE                 │ NULLABLE │\n├──────────────────────────────────────────┼──────────────────────┼──────────┤\n│ value                                    │ STRING               │ NO       │\n└──────────────────────────────────────────┴──────────────────────┴──────────┘","title":"Handling SQL query Errors"},{"location":"releases/migrations/v0.8.0-Migration-Guide.html#bigquery-storage-api","text":"BigQuery Storage API provides fast access to BigQuery managed storage by using an rpc-based protocol.\nIf you already use BigQuery, the BigQuery Storage api that we provide will look very familiar as it provides the standard and the type safe api. Switching to this new strategy should be very straightforward.\nUsing the type safe api is almost the same as the previous provided strategies. We just need to use @BigQueryType.fromStorage. The example below retrieves all columns from a given table.\nimport com.spotify.scio.bigquery._\n\n@BigQueryType.fromStorage(\"data-integration-test:storage.nested\")\nclass Example\nHowever if you don’t want to pull everything, you can always specify which fields you want and even set some filtering. Note that the field names in selectedFields must appear in the same order as the columns appear in the table.\nimport com.spotify.scio.bigquery._\n\n@BigQueryType.fromStorage(\n    \"data-integration-test:storage.%s\",\n    List(\"nested\"),\n    selectedFields = List(\"required\", \"optional.int\"),\n    rowRestriction = \"required.int < 5\"\n)\nclass Example\nUsing the above annotated classes can be done through the following methods.\nimport com.spotify.scio._\nimport com.spotify.scio.bigquery._\nimport com.spotify.scio.values._\n\ndef sc: ScioContext = ???\n\ndef below: SCollection[Example] = sc.typedBigQuery[Example]()\n\n// or\n\ndef above: SCollection[Example] = sc.typedBigQueryStorage[Example](rowRestriction = \"required.int > 5\")\nWhen not using the type safe api you can always read as:\nimport com.spotify.scio._\nimport com.spotify.scio.values._\nimport com.spotify.scio.bigquery._\n\ndef sc: ScioContext = ???\n\ndef result: SCollection[TableRow] = sc.bigQueryStorage(\n    Table.Spec(\"apache-beam-testing:samples.weather_stations\"),\n    selectedFields = List(\"tornado\", \"month\"),\n    rowRestriction = \"tornado = true\"\n  )","title":"BigQuery Storage API"},{"location":"releases/migrations/v0.8.0-Migration-Guide.html#generic-case-class-type-conversion","text":"With the introduction of automatic schema derivation for data types it becomes really easy to convert between “compatible” generic case classes.\nimport com.spotify.scio.values._\nimport com.spotify.scio.schemas._\n\ncase class FromExample(i: Int, s: String)\n\ncase class ToExample(s: String)\n\ndef examples: SCollection[FromExample] = ???\nTo convert FromExample => ToExample we can use two methods unsafe and safe. The main difference between them is when the conversion compatibility check happens. With unsafe it happens at runtime while with safe it’s at compile time.\ndef unsafe: SCollection[ToExample] = examples.to[ToExample](To.unsafe)\n\ndef safe: SCollection[ToExample] = examples.to[ToExample](To.safe)","title":"Generic case class type conversion"},{"location":"releases/migrations/v0.8.0-Migration-Guide.html#deprecations-and-breaking-changes","text":"","title":"Deprecations and Breaking changes"},{"location":"releases/migrations/v0.8.0-Migration-Guide.html#scala-concurrent-future-removed-from-scioios","text":"The removal of Future led to some semantic and behaviour changes.\nScioIOs no longer return Future\ntrait ScioIO[T] {\n  ....\n- protected def write(data: SCollection[T], params: WriteP): Future[Tap[tapT.T]]\n+ protected def write(data: SCollection[T], params: WriteP): Tap[tapT.T]\n  ...\n}\nSCollection#write returns ClosedTap[T]\nsealed trait SCollection[T] extends PCollectionWrapper[T] {\n    ...\n- def write(io: ScioIO[T])(params: io.WriteP)(implicit coder: Coder[T]): Future[Tap[io.tapT.T]]\n+ def write(io: ScioIO[T])(params: io.WriteP)(implicit coder: Coder[T]): ClosedTap[io.tapT.T]\n    ...\n}\nClosedTap[T] encapsulates the IO Tap[T] and it’s only possible to read from it once the pipeline execution is done. This is demonstrated in the following example:\nimport com.spotify.scio._\nimport com.spotify.scio.io._\n\ndef sc: ScioContext = ???\ndef closedTap: ClosedTap[String] =\n     sc\n      .parallelize(1 to 100)\n      .sum\n      .map(_.toString)\n      .saveAsTextFile(\"...\")\n\ndef scioResult: ScioResult = sc.run().waitUntilDone()\n\n// open tap for read\ndef openedTap: Tap[String] = scioResult.tap(closedTap)","title":"scala.concurrent.Future removed from ScioIOs"},{"location":"releases/migrations/v0.8.0-Migration-Guide.html#sciocontext","text":"ScioContext#close changed it’s return type to ScioExecutionContext.\n- def close(): ScioResult\n+ def close(): ScioExecutionContext\nScioContext#close is also being deprecated in favor of ScioContext#run. With this change, --blocking flag is also deprecated.\nTo achieve the same behaviour with the new ScioContext#run you can do the following:\nimport com.spotify.scio._\nimport scala.concurrent.duration._\n\ndef scioResult(sc: ScioContext): ScioResult = sc.run().waitUntilDone(Duration.Inf)","title":"ScioContext"},{"location":"releases/migrations/v0.8.0-Migration-Guide.html#remove-tensorflow-methods-related-to-schema-inference","text":"In scio 0.7.0 scio-tensorflow saw some of its operations being deprecated. They are no longer available in this version and we recommend users to use TensorFlow Data Validation instead.\nRemoved operations:\nsaveExampleMetadata saveAsTfExampleFileWithSchema","title":"Remove tensorflow methods related to schema inference"},{"location":"releases/migrations/v0.8.0-Migration-Guide.html#bigquery","text":"If you are using BigQuery you will see some @deprecated warnings:\n// method bigQueryTable in class ScioContextOps is deprecated (since Scio 0.8):\n// this method will be removed; use bigQueryTable(Table.Spec(table)) instead\nAll BigQuery io operations now support new data types Table to reference a specific table and Query to a query. Every method that uses tableSpec: String, tableReference: TableReference, or query: String has been deprecated.\n- def bigQueryTable(tableSpec: String): SCollection[TableRow]\n- def bigQueryTable(tableReference: TableReference): SCollection[TableRow]\n- def bigQuerySelect(query: String): SCollection[TableRow]\n\n+ def bigQueryTable(table: Table): SCollection[TableRow]\n+ def bigQuerySelect(query: Query): SCollection[TableRow]\nThe new Table type can be created according to the following examples:\ndef tableSpecString: String = ???\n\ndef table: Table = Table.Spec(tableSpecString)\nor:\ndef tableReference: TableReference = ???\n\ndef table: Table = Table.Ref(tableReference)\nThe advantage of this over the previous usage of String or TableReference is that now we are able to safely disambiguate between table spec and table reference.","title":"BigQuery"},{"location":"releases/migrations/v0.8.0-Migration-Guide.html#async-dofns","text":"Async DoFns were refactored.\nAsyncLookupDoFn was renamed to BaseAsyncLookupDoFn and we now have better support for Guava, Java 8 and scala Future lookup DoFn’s through the following implementations GuavaAsyncLookupDoFn, JavaAsyncLookupDoFn and ScalaAsyncLookupDoFn.","title":"Async DoFns"},{"location":"releases/migrations/v0.8.0-Migration-Guide.html#remove-support-for-lisp-case-cli-arguments","text":"In order to be consistent with Beam’s way of passing arguments into the application and construct PipelineOptions, we decided to drop support for lisp-case arguments.\nWhat this means is that if you were passing arguments like --foo-bar now you need to pass it as --fooBar.","title":"Remove support for lisp-case CLI arguments"},{"location":"releases/migrations/v0.9.0-Migration-Guide.html","text":"","title":"Scio v0.9.0"},{"location":"releases/migrations/v0.9.0-Migration-Guide.html#scio-v0-9-0","text":"","title":"Scio v0.9.0"},{"location":"releases/migrations/v0.9.0-Migration-Guide.html#tl-dr","text":"Bloom Filter IO’s ScioContext Scala 2.11 drop\nWarning For all the details, refer to the release notes on GitHub.","title":"TL;DR"},{"location":"releases/migrations/v0.9.0-Migration-Guide.html#bloom-filter","text":"In 0.9.0 we switched from our custom Bloom Filter implementation to Guava Bloom Filter for sparse transforms, e.g. sparseJoin, sparseLookup. As a result, we also switched from Algebird Hash128[K] to Guava Funnel[K] type class for hashing items into the Bloom Filter. Implicit Funnel[K] instances are available through magnolify-guava and need to be imported like this:\nimport magnolify.guava.auto._\nIf for error messages like this:\ncould not find implicit value for parameter hash: com.spotify.scio.hash.BloomFilter.Hash[T]\nThe switch also adds the following benefits:\nPreviously Hash128[K] only provides instances for Int, Long, String, Array[Byte], Array[Int] and Array[Long], while magnolify-guava can derive Funnel[K] for most common types including tuples, case classes, etc.\nWe also added an ApproxFilter abstraction to allow extensible approximate filter implementations. BloomFilter extends ApproxFilter and allows us to create filters & side inputs from Iterable[T] & SCollection[T]. The result filter instances are serializable. For example:\nimport com.spotify.scio._\nimport com.spotify.scio.coders.Coder\nimport com.spotify.scio.hash._\nimport com.spotify.scio.values._\nimport magnolify.guava._\n\nval bf: BloomFilter[String] = Seq(\"a\", \"b\", \"c\").asApproxFilter(BloomFilter)\n\nval sc = ScioContext()\nval data = sc.parallelize(Seq(\"a\", \"b\", \"c\"))\nval bfs: SCollection[BloomFilter[String]] = data.asApproxFilter(BloomFilter)\nval bfsi: SideInput[BloomFilter[String]] = data.asApproxFilterSideInput(BloomFilter)\n\nval bfCoder: Coder[BloomFilter[String]] = BloomFilter.filterCoder","title":"Bloom Filter"},{"location":"releases/migrations/v0.9.0-Migration-Guide.html#ios","text":"","title":"IO’s"},{"location":"releases/migrations/v0.9.0-Migration-Guide.html#bigquery","text":"In scio 0.8.0 we introduced some deprecations and with this version, we are enforcing them. What this means is that all BigQuery operations should expect a Table type that can be created either from a table reference or spec:\ndef tableSpecString: String = ???\n\ndef table: Table = Table.Spec(tableSpecString)\nor\ndef tableReference: TableReference = ???\n\ndef table: Table = Table.Ref(tableReference)\nBellow are some of the affected methods and suggestion on how you can migrate:\n- typedBigQuery(table: TableReference, ...)\n+ typedBigQuery(table: Table.Ref(tableReference), ...)\n\n- typedBigQuery(tableSpec: String, ...)\n+ typedBigQuery(tableSpec: Table.Spec(tableSpec), ...)\n\n- saveAsBigQuery(table: TableReference, ...)\n+ saveAsBigQueryTable(table: Table.Ref(tableReference), ...)\n\n- saveAsBigQuery(tableSpec: String, ...)\n+ saveAsBigQueryTable(tableSpec: Table.Spec(tableSpec), ...)\n\n- saveAsTypedBigQuery(table: TableReference, ...)\n+ saveAsTypedBigQueryTable(table: Table.Ref(tableReference), ...)\n\n- saveAsTypedBigQuery(tableSpec: String, ...)\n+ saveAsTypedBigQueryTable(tableSpec: Table.Spec(tableSpec), ...)\nMethods with only argument type change:\n- bigQuerySelect(query: String, ...)\n+ bigQuerySelect(query: Query(sql), ...)\n\n- saveAsTypedBigQuery(table: TableReference, ...)\n+ saveAsTypedBigQuery(table: Table.Ref(tableReference), ...)\n\n- saveAsTypedBigQuery(tableSpec: String, ...)\n+ saveAsTypedBigQuery(tableSpec: Table.Spec(tableSpec), ...)","title":"BigQuery"},{"location":"releases/migrations/v0.9.0-Migration-Guide.html#bigquery-deprecations","text":"With 0.9.0 we introduced a new method queryRaw to BigQueryType.fromQuery and deprecated the existing one query. This is scheduled for removal in the next release.","title":"BigQuery deprecations"},{"location":"releases/migrations/v0.9.0-Migration-Guide.html#avro","text":"ReflectiveRecordIO was removed in this release and this means that we no longer need to pass a type param when reading GenericRecord making things a little bit cleaner. This unfortunately means that you will need to update your code by removing the type param from avroFile.\nval sc: ScioContext = ???\n\n- sc.avroFile[GenericRecord](path, schema)\n+ sc.avroFile(path, schema)","title":"Avro"},{"location":"releases/migrations/v0.9.0-Migration-Guide.html#tensorflow","text":"Removed saveAsTfExampleFile in favor of saveAsTfRecordFile as they express better underlying format in each Example’s are being written.\nval coll: SCollection[T <: Example] = ???\n\n- coll.saveAsTfExampleFile(...)\n+ coll.saveAsTfRecordFile(...)","title":"Tensorflow"},{"location":"releases/migrations/v0.9.0-Migration-Guide.html#end-of-life","text":"scio-cassandra2 and scio-elasticsearch2 reached end-of-life and were removed.","title":"End-of-Life"},{"location":"releases/migrations/v0.9.0-Migration-Guide.html#sciocontext","text":"All the deprecated behavior around execution and pipeline result in 0.8.x was removed!\nThis means that to start your pipeline you need to:\nval sc: ScioContext = ???\n\n- val result: ScioResult = sc.close()\n+ val execution: ScioExecutionContext = sc.run()\nand to get a ScioResult you need to:\nval sc: ScioContext = ???\n\n- val result: ScioResult = sc.close()\n+ val result: ScioResult = sc.run().waitUntilDone(Duration.Inf)","title":"ScioContext"},{"location":"releases/migrations/v0.9.0-Migration-Guide.html#scala-2-11-drop","text":"2.11 served us well! The ecosystem is moving on and so are we! From this version forward we will only support 2.12 and 2.13!\nMigrating from 2.11 to 2.12 should not imply any code update, it should be as easy as updating your build.sbt:\n- scalaVersion := \"2.11.12\"\n+ scalaVersion := \"2.12.13\"\nHowever, migrating to 2.13 might require some changes, especially around collections! we advise you to look at the Scala migration guide for an in-depth overview of the most important changes.","title":"Scala 2.11 drop"},{"location":"releases/migrations/v0.10.0-Migration-Guide.html","text":"","title":"Scio v0.10.0"},{"location":"releases/migrations/v0.10.0-Migration-Guide.html#scio-v0-10-0","text":"","title":"Scio v0.10.0"},{"location":"releases/migrations/v0.10.0-Migration-Guide.html#tl-dr","text":"Google Cloud Platform Modules Coder Implicits","title":"TL;DR"},{"location":"releases/migrations/v0.10.0-Migration-Guide.html#google-cloud-platform-modules","text":"All Google Cloud Platform related components are moved under scio-google-cloud-platform. This includes Datastore, originally part of scio-core and the following modules.\nscio-bigquery scio-bigtable scio-spanner\nUpdate your build.sbt accordingly, for example:\nlibraryDependencies ++= Seq(\n      \"com.spotify\" %% \"scio-core\" % scioVersion,\n-     \"com.spotify\" %% \"scio-bigquery\" % scioVersion,\n-     \"com.spotify\" %% \"scio-bigtable\" % scioVersion,\n-     \"com.spotify\" %% \"scio-spanner\" % scioVersion,\n+     \"com.spotify\" %% \"scio-google-cloud-platform\" % scioVersion,\n      \"com.spotify\" %% \"scio-test\" % scioVersion % Test\n    )","title":"Google Cloud Platform Modules"},{"location":"releases/migrations/v0.10.0-Migration-Guide.html#coder-implicits","text":"Coder inference has been greatly simplified and many cases of (implicit koder: Coder[K], voder: Coder[V]) implicit arguments are removed from the public API. While this is source level compatible for the most part, there are a few exceptions. They should also be covered by the scalafix rules.\nval ints: SCollection[Int] = sc.parallelize(1 to 100)\n    val kvs: SCollection[(String, Int)] = ints.keyBy(\"key\" + _)\n\n    // SCollection methods with implicit Ordering[T]\n-   ints.top(10, Ordering[Int].reverse)\n-   ints.quantileApprox(10, Ordering[Int].reverse)\n+   ints.top(10)(Ordering[Int].reverse)\n+   ints.quantileApprox(10)(Ordering[Int].reverse)\n\n    // PairSCollection methods with implicit Ordering[T]\n-   kvs.topByKey(10, Ordering[Int].reverse)\n-   kvs.approxQuantilesByKey(10, Ordering[Int].reverse)\n+   kvs.topByKey(10)(Ordering[Int].reverse)\n+   kvs.approxQuantilesByKey(10)(Ordering[Int].reverse)","title":"Coder Implicits"},{"location":"releases/migrations/v0.12.0-Migration-Guide.html","text":"","title":"Scio v0.12.0"},{"location":"releases/migrations/v0.12.0-Migration-Guide.html#scio-v0-12-0","text":"","title":"Scio v0.12.0"},{"location":"releases/migrations/v0.12.0-Migration-Guide.html#com-spotify-scio-extra-bigquery-removal","text":"For usages of saveAvroAsBigQuery, use saveAsBigQueryTable from com.spotify.scio.bigquery instead.\n- scoll.saveAvroAsBigQuery(tableRef)\n+ scoll.saveAsBigQueryTable(table)\nNote: you can run the following sbt command to run the relevant scalafix rules to update your BQ API usages:\nsbt \"scalafixEnable; scalafix https://raw.githubusercontent.com/spotify/scio/main/scalafix/rules/src/main/scala/fix/v0_12_0/FixBqSaveAsTable.scala\"","title":"com.spotify.scio.extra.bigquery removal"},{"location":"releases/migrations/v0.12.0-Migration-Guide.html#removal-of-com-spotify-scio-pubsub-specializations","text":"Specialized methods in com.spotify.scio.pubsub have been removed in favor of generic .read and .write methods.\nThe PubsubIO.apply method has been completely removed, use PubsubIO.string, PubsubIO.avro, PubsubIO.proto, PubsubIO.pubsub or PubsubIO.coder instead.\nAdditionally:\nPubsubIO.readString is replaced by PubsubIO.string PubsubIO.readAvro is replaced by PubsubIO.avro PubsubIO.readProto is replaced by PubsubIO.proto PubsubIO.readPubsub is replaced by PubsubIO.pubsub PubsubIO.readCoder is replaced by PubsubIO.coder\nThe pubsubSubscription and pubsubTopic methods are replaced by one of the preceding IOs in conjunction with a PubsubIO.ReadParam. For example:\nsc.read(PubsubIO.string(subscription, idAttribute, timestampAttribute))(PubsubIO.ReadParam(PubsubIO.Subscription))\nsc.read(PubsubIO.string(topic, idAttribute, timestampAttribute))(PubsubIO.ReadParam(PubsubIO.Topic))\nThe pubsubSubscriptionWithAttributes and pubsubTopicWithAttributes methods are replaced by PubsubIO.withAttributes. For example:\nsc.read(PubsubIO.withAttributes[String](subscription, idAttribute, timestampAttribute))(PubsubIO.ReadParam(PubsubIO.Subscription))\nsc.read(PubsubIO.withAttributes[String](topic, idAttribute, timestampAttribute))(PubsubIO.ReadParam(PubsubIO.Topic))\nThe saveAsPubsub and saveAsPubsubWithAttributes are similarly replaced in conjunction with a PubsubIO.WriteParam:\nscoll.write(PubsubIO.string(topic, idAttribute, timestampAttribute))(PubsubIO.WriteParam())\nscoll.write(PubsubIO.withAttributes[String](topic, idAttribute, timestampAttribute))(PubsubIO.WriteParam())\nNote: you can run the following sbt command to run the relevant scalafix rules to automatically update deprecated Pub/Sub API usages:\nsbt \"scalafixEnable; scalafix https://raw.githubusercontent.com/spotify/scio/main/scalafix/rules/src/main/scala/fix/v0_12_0/FixPubsubSpecializations.scala\"","title":"Removal of com.spotify.scio.pubsub specializations"},{"location":"releases/migrations/v0.12.0-Migration-Guide.html#changed-type-signatures-of-smb-methods","text":"There are substantial changes to the java SMB API to accommodate secondary-keyed SMB. For example, AvroSortedBucketIO.Write changes signature from Write<K, T extends GenericRecord> to Write<K1, K2, T extends GenericRecord>. Most users will only interact with the scala API.","title":"Changed type signatures of SMB methods"},{"location":"releases/migrations/v0.12.0-Migration-Guide.html#removed-beam-sql","text":"Removed typedBigQueryTable methods. Use instead bigQuerySelect, bigQueryTable, bigQueryStorage, typedBigQuery, or typedBigQueryStorage.","title":"Removed Beam-SQL"},{"location":"releases/migrations/v0.12.0-Migration-Guide.html#file-io-file-naming","text":"File-based IO methods now have a consistent file-naming interface. saveAs* methods now accept, in addition to path and suffix, an optional shardNameTemplate, tempDirectory, and filenamePolicySupplier. shardNameTemplate and filenamePolicySupplier are mutually exclusive.\nshardNameTemplate is a string pattern for filenames as accepted by DefaultFilenamePolicy.constructName.\nfilenamePolicySupplier is an instance of FilenamePolicySupplier, which takes the path and suffix as provided to most saveAs* methods and returns a FilenamePolicy.","title":"File IO file naming"},{"location":"releases/migrations/v0.12.0-Migration-Guide.html#binaryio-saveasbinaryfile","text":"saveAsBinaryFile has been updated to use FilenamePolicySupplier per above and drops support for FileNaming.","title":"BinaryIO saveAsBinaryFile"},{"location":"releases/migrations/v0.12.0-Migration-Guide.html#parquetio-saveasdynamicparquetavrofile-saveasparquetavrofile","text":"saveAsDynamicParquetAvroFile had an inconsistent interface compared to other saveAsDynamic* methods.\nThe pre-0.12 behavior of the filenameFunction parameter of saveAsDynamicParquetAvroFile is now supported via the FilenamePolicySupplier parameter of the non-dynamic saveAsParquetAvroFile, per above.\nA new and more consistent saveAsDynamicParquetAvroFile is added:\nimport com.spotify.scio.values.SCollection\nimport com.spotify.scio.parquet.avro.dynamic._\ncase class MyClass(s: String, i: Int)\nval scoll: SCollection[MyClass] = ???\nscoll.saveAsDynamicParquetAvroFile(\"gs://output/\") { m => s\"/${m.s}/${m.i}\"}","title":"ParquetIO saveAsDynamicParquetAvroFile → saveAsParquetAvroFile"},{"location":"releases/migrations/v0.12.0-Migration-Guide.html#parquet-reads","text":"Alongside the existing Parquet read implementation (“legacy Parquet”), we’re concurrently offering a new Parquet read implementation that uses Beam’s new SplittableDoFn API. Legacy Parquet is still the default read format, but can enable the new implementation in your Configuration:\nimport com.spotify.scio.parquet._\n\nsc.typedParquetFile[T](path, conf = ParquetConfiguration.of(\"scio.parquet.read.useSplittableDoFn\" -> true))\nsc.parquetAvroFile[T](path, conf = ParquetConfiguration.of(\"scio.parquet.read.useSplittableDoFn\" -> true))\nsc.parquetExampleFile(path, conf = ParquetConfiguration.of(\"scio.parquet.read.useSplittableDoFn\" -> true))\nAdditionally, you can enable it for all Scio jobs in your project by adding it to your project’s src/main/resources/core-site.xml file:\n<configuration>\n  <property>\n    <name>scio.parquet.read.useSplittableDoFn</name>\n    <value>true</value>\n    <description>Use SplittableDoFn implementation for Parquet reads</description>\n  </property>\n</configuration>\nNote that if you’re using DataflowRunner, you’ll get the best performance (in terms of worker scaling and overall resource usage) out of a SplittableDoFn-based read by enabling Dataflow Runner V2. You can enable this in your Dataflow pipeline by supplying the pipeline argument --experiments=use_runner_v2 to your job.\nOur plan is to support Legacy Parquet for all Scio 0.12.x versions, but fully deprecate and remove support by 0.13.x.","title":"Parquet Reads"},{"location":"releases/migrations/v0.12.0-Migration-Guide.html#async-lookup-dofn","text":"All Async lookup DoFn have been reworked and now extends DoFnWithResource. After upgrade, you’ll get the following error:\nclass MyLookupDoFn needs to be abstract, since method getResourceType in class DoFnWithResource of type ()com.spotify.scio.transforms.DoFnWithResource.ResourceType is not defined\nYou must now implement the method and return the appropriate resource type for your client: - ResourceType.PER_INSTANCE if your client is thread safe (this was the previous behavior) - ResourceType.PER_CLONE if your client is not thread safe - ResourceType.PER_CLASS if your client is meant to be shared among all instances","title":"Async lookup DoFn"},{"location":"releases/migrations/v0.13.0-Migration-Guide.html","text":"","title":"Scio v0.13.0"},{"location":"releases/migrations/v0.13.0-Migration-Guide.html#scio-v0-13-0","text":"","title":"Scio v0.13.0"},{"location":"releases/migrations/v0.13.0-Migration-Guide.html#gcs-connector-now-explicitly-required","text":"Previously Scio shipped with com.google.cloud.bigdataoss:gcs-connector as part of scio-parquet. This dependency is now removed, so gcs-connector must be explicitly enabled if using parquet on GCS:\nval bigdataossVersion = \"2.2.6\"\n\nlibraryDependencies ++= Seq(\n  \"com.google.cloud.bigdataoss\" % \"gcs-connector\" % s\"hadoop2-$bigdataossVersion\"\n)","title":"gcs-connector now explicitly required"},{"location":"releases/migrations/v0.13.0-Migration-Guide.html#removed-scio-elasticsearch6","text":"Please migrate to scio-elasticsearch8.","title":"Removed scio-elasticsearch6"},{"location":"releases/migrations/v0.13.0-Migration-Guide.html#scio-elasticsearch7-migrated-to-java-client","text":"saveAsElasticsearch now requires a transform function returning co.elastic.clients.elasticsearch.core.bulk.BulkOperation instead of org.elasticsearch.action.DocWriteRequest.","title":"scio-elasticsearch7 migrated to java client"},{"location":"releases/migrations/v0.13.0-Migration-Guide.html#new-file-based-scioio-parameters","text":"File-based IOs now consistently have a suffix parameter. In cases where ReadParam was Unit, then a new param will be required. This is the case for example with AvroIO and GenericRecordIO:\n- sc.read(GenericRecordIO(path, schema))\n+ sc.read(GenericRecordIO(path, schema))(AvroIO.ReadParam(suffix))\n- sc.read(SpecificRecordIO[T](path))\n+ sc.read(SpecificRecordIO[T](path))(AvroIO.ReadParam(suffix))","title":"New File based ScioIO parameters"},{"location":"releases/migrations/v0.13.0-Migration-Guide.html#kryo-coders-nondeterministic","text":"Kryo coders in Scio have long been marked as deterministic but users were cautioned to not use them in cases where determinism is important (e.g. with distinct or to encode keys in keyed operations) and when the Kryo coders were not explicitly known to be deterministic. Users who did not understand or follow these instructions could silently produce corrupt data or incomplete results.\nKryo coders are now marked as nondeterministic in all cases and an exception will be thrown if used in keyed operations.","title":"Kryo Coders nondeterministic"},{"location":"releases/migrations/v0.13.0-Migration-Guide.html#changed-skewedjoin-api","text":"Removes some variants of skewedJoin APIs with Long threshold parameters. Use the variants with a HotKeyMethod parameter instead, providing HotKeyMethod.Threshold(myThresold) as its value.","title":"Changed skewedJoin API"},{"location":"releases/migrations/v0.13.0-Migration-Guide.html#tensorflow-unused-predict-type-parameter","text":"The Tensorflow predict and predictWithSigDef methods had an unused type parameter that is now removed.\n- elements.predict[B, D](\"gs://model-path\", fetchOpts, options)(toTensors)(fromTensors)\n+ elements.predict[B](\"gs://model-path\", fetchOpts, options)(toTensors)(fromTensors)\n- elements.predictWithSigDef[B, D](\"gs://model-path\", options)(toTensors)(fromTensors _)\n+ elements.predictWithSigDef[B](\"gs://model-path\", options)(toTensors)(fromTensors _)","title":"Tensorflow unused predict type parameter"},{"location":"releases/migrations/v0.14.0-Migration-Guide.html","text":"","title":"Scio v0.14.0"},{"location":"releases/migrations/v0.14.0-Migration-Guide.html#scio-v0-14-0","text":"","title":"Scio v0.14.0"},{"location":"releases/migrations/v0.14.0-Migration-Guide.html#coders","text":"Some coders have moved away from the default implicit scope. By updating to scio 0.14, you may encounter the following error:\nCannot find an implicit Coder instance for type:\n...\nIf the type is or contains an Avro class (either GenericRecord or a SpecificRecord implementation), you can import the com.spotify.scio.avro._ package to get the implicit avro coders back in scope. This is likely to happen if you are using any readAsAvro.. API or AvroSortedBucketIO from scio-smb. See avro removed from core for more details.\nIf the type relied on a fallback coder, we advise you to create a custom coder. See coders for more details. If you want to use kryo implicit fallback coders as before, this requires now to import com.spotify.scio.coders.kryo._ explicitly.","title":"Coders"},{"location":"releases/migrations/v0.14.0-Migration-Guide.html#avro-removed-from-core","text":"Avro coders are now a part of the com.spotify.scio.avro package.\nimport com.spotify.scio.avro._\nUpdate direct usage:\n- Coder.avroGenericRecordCoder(schema)\n+ avroGenericRecordCoder(schema)\n- Coder.avroGenericRecordCoder\n+ avroGenericRecordCoder\n- Coder.avroSpecificRecordCoder[T]\n+ avroSpecificRecordCoder[T]\n- Coder.avroSpecificFixedCoder[U]\n+ avroSpecificFixedCoder[U]\nDynamic avro and protobuf writes are now in com.spotify.scio.avro.dynamic. If using saveAsDynamicAvroFile or saveAsDynamicProtobufFile, add the following:\nimport com.spotify.scio.avro.dynamic._\nAvro schemas are now in com.spotify.scio.avro.schemas package:\nimport com.spotify.scio.avro.schemas._","title":"Avro removed from core"},{"location":"releases/migrations/v0.14.0-Migration-Guide.html#materialize-no-longer-splittable","text":"Materialize was previously implemented using an Avro wrapper around byte arrays. To keep materialize in scio-core it has been reimplemented with saveAsBinaryFile, which writes a sequence of records with no sub-file blocks, and thus does not support trivially splitting the file on read. We have found little use of materialize for large datasets that are not also saved permanently, so we expect the impact of this change to be minimal.","title":"Materialize no longer splittable"},{"location":"releases/migrations/v0.14.0-Migration-Guide.html#new-binaryfile-read","text":"See the relevant binaryFile scaladoc and example BinaryInOut.","title":"New binaryFile read"},{"location":"releases/migrations/v0.14.0-Migration-Guide.html#parquet-tensorflow-metadata","text":"When using tensorflow with scio-parquet, you must now depend on scio-tensorflow as well.\nThe parquet-tensorflow API has been migrated from custom parquet-extra to the official metadata API.\nschema and projection are now of type org.tensorflow.metadata.v0.Schema.","title":"parquet-tensorflow metadata"},{"location":"releases/migrations/v0.14.0-Migration-Guide.html#scio-smb-provided-implementations","text":"When using scio-smb, you also need to depend on the scio module that provides the file format implementation you want to use. See Sort-Merge-Bucket for more details.","title":"scio-smb provided implementations"},{"location":"releases/breaking-changes.html","text":"","title":"Breaking Changelog"},{"location":"releases/breaking-changes.html#breaking-changelog","text":"","title":"Breaking Changelog"},{"location":"releases/breaking-changes.html#breaking-changes-since-0-13-0","text":"Removed scio-elasticsearch6 Migrated scio-elasticsearch7 to new java client Changed skewedJoin API (scalafix rule provided) New File based ScioIO parameters (notably suffix in the read params) Removal of unused type parameter on tensorflow predict and predictWithSigDef","title":"Breaking changes since 0.13.0"},{"location":"releases/breaking-changes.html#breaking-changes-since-0-12-0-","text":"Removed com.spotify.scio.extra.bigquery Removed com.spotify.scio.pubsub specializations Changed type signatures of SMB methods to accommodate secondary-keyed SMB Removed beam-sql support","title":"Breaking changes since 0.12.0 (v0.12.0 Migration Guide)"},{"location":"releases/breaking-changes.html#important-changes-in-0-11-3","text":"Fixed a severe Parquet IO issue introduced in 0.11.2. Incompatible versions of com.google.http-client:google-http-client:1.40.0 and com.google.cloud.bigdataoss:gcsio:2.2.2 were leading to jobs reading Parquet getting stuck. The mitigation for 0.11.2 is to pin google-http-client to 1.39.2 in your build.sbt: scala dependencyOverrides ++= Seq( \"com.google.http-client\" % \"google-http-client\" % \"1.39.2\" )","title":"Important changes in 0.11.3"},{"location":"releases/breaking-changes.html#breaking-changes-since-scio-0-10-0-","text":"Move GCP modules to scio-google-cloud-platform Simplify coder implicits","title":"Breaking changes since Scio 0.10.0 (v0.10.0 Migration Guide)"},{"location":"releases/breaking-changes.html#breaking-changes-since-scio-0-9-0-","text":"Drop Scala 2.11, add Scala 2.13 support Remove deprecated modules scio-cassandra2 and scio-elasticsearch2 Remove deprecated methods since 0.8.0 Switch from Algebird Hash128[K] to Guava Funnel[K] for Bloom filter and sparse transforms","title":"Breaking changes since Scio 0.9.0 (v0.9.0 Migration Guide)"},{"location":"releases/breaking-changes.html#breaking-changes-since-scio-0-8-0-","text":"ScioIOs no longer return Future ScioContext#close returns ScioExecutionContext instead of ScioResult Async DoFn refactor Deprecate scio-cassandra2 and scio-elasticsearch2 ContextAndArgs#typed no longer accepts list-case #2221","title":"Breaking changes since Scio 0.8.0 (v0.8.0 Migration Guide)"},{"location":"releases/breaking-changes.html#breaking-changes-since-scio-0-7-0-","text":"New Magnolia based Coders derivation New ScioIO replaces TestIO[T] to simplify IO implementation and stubbing in JobTest","title":"Breaking changes since Scio 0.7.0 (v0.7.0 Migration Guide)"},{"location":"releases/breaking-changes.html#breaking-changes-since-scio-0-6-0","text":"scio-cassandra2 now requires Cassandra 2.2 instead of 2.0","title":"Breaking changes since Scio 0.6.0"},{"location":"releases/breaking-changes.html#breaking-changes-since-scio-0-5-0","text":"BigQueryIO in JobTest now requires a type parameter which could be either TableRow for JSON or T for type-safe API where T is a type annotated with @BigQueryType. Explicit .map(T.toTableRow) of test data is no longer needed. See changes in BigQueryTornadoesTest and TypedBigQueryTornadoesTest for more. Typed AvroIO now accepts case classes instead of Avro records in JobTest. Explicit .map(T.toGenericRecord) of test data is no longer needed. See this change for more. Package com.spotify.scio.extra.transforms is moved from scio-extra to scio-core, under com.spotify.scio.transforms.","title":"Breaking changes since Scio 0.5.0"},{"location":"releases/breaking-changes.html#breaking-changes-since-scio-0-4-0","text":"Accumulators are replaced by the new metrics API, see MetricsExample for more com.spotify.scio.hdfs package and related APIs (ScioContext#hdfs*, SCollection#saveAsHdfs*) are removed, regular file IO API should now support both GCS and HDFS (if scio-hdfs is included as a dependency). Starting Scio 0.4.4, Beam runner is completely decoupled from scio-core. See Runners page for more details.","title":"Breaking changes since Scio 0.4.0"},{"location":"releases/breaking-changes.html#breaking-changes-since-scio-0-3-0","text":"See this page for a list of breaking changes from Dataflow Java SDK to Beam Scala 2.10 is dropped, 2.11 and 2.12 are the supported Scala binary versions Java 7 is dropped and Java 8+ is required DataflowPipelineRunner is renamed to DataflowRunner DirectPipelineRunner is renamed to DirectRunner BlockingDataflowPipelineRunner is removed and ScioContext#close() will not block execution; use sc.run().waitUntilDone() to retain the blocking behavior, i.e. if you launch job from an orchestration engine like Airflow or Luigi You should set tempLocation instead of stagingLocation regardless of runner; set it to a local path for DirectRunner or a GCS path for DataflowRunner; if not set, DataflowRunner will create a default bucket for the project Type safe BigQuery is now stable API; use import com.spotify.scio.bigquery._ instead of import com.spotify.scio.experimental._ scio-bigtable no longer depends on HBase and uses Protobuf based Bigtable API; check out the updated example Custom IO, i.e. ScioContext#customInput and SCollection#saveAsCustomOutput require a name: String parameter","title":"Breaking changes since Scio 0.3.0"},{"location":"releases/v0.12.0.html","text":"","title":"v0.12.0 Release Blog"},{"location":"releases/v0.12.0.html#v0-12-0-release-blog","text":"Scio 0.12.0 contains many new features, performance improvements, and a few breaking changes. You can find the technical Migration Guide here with code samples and Scalafix instructions, and the full release notes are here.","title":"v0.12.0 Release Blog"},{"location":"releases/v0.12.0.html#new-features","text":"","title":"New Features"},{"location":"releases/v0.12.0.html#windowing-api-for-file-writes","text":"We’ve improved Scio API support for windowed file writes. Most file IOs now have three new, optional parameters: shardNameTemplate, filenamePolicySupplier, and tempDirectory. Windowed writes can be applied with either a shardNameTemplate or a filenamePolicySupplier. shardNameTemplate allows users to specify a shorthand for their desired output file format, for example:\n// SSS and NNN refer to current shard ID and total number of shards, respectively \ndata.saveAsAvroFile(\"/some-path\", numShards = 2, shardNameTemplate = \"-SSS-of-NNN\")\nFiles with this shard template would be written to disk like:\n% ls /some-path\npart-000-of-002.avro\npart-001-of-002.avro\nIn contrast, filenamePolicySupplier gives you more fine-grained control by allowing you to specify a full FilenamePolicy based on the supplied path and suffix:\ndata.saveAsAvroFile(path, schema, filenamePolicySupplier =\n  (path: String, suffix: String) => new FilenamePolicy {\n    override def windowedFilename(shardNumber: Int, numShards: Int, window: BoundedWindow, paneInfo: PaneInfo, outputFileHints: FileBasedSink.OutputFileHints): ResourceId = ???\n    override def unwindowedFilename(shardNumber: Int, numShards: Int, outputFileHints: FileBasedSink.OutputFileHints): ResourceId = ???\n  }))","title":"Windowing API for file writes"},{"location":"releases/v0.12.0.html#kv-batch-api","text":"Keyed SCollections have new built-in batching APIs: batchByKey, batchByteSizedByKey, and batchWeightedByKey, which allow you to create Iterable-backed batches within your SCollection, determined by a target batch size, byte size, or weighting scheme, respectively.\ndata\n  .keyBy(_.1)\n  .batchByKey(batchSize = 100L)","title":"KV Batch API"},{"location":"releases/v0.12.0.html#splittabledofn-based-parquet-reads","text":"We’ve introduced an opt-in new Parquet reads implementation, which implements Beam’s SplittableDoFn API, as an alternative to the default BoundedSource-backed implementation. We’ve observed greatly improved Dataflow metrics with this new implementation (total VCPU time, total memory time), when run with Dataflow Runner V1 (the default), workers may not scale up as much, resulting in an overall slower wall time. Therefore, we recommend trying out the new Parquet read in conjunction with Dataflow Runner V2.\nYou can opt in by adding an entry to your Parquet read’s Configuration:\nimport com.spotify.scio.parquet._\n\nsc.typedParquetFile[T](path, conf = ParquetConfiguration.of(\"scio.parquet.read.useSplittableDoFn\" -> true))\nYou can find more information, and other migration options, here.","title":"SplittableDoFn-based Parquet reads"},{"location":"releases/v0.12.0.html#grpc-lookup-api","text":"Scio 0.12.0 includes a new artifact, scio-grpc, that provides a custom AsyncLookupDoFn implementation specifically for GRPC service lookups. Both unary and server-streaming lookups are supported.\nimport com.spotify.scio.grpc._\n\ndata\n  .map { case (str1, str2) => ConcatRequest.newBuilder.setStr1(str1).setStr2(str2).build }\n  .grpcLookup[ConcatResponse, ConcatServiceFutureStub](\n      () => NettyChannelBuilder.forTarget(ServiceUri).usePlaintext().build(),\n      ConcatServiceGrpc.newFutureStub,\n      10 // Max pending requests\n    )(_.concat)","title":"GRPC Lookup API"},{"location":"releases/v0.12.0.html#scio-neo4j-module","text":"We have a second new artifact in 0.12.0, scio-neo4j, that supports Neo4J reads and writes.\nimport com.spotify.scio.neo4j._\n\ncase class Entity(id: String, property: Option[String])\n\nval entities = sc\n  .neo4jCypher[Entity](\n    Neo4jOptions(Neo4jConnectionOptions(\"neo4j://neo4j.com:7687\", \"username\", \"password\")),\n    \"\"\"MATCH (e:Entity)\n      |WHERE e.property = 'value'\n      |RETURN e\"\"\".stripMargin\n)\n\nentities\n  .map(someFn)\n  .saveAsNeo4j(\n    Neo4jOptions(Neo4jConnectionOptions(\"neo4j://neo4j.com:7687\", \"username\", \"password\")),\n    \"\"\"UNWIND $rows AS row\n      |MERGE (e:Entity {id: row.id})\n      |ON CREATE SET p.id = row.id, p.property = row.property\n      |\"\"\".stripMargin\n  )","title":"scio-neo4j module"},{"location":"releases/v0.12.0.html#secondary-sort-key-for-smb-writes-and-reads","text":"SMB now supports secondary sort keys. We’ve extended most SMB APIs in the style of (k: K) to also support (k1: K1, k2: K2). For example, reads and transforms can now specify a secondary key:\nsc\n  .sortMergeJoin(\n    classOf[String],\n    classOf[Int], // Secondary sort key\n    AvroSortedBucketIO.read(new TupleTag[A](\"a\"), aSchema).from(aPath),\n    AvroSortedBucketIO.read(new TupleTag[B](\"b\"), bClass).from(bPath)\n  ).map { case ((k1, k2), (aData, bData)) =>\n    // ...\n  }\nWrites, too, have an expanded API:\ndata\n  .saveAsSortedBucket(\n    AvroSortedBucketIO.write[String, Integer, Account](\n      classOf[String], // Key class primary\n      \"name\", // Key field primary\n      classOf[Integer], // Key class secondary\n      \"age\", // Key field secondary\n      classOf[Account])\n  )","title":"Secondary sort key for SMB writes and reads"},{"location":"releases/v0.12.0.html#magnolify-upgrade","text":"Scio 0.12.0 uses Magnolify 0.6.2, which contains a few new features: neo4j support, AvroType performance improvements, and the capability to annotate Parquet case classes when used in AvroCompat mode:\nimport magnolify.parquet.ParquetArray.AvroCompat._\nimport magnolify.shared.doc\n\n@doc(\"Record-level doc\")\ncase class SomeCaseClass(@doc(\"field-level doc 1\") s: String, @doc(\"field-level doc 2\") i: Int)\n\ndata\n  .map { case (s, i) => SomeCaseClass(s, i) }\n  .saveAsTypedParquetFile(\"some-path\")\nThe annotations will appear in the converted Avro schema in the file’s metadata:\n% parquet-tools meta some-path/part-00001-of-00005.parquet \nfile:        file:/some-path/part-00001-of-00005.parquet \ncreator:     parquet-mr version 1.12.3 (build f8dced182c4c1fbdec6ccb3185537b5a01e6ed6b) \nextra:       writer.model.name = magnolify \nextra:       parquet.avro.schema = {\"type\":\"record\",\"name\":\"SomeCaseClass\",\"namespace\":\"com.spotify.data.parquet\",\"doc\":\"Record-level doc\",\"fields\":[{\"name\":\"s\",\"type\":\"string\",\"doc\":\"field-level doc 1\"},{\"name\":\"i\",\"type\":\"int\",\"doc\":\"field-level doc 2\"}]}","title":"Magnolify upgrade"},{"location":"releases/v0.12.0.html#bug-fixes-performance-improvements","text":"Pipelines are unblocked from running on Dataflow RunnerV2 by fixing incorrect and deprecated API usages Coders now have a smaller memory footprint; you can expect savings in your total job graph size\nYou can see a full list on the release notes page.","title":"Bug fixes/Performance improvements"},{"location":"releases/v0.12.0.html#breaking-changes","text":"Scio 0.12.0 has a few breaking changes. The most impactful changes include:\nPubsub read API changes scio-extra bigquery removal Parquet’s saveAsDynamicParquetAvroFile removed in favor of saveAsParquetAvroFile\nA full list of breaking changes can be found on our Migration Guide.","title":"Breaking changes"},{"location":"FAQ.html","text":"","title":"FAQ"},{"location":"FAQ.html#faq","text":"General questions What’s the status of Scio? Who’s using Scio? What’s the relationship between Scio and Apache Beam? What’s the relationship between Scio and Google Cloud Dataflow? How does Scio compare to Scalding or Spark? What are GCE availability zone and GCS bucket location? Programming questions How do I setup a new SBT project? How do I deploy Scio jobs to Dataflow? How do I use the SNAPSHOT builds of Scio? How do I unit test pipelines? How do I combine multiple input sources? How do I log in a job? How do I use Beam’s Java API in Scio? What are the different types of joins and performance implication? How to create Dataflow job template? How do I cancel a job after certain time period? Why can’t I have an SCollection inside another SCollection? BigQuery questions What is BigQuery dataset location? How stable is the type safe BigQuery API? How do I work with nested Options in type safe BigQuery? How do I unit test BigQuery queries? How do I stream to a partitioned BigQuery table? How do I invalidate cached BigQuery results or disable cache? How does BigQuery determine job priority? Streaming questions How do I update a streaming job? Other IO components How do I access various files outside of a ScioContext? How do I reduce Datastore boilerplate? How do I throttle Bigtable writes? How do I use custom Kryo serializers? What Kryo tuning options are there? Development environment issues How do I keep SBT from running out of memory? How do I fix SBT heap size error in IntelliJ? How do I fix “Unable to create parent directories” error in IntelliJ? How to make IntelliJ IDEA work with type safe BigQuery classes? Common issues What does “Cannot prove that T1 <:< T2” mean? How do I fix invalid default BigQuery credentials? Why are my typed BigQuery case classes not up to date? How do I fix “SocketTimeoutException” with BigQuery? Why do I see names like “main@{NativeMethodAccessorImpl...}” in the UI? How do I fix “RESOURCE_EXHAUSTED” error? Can I use “scala.App” trait instead of “main” method? How to inspect the content of an SCollection? How do I improve side input performance? How do I control concurrency (number of DoFn threads) in Dataflow workers How to manually investigate a Cloud Dataflow worker","title":"FAQ"},{"location":"FAQ.html#general-questions","text":"","title":"General questions"},{"location":"FAQ.html#whats-the-status-of-scio-","text":"Scio is widely being used for production data pipelines at Spotify and is our preferred framework for building new pipelines on Google Cloud. We run Scio on Google Cloud Dataflow service in both batch and streaming modes. It is still under development and there may be minor breaking API changes.","title":"What’s the status of Scio?"},{"location":"FAQ.html#whos-using-scio-","text":"Spotify uses Scio for all new data pipelines running on Google Cloud Platform, including music recommendation, monetization, artist insights and business analysis. We also use BigQuery, Bigtable and Datastore heavily with Scio. We use Scio in both batch and streaming mode.\nAs of mid 2017, there are 200+ developers and 700+ production pipelines. The largest batch job we’ve seen uses 800 n1-highmem-32 workers (25600 CPUs, 166.4TB RAM) and processes 325 billion rows from Bigtable (240TB). We also have numerous jobs that process 10TB+ of BigQuery data daily. On the streaming front, we have many jobs with 30+ n1-standard-16 workers (480 CPUs, 1.8TB RAM) and SSD disks for real time machine learning or reporting.","title":"Who’s using Scio?"},{"location":"FAQ.html#whats-the-relationship-between-scio-and-apache-beam-","text":"Scio is a Scala API built on top of Apache Beam’s Java SDK. Scio offers a concise, idiomatic Scala API for a subset of Beam’s features, plus extras we find useful, like REPL, type safe BigQuery, and IO taps.","title":"What’s the relationship between Scio and Apache Beam?"},{"location":"FAQ.html#whats-the-relationship-between-scio-and-google-cloud-dataflow-","text":"Scio (version before 0.3.0) was originally built on top of Google Cloud Dataflow’s Java SDK. Google donated the code base to Apache and renamed it Beam. Cloud Dataflow became one of the supported runners, alongside Apache Flink & Apache Spark. Scio 0.3.x is built on top of Beam 0.6.0 and 0.4.x is built on top of Beam 2.x. Many users run Scio on the Dataflow runner today.","title":"What’s the relationship between Scio and Google Cloud Dataflow?"},{"location":"FAQ.html#how-does-scio-compare-to-scalding-or-spark-","text":"Check out the wiki page on Scio, Scalding and Spark. Also check out Big Data Rosetta Code for some snippets.","title":"How does Scio compare to Scalding or Spark?"},{"location":"FAQ.html#what-are-gce-availability-zone-and-gcs-bucket-location-","text":"GCE availability zone is where the Google Cloud Dataflow service spins up VM instances for your job, e.g. us-east1-a. Each GCS bucket (gs://bucket) has a storage class and bucket location that affects availability, latency and price. The location should be close to GCE availability zone. Dataflow uses --stagingLocation for job jars, temporary files and BigQuery I/O.","title":"What are GCE availability zone and GCS bucket location?"},{"location":"FAQ.html#programming-questions","text":"","title":"Programming questions"},{"location":"FAQ.html#how-do-i-setup-a-new-sbt-project-","text":"Read the documentation.","title":"How do I setup a new SBT project?"},{"location":"FAQ.html#how-do-i-deploy-scio-jobs-to-dataflow-","text":"When developing locally, you can do sbt \"runMain MyClass ... or just runMain MyClass ... in the SBT console without building any artifacts.\nWhen deploying to the cloud, we recommend using sbt-pack or sbt-native-packager plugin instead of sbt-assembly. Unlike assembly, they pack dependency jars in a directory instead of merging them, so that we don’t have to deal with merge strategy and dependency jars can be cached by Dataflow service.\nAt Spotify we pack jars with sbt-pack, build docker images with sbt-docker together with orchestration components e.g. Luigi or Airflow and deploy them with Styx.","title":"How do I deploy Scio jobs to Dataflow?"},{"location":"FAQ.html#how-do-i-use-the-snapshot-builds-of-scio-","text":"Commits to Scio master are automatically published to Sonatype via continuous integration. To use the latest SNAPSHOT artifact, add the following line to your build.sbt.\nresolvers += Resolver.sonatypeRepo(\"snapshots\")\nOr you can configure SBT globally by adding the following to ~/.sbt/1.0/global.sbt.\nresolvers ++= Seq(\n  Resolver.sonatypeRepo(\"snapshots\")\n  // other resolvers\n)","title":"How do I use the SNAPSHOT builds of Scio?"},{"location":"FAQ.html#how-do-i-unit-test-pipelines-","text":"Any Scala or Java unit testing frameworks can be used with Scio, but we provide some utilities for ScalaTest.\nPipelineTestUtils - utilities for testing parts of a pipeline JobTest - for testing pipelines end-to-end with complete arguments and IO coverage SCollectionMatchers - ScalaTest matchers for SCollection PipelineSpec - shortcut for ScalaTest FlatSpec with utilities and matchers\nThe best place to find example usage of JobTest and SCollectionMatchers are their respective tests in JobTestTest and SCollectionMatchersTest. For more examples see:\nscio-examples https://github.com/spotify/big-data-rosetta-code/tree/master/src/test/scala/com/spotify/bdrc/testing","title":"How do I unit test pipelines?"},{"location":"FAQ.html#how-do-i-combine-multiple-input-sources-","text":"How do I combine multiple input sources, e.g. different BigQuery tables, files located in different GCS buckets? You can combine SCollections from different sources into one using the companion method SCollection.unionAll, for example:\nimport com.spotify.scio._\nimport com.spotify.scio.avro._\nimport com.spotify.scio.values._\nimport com.spotify.scio.avro.TestRecord\n\nobject MyJob {\n  def main(cmdlineArgs: Array[String]): Unit = {\n    val (sc, args) = ContextAndArgs(cmdlineArgs)\n\n    val collections = Seq(\n      \"gs://bucket1/data/\",\n      \"gs://bucket2/data/\"\n    ).map(path => sc.avroFile[TestRecord](path, suffix=\".avro\"))\n\n    val all = SCollection.unionAll(collections)\n  }\n}","title":"How do I combine multiple input sources?"},{"location":"FAQ.html#how-do-i-log-in-a-job-","text":"You can log in a Scio job with most common logging libraries but slf4j is included as a dependency. Define the logger instance as a member of the job object and use it inside a lambda.\nimport com.spotify.scio._\nimport org.slf4j.LoggerFactory\n\nobject MyJob {\n  private val logger = LoggerFactory.getLogger(this.getClass)\n  def main(cmdlineArgs: Array[String]): Unit = {\n    val (sc, args) = ContextAndArgs(cmdlineArgs)\n\n    sc.parallelize(1 to 100)\n      .map { i =>\n        logger.info(s\"Element $i\")\n        i * i\n      }\n    // ...\n  }\n}","title":"How do I log in a job?"},{"location":"FAQ.html#how-do-i-use-beams-java-api-in-scio-","text":"Scio exposes a few things to allow easy integration with native Beam Java API, notably:\nScioContext#customInput to apply a PTransform[_ >: PBegin, PCollection[T]] (source) and get an SCollection[T]. SCollection#applyTransform to apply a PTransform[_ >: PCollection[T], PCollection[U]] and get an SCollection[U] SCollection#saveAsCustomOutput to apply a PTransform[_ >: PCollection[T], PDone] (sink) and get a ClosedTap[T].\nSee BeamExample for more details. Custom I/O can also be tested via the JobTest harness.","title":"How do I use Beam’s Java API in Scio?"},{"location":"FAQ.html#what-are-the-different-types-of-joins-and-performance-implication-","text":"Inner (a.join(b)), left (a.leftOuterJoin(b)), outer (a.fullOuterJoin(b)) performs better with a large LHS. So a should be the larger data set with potentially more hot keys, i.e. key with many values. Every key-value pair from every input is shuffled. join/leftOuterJoin may be replaced by hashJoin/leftHashJoin if the RHS is small enough to fit in memory (e.g. < 1GB). The RHS is used as a multi-map side input for the LHS. No shuffle is performed. Consider skewedJoin if some keys on the LHS are extremely hot. Consider sparseOuterJoin if you want a full outer join where RHS is much smaller than LHS, but may not fit in memory. Consider cogroup if you need to access value groups of each key. MultiJoin supports inner, left, outer join and cogroup of up to 22 inputs. For multi-joins larger inputs should be on the left, e.g. size(a) >= size(b) >= size(c) >= size(d) in MultiJoin(a, b, c, d). Check out these slides for more information on joins. Also see this section on Cloud Dataflow Shuffle service.","title":"What are the different types of joins and performance implication?"},{"location":"FAQ.html#how-to-create-dataflow-job-template-","text":"For Apache Beam based Scio (version >= 0.3.0) use DataflowRunner and specify templateLocation option. For example in CLI --templateLocation=gs://<bucket>/job1. Read more about templates here.","title":"How to create Dataflow job template?"},{"location":"FAQ.html#how-do-i-cancel-a-job-after-certain-time-period-","text":"You can wait on the ScioResult and call the internal PipelineResult#cancel() method if a timeout exception happens.\nimport com.spotify.scio._\nimport scala.concurrent.duration._\n\nobject MyJob {\n  def main(cmdlineArgs: Array[String]): Unit = {\n    val (sc, args) = ContextAndArgs(cmdlineArgs)\n\n    // ...\n    val closedSc: ScioExecutionContext = sc.run()\n    val result: ScioResult = closedSc.waitUntilFinish(1.minute, cancelJob = true)\n  }\n}","title":"How do I cancel a job after certain time period?"},{"location":"FAQ.html#why-cant-i-have-an-scollection-inside-another-scollection-","text":"You cannot have an SCollection inside another SCollection, i.e. anything with type SCollection[SCollection[T]]. To explain this we have to go back to the relationship between ScioContext and SCollection. Every ScioContext represents a unique pipeline and every SCollection represents a stage in the pipeline execution, i.e. the state of the pipeline after some transforms has be applied. We start a pipeline code with val sc = ..., create new SCollections with methods on sc, e.g. sc.textFile, and transform them with methods like .map, .filter, .join. Therefore each SCollection can trace its root to one single sc. The pipeline is submitted for execution when we call sc.run(). Hence we cannot have an SCollection inside another SCollection just as we cannot have a pipeline inside another pipeline.","title":"Why can’t I have an SCollection inside another SCollection?"},{"location":"FAQ.html#bigquery-questions","text":"","title":"BigQuery questions"},{"location":"FAQ.html#what-is-bigquery-dataset-location-","text":"Each BigQuery dataset has a location (e.g. US, EU) and every table inside are stored in the same location. Tables in a JOIN must be from the same region. Also one can only import/export tables to a GCS bucket in the same location. Starting from v0.2.1, Scio will detect the dataset location of a query and create a staging dataset for ScioContext#bigQuerySelect and @BigQueryType.fromQuery. This location should be the same as that of your --stagingLocation GCS bucket. The old -Dbigquery.staging_dataset.location flag is removed.\nBecause of these limitations and performance reasons, make sure --zone, --stagingLocation and -Dbigquery.staging_dataset.location location of BigQuery datasets are consistent.","title":"What is BigQuery dataset location?"},{"location":"FAQ.html#how-stable-is-the-type-safe-bigquery-api-","text":"BigQuery API is considered stable and widely used at Spotify. There are several caveats however:\nBoth legacy and SQL syntax are supported although the SQL syntax is highly recommended The system will detect legacy or SQL syntax and choose the correct one To override auto-detection, start the query with either #legacysql or #standardsql comment line Legacy syntax is less predictable, especially for complex queries and may be disabled in the future Case classes generated by @BigQueryType.fromTable or @BigQueryType.fromQuery are not recognized in IntelliJ IDEA, but see this section for a workaround","title":"How stable is the type safe BigQuery API?"},{"location":"FAQ.html#how-do-i-work-with-nested-options-in-type-safe-bigquery-","text":"Any nullable field in BigQuery is translated to Option[T] by the type safe BigQuery API and it can be clunky to work with rows with multiple or nested fields. For example:\ndef doSomething(s: String): Unit = ()\nimport com.spotify.scio.bigquery.types.BigQueryType\n\n@BigQueryType.fromSchema(\"\"\"{\n    |\"fields\": [{\n    | \"type\":\"RECORD\",\n    | \"mode\": \"NULLABLE\",\n    | \"name\":\"user\",\n    | \"fields\":[\n    |   {\"mode\": \"NULLABLE\", \"name\":\"email\", \"type\": \"STRING\"},\n    |   {\"mode\": \"REQUIRED\",\"name\":\"name\",\"type\":\"STRING\"}]\n    |}]}\"\"\".stripMargin)\nclass Row\n\ndef doSomethingWithRow(row: Row) = {\n  if (row.user.isDefined) {  // Option[User]\n    val email = row.user.get.email  // Option[String]\n    if (email.isDefined) {\n      doSomething(email.get)\n    }\n  }\n}\nFor comprehension is a nicer alternative in these cases:\ndef doSomethingWithRowUsingFor(row: Row) = {\n  val e: Option[String] =\n    for {\n      u <- row.user\n      e <- u.email\n    } yield e\n  e.foreach(doSomething)\n}\nAlso see these slides and this blog article.","title":"How do I work with nested Options in type safe BigQuery?"},{"location":"FAQ.html#how-do-i-unit-test-bigquery-queries-","text":"BigQuery doesn’t provide a way to unit test query logic locally, but we can query the service directly in an integration test. Take a look at BigQueryIT.scala. MockBigQuery will create temporary tables on the service, feed them with mock data, and substitute table references in your query string with the mocked ones.","title":"How do I unit test BigQuery queries?"},{"location":"FAQ.html#how-do-i-stream-to-a-partitioned-bigquery-table-","text":"Currently, there is no way to create a partitioned BigQuery table via Scio/Beam when streaming, however it is possible to stream to a partitioned table if it is already created.\nThis can be done by using fixed windows and using the window bounds to infer date. As of Scio 0.4.0-beta2 this looks as follows:\nimport com.spotify.scio._\nimport com.spotify.scio.pubsub._\nimport org.apache.beam.sdk.values.ValueInSingleWindow\nimport org.apache.beam.sdk.transforms.SerializableFunction\nimport org.apache.beam.sdk.transforms.windowing.IntervalWindow\nimport com.google.api.services.bigquery.model.TableRow\nimport org.apache.beam.sdk.io.gcp.bigquery.{BigQueryIO, TableDestination}\nimport BigQueryIO.Write.{CreateDisposition, WriteDisposition}\nimport org.joda.time.format.DateTimeFormat\nimport org.joda.time.{DateTimeZone, Duration}\n\nclass DayPartitionFunction() extends SerializableFunction[ValueInSingleWindow[TableRow], TableDestination] {\n  override def apply(input: ValueInSingleWindow[TableRow]): TableDestination = {\n    val partition = DateTimeFormat.forPattern(\"yyyyMMdd\").withZone(DateTimeZone.UTC)\n      .print(input.getWindow.asInstanceOf[IntervalWindow].start())\n    new TableDestination(\"project:dataset.partitioned$\" + partition, \"\")\n  }\n}\n\nobject BQPartitionedJob {\n\n  def myStringToTableRowConversion: String => TableRow = ???\n\n  def main(cmdlineArgs: Array[String]): Unit = {\n    val (sc, args) = ContextAndArgs(cmdlineArgs)\n\n\n    sc.read(PubsubIO.string(\"projects/data-university/topics/data-university\"))(PubsubIO.ReadParam(PubsubIO.Subscription))\n      .withFixedWindows(Duration.standardSeconds(30))\n      // Convert to `TableRow`\n      .map(myStringToTableRowConversion)\n      .saveAsCustomOutput(\n        \"SaveAsDayPartitionedBigQuery\",\n        BigQueryIO.writeTableRows().to(\n          new DayPartitionFunction())\n          .withWriteDisposition(WriteDisposition.WRITE_APPEND)\n          .withCreateDisposition(CreateDisposition.CREATE_NEVER)\n      )\n\n    sc.run()\n  }\n}\nIn Scio 0.3.X it is possible to achieve the same behaviour using SerializableFunction[BoundedWindow, String] and BigQueryIO.Write.to. It is also possible to stream to separate tables with a Date suffix by modifying DayPartitionFunction, specifying the Schema, and changing the CreateDisposition to CreateDisposition.CREATE_IF_NEEDED.","title":"How do I stream to a partitioned BigQuery table?"},{"location":"FAQ.html#how-do-i-invalidate-cached-bigquery-results-or-disable-cache-","text":"Scio’s BigQuery client in Scio caches query result in system property bigquery.cache.directory, which defaults to $PWD/.bigquery. Use rm -rf .bigquery to invalidate all cached results. To disable caching, set system property bigquery.cache.enabled to false.","title":"How do I invalidate cached BigQuery results or disable cache?"},{"location":"FAQ.html#how-does-bigquery-determine-job-priority-","text":"By default, Scio runs BigQuery jobs with BATCH priority except when in the REPL where it runs with INTERACTIVE. To override this, set system property bigquery.priority to either BATCH or INTERACTIVE.","title":"How does BigQuery determine job priority?"},{"location":"FAQ.html#streaming-questions","text":"","title":"Streaming questions"},{"location":"FAQ.html#how-do-i-update-a-streaming-job-","text":"Dataflow allows streaming jobs to be updated on the fly by specifying --update, along with --jobName=[your_job] on the command line. See https://cloud.google.com/dataflow/pipelines/updating-a-pipeline for detailed docs. Note that for this to work, Dataflow needs to be able to identify which transformations from the original job map to those in the replacement job. The easiest way to do so is to give unique names to transforms in the code itself. In Scio, this can be achieved by calling .withName() before applying the transform. For example:\nimport com.spotify.scio._\n\ndef main(cmdlineArgs: Array[String]): Unit = {\n  val (sc, args) = ContextAndArgs(cmdlineArgs)\n  sc.textFile(args(\"input\"))\n    .withName(\"MakeUpper\").map(_.toUpperCase)\n    .withName(\"BigWords\").filter(_.length > 6)\n}\nIn this example, the map’s transform name is “MakeUpper” and the filter’s is “BigWords”. If we later decided that we want to count 6 letter words as “big” too, then we can change it to _.length > 5, and because the transform name is the same the job can be updated on the fly.","title":"How do I update a streaming job?"},{"location":"FAQ.html#other-io-components","text":"","title":"Other IO components"},{"location":"FAQ.html#how-do-i-access-various-files-outside-of-a-sciocontext-","text":"For Scio version >= 0.4.0\nStarting from Scio 0.4.0 you can use Apache Beam’s Filesystems abstraction:\nimport org.apache.beam.sdk.io.FileSystems\n// the path can be any of the supported Filesystems, e.g. local, GCS, HDFS\ndef readmeResource = FileSystems.matchNewResource(\"gs://<bucket>/README.md\", false)\ndef readme = FileSystems.open(readmeResource)\nFor Scio version < 0.4.0\nNote This part is GCS specific.\nYou can get a GcsUtil instance from ScioContext, which can be used to open GCS files in read or write mode.\nimport com.spotify.scio.ContextAndArgs\nimport org.apache.beam.sdk.extensions.gcp.options.GcsOptions\n\ndef main(cmdlineArgs: Array[String]): Unit = {\n  val (sc, args) = ContextAndArgs(cmdlineArgs)\n  val gcsUtil = sc.optionsAs[GcsOptions].getGcsUtil\n  // ...\n}","title":"How do I access various files outside of a ScioContext?"},{"location":"FAQ.html#how-do-i-reduce-datastore-boilerplate-","text":"Datastore Entity class is actually generated from Protobuf which uses the builder pattern and very boilerplate heavy. You can use the Magnolify library to seamlessly convert between case classes and Entitys. See MagnolifyDatastoreExample.scala for an example job and MagnolifyDatastoreExampleTest.scala for tests.","title":"How do I reduce Datastore boilerplate?"},{"location":"FAQ.html#how-do-i-throttle-bigtable-writes-","text":"Currently, Dataflow autoscaling may not work well with large writes BigtableIO. Specifically It does not take into account Bigtable IO rate limits and may scale up more workers and end up hitting the limit and eventually fail the job. As a workaround, you can enable throttling for Bigtable writes in Scio 0.4.0-alpha2 or later.\nval btProjectId = \"\"\nval btInstanceId = \"\"\nval btTableId = \"\"\nimport com.spotify.scio.values._\nimport com.spotify.scio.bigtable._\nimport com.google.cloud.bigtable.config.{BigtableOptions, BulkOptions}\nimport com.google.bigtable.v2.Mutation\nimport com.google.protobuf.ByteString\n\ndef main(cmdlineArgs: Array[String]): Unit = {\n  // ...\n\n  val data: SCollection[(ByteString, Iterable[Mutation])] = ???\n\n  val btOptions =\n    BigtableOptions.builder()\n      .setProjectId(btProjectId)\n      .setInstanceId(btInstanceId)\n      .setBulkOptions(BulkOptions.builder()\n        .enableBulkMutationThrottling()\n        .setBulkMutationRpcTargetMs(10) // lower latency threshold, default is 100\n        .build())\n      .build()\n  data.saveAsBigtable(btOptions, btTableId)\n\n  // ...\n}","title":"How do I throttle Bigtable writes?"},{"location":"FAQ.html#how-do-i-use-custom-kryo-serializers-","text":"See Kryo for more.\nDefine a registrar class that extends IKryoRegistrar and annotate it with @KryoRegistrar. Note that the class name must ends with KryoRegistrar, i.e. MyKryoRegistrar for Scio to find it.\ntrait UserRecord\ntrait AccountRecord\nimport com.twitter.chill.KSerializer\nimport com.esotericsoftware.kryo.Kryo\nimport com.esotericsoftware.kryo.io.{Input, Output}\n\nclass UserRecordSerializer extends KSerializer[UserRecord] {\n  def read(x$1: Kryo, x$2: Input, x$3: Class[UserRecord]): UserRecord = ???\n  def write(x$1: Kryo, x$2: Output, x$3: UserRecord): Unit = ???\n}\nclass AccountRecordSerializer extends KSerializer[AccountRecord] {\n  def read(x$1: Kryo, x$2: Input, x$3: Class[AccountRecord]): AccountRecord = ???\n  def write(x$1: Kryo, x$2: Output, x$3: AccountRecord): Unit = ???\n}\nimport com.twitter.chill._\nimport com.esotericsoftware.kryo.Kryo\nimport com.spotify.scio.coders.KryoRegistrar\nimport com.twitter.chill.IKryoRegistrar\n\n@KryoRegistrar\nclass MyKryoRegistrar extends IKryoRegistrar {\n  override def apply(k: Kryo): Unit = {\n    // register serializers for additional classes here\n    k.forClass(new UserRecordSerializer)\n    k.forClass(new AccountRecordSerializer)\n    //...\n  }\n}\nRegistering just the classes can also improve Kryo performance. By registering, classes will be serialized as numeric IDs instead of fully qualified class names, hence saving space and network IO while shuffling. make\ntrait MyRecord1\ntrait MyRecord2\nimport com.twitter.chill._\nimport com.esotericsoftware.kryo.Kryo\nimport com.spotify.scio.coders.KryoRegistrar\nimport com.twitter.chill.IKryoRegistrar\n\n@KryoRegistrar\nclass MyKryoRegistrar extends IKryoRegistrar {\n  override def apply(k: Kryo): Unit = {\n    k.registerClasses(List(classOf[MyRecord1], classOf[MyRecord2]))\n  }\n}","title":"How do I use custom Kryo serializers?"},{"location":"FAQ.html#what-kryo-tuning-options-are-there-","text":"See KryoOptions.java for a complete list of available Kryo tuning options. These can be passed via command line, for example:\n--kryoBufferSize=1024 --kryoMaxBufferSize=8192 --kryoReferenceTracking=false --kryoRegistrationRequired=true\nAmong these, --kryoRegistrationRequired=true might be useful when developing to ensure that all data types in the pipeline are registered.","title":"What Kryo tuning options are there?"},{"location":"FAQ.html#development-environment-issues","text":"","title":"Development environment issues"},{"location":"FAQ.html#how-do-i-keep-sbt-from-running-out-of-memory-","text":"SBT might run out of memory sometimes and show an OutOfMemoryError: Metaspace error. Override default memory setting with -mem <integer>, e.g. sbt -mem 1024.","title":"How do I keep SBT from running out of memory?"},{"location":"FAQ.html#how-do-i-fix-sbt-heap-size-error-in-intellij-","text":"If you encounter an SBT error with message “Initial heap size set to a larger value than the maximum heap size”, that is because IntelliJ has a lower default -Xmx for SBT than -Xms in our .jvmopts. To fix that, open Preferences -> Build, Execution, Deployment -> Build Tools -> sbt, and update Maximum heap size, MB to 2048.","title":"How do I fix SBT heap size error in IntelliJ?"},{"location":"FAQ.html#how-do-i-fix-error-in-intellij-","text":"You might get an error message like java.io.IOException: Unable to create parent directories of /Applications/IntelliJ IDEA CE.app/Contents/bin/.bigquery/012345abcdef.schema.json. This usually happens to people who run IntelliJ IDEA with its bundled JVM. There are two solutions.\nInstall JDK from java.com and switch to it by following the “All platforms: switch between installed runtimes” section in this page. Override the bigquery .cache directory as a JVM compiler parameter. On the bottom right of the IntelliJ window, click the icon that looks like a clock, and then “Configure…”. Then, edit the JVM parameters to include the line -Dbigquery.cache.directory=</path/to/repository>/.bigquery. Then, restart the compile server by clicking on the clock icon -> Stop, and then Start.","title":"How do I fix “Unable to create parent directories” error in IntelliJ?"},{"location":"FAQ.html#how-to-make-intellij-idea-work-with-type-safe-bigquery-classes-","text":"Due to issue SCL-8834 case classes generated by @BigQueryType.fromTable or @BigQueryType.fromQuery are not recognized in IntelliJ IDEA. There are two workarounds. The first, IDEA plugin solution, is highly recommended.\nIDEA Plugin\nInside IntelliJ, Preferences -> Plugins -> Browse repositories ... and search Scio. Install the plugin, restart IntelliJ, recompile the project (use SBT or IntelliJ). You have to recompile the project each time you add/edit @BigQueryType macro. Plugin requires Scio >= 0.2.2. Documentation.\nUse case class from @BigQueryType.toTable\nFirst start Scio REPL and generate case classes from your query or table.\nimport com.spotify.scio.bigquery.types.BigQueryType\n\n@BigQueryType.fromQuery(\"SELECT tornado, month FROM [bigquery-public-data:samples.gsod]\")\nclass Tornado\nNext print Scala code of the generated classes.\nTornado.toPrettyString()\nYou can then paste the @BigQueryType.fromQuery code into your pipeline and use it with sc.typedBigQuery.\nimport com.spotify.scio._\nimport com.spotify.scio.values._\nimport com.spotify.scio.bigquery._\n\ndef main(cmdlineArgs: Array[String]): Unit = {\n  val (sc, args) = ContextAndArgs(cmdlineArgs)\n\n  val data: SCollection[Tornado] = sc.typedBigQuery[Tornado]()\n  // ...\n}","title":"How to make IntelliJ IDEA work with type safe BigQuery classes?"},{"location":"FAQ.html#common-issues","text":"","title":"Common issues"},{"location":"FAQ.html#what-does-mean-","text":"Sometimes you get an error message like Cannot prove that T1 <:< T2 when saving an SCollection. This is because some sink methods have an implicit argument like this which means element type T of SCollection[T] must be a subtype of TableRow in order to save it to BigQuery. You have to map out elements to the required type before saving.\ndef saveAsBigQuery(tableSpec: String)(implicit ev: T <:< TableRow)\nIn the case of saveAsTypedBigQuery you might get an Cannot prove that T <:< com.spotify.scio.bigquery.types.BigQueryType.HasAnnotation. error message. This API requires an SCollection[T] where T is a case class annotated with @BigQueryType.toTable. For example:\nimport com.spotify.scio._\nimport com.spotify.scio.values._\nimport com.spotify.scio.bigquery._\nimport com.spotify.scio.bigquery.types.BigQueryType\n\n@BigQueryType.toTable\ncase class Result(user: String, score: Int)\n\ndef main(cmdlineArgs: Array[String]): Unit = {\n  val (sc, args) = ContextAndArgs(cmdlineArgs)\n  val p: SCollection[(String, Int)] = ???\n\n  p.map(kv => Result(kv._1, kv._2))\n   .saveAsTypedBigQueryTable(Table.Spec(args(\"output\")))\n}\nNote Scio uses Macro Annotations and Macro Paradise plugin to implement annotations. You need to add Macro Paradise plugin to your scala compiler as described here.","title":"What does “Cannot prove that T1 <:< T2” mean?"},{"location":"FAQ.html#how-do-i-fix-invalid-default-bigquery-credentials-","text":"If you don’t specify a secret credential file for BigQuery [1], Scio will use your default credentials (via GoogleCredential.getApplicationDefault), which:\nReturns the Application Default Credentials which are used to identify and authorize the whole application. The following are searched (in order) to find the Application Default Credentials: - Credentials file pointed to by the GOOGLE_APPLICATION_CREDENTIALS environment variable - Credentials provided by the Google Cloud SDK - gcloud auth application-default login command - Google App Engine built-in credentials - Google Cloud Shell built-in credentials - Google Compute Engine built-in credentials\nThe easiest way to configure it on your local machine is to use the gcloud auth application-default login command.\n[1] Keep in mind that you can specify your credential file via -Dbigquery.secret.","title":"How do I fix invalid default BigQuery credentials?"},{"location":"FAQ.html#why-are-my-typed-bigquery-case-classes-not-up-to-date-","text":"Case classes generated by @BigQueryType.fromTable or other macros might not update after table schema change. To solve this problem, remove the cached BigQuery metadata by deleting the .bigquery directory in your project root. If you would rather avoid any issues resulting from caching and schema evolution entirely, you can disable caching by setting the system property bigquery.cache.enabled to false.","title":"Why are my typed BigQuery case classes not up to date?"},{"location":"FAQ.html#how-do-i-fix-with-bigquery-","text":"BigQuery requests may sometimes timeout, i.e. for complex queries over many tables.\nexception during macro expansion:\n[error] java.net.SocketTimeoutException: Read timed out\nIt can be fixed by increasing the timeout settings (default 20s).\nsbt -Dbigquery.connect_timeout=30000 -Dbigquery.read_timeout=30000","title":"How do I fix “SocketTimeoutException” with BigQuery?"},{"location":"FAQ.html#why-do-i-see-names-like-in-the-ui-","text":"Scio traverses JVM stack trace to figure out the proper name of each transform, i.e. flatMap@{UserAnalysis.scala:30} but may get confused if your jobs are under the com.spotify.scio package. Move them to a different package, e.g. com.spotify.analytics to fix the issue.","title":"Why do I see names like “main@{NativeMethodAccessorImpl...}” in the UI?"},{"location":"FAQ.html#how-do-i-fix-error-","text":"You might see errors like RESOURCE_EXHAUSTED: IO error: No space left on disk in a job. They usually indicate that you have allocated insufficient local disk space to process your job. If you are running your job with default settings, your job is running on 3 workers, each with 250 GB of local disk space. Consider modifying the default settings to increase the number of workers available to your job (via --numWorkers), to increase the default disk size per worker (via --diskSizeGb).","title":"How do I fix “RESOURCE_EXHAUSTED” error?"},{"location":"FAQ.html#can-i-use-trait-instead-of-method-","text":"Your Scio applications should define a main method instead of extending scala.App. Applications extending scala.App due to delayed initialization and closure cleaning may not work properly.","title":"Can I use “scala.App” trait instead of “main” method?"},{"location":"FAQ.html#how-to-inspect-the-content-of-an-scollection-","text":"There are multiple options here:\nUse debug() method on an SCollection to print its content as the data flows through the DAG during the execution (after the run or runAndCollect) Use a debugger and setup break points - make sure to break inside of your functions to stop control at the execution not the pipeline construction time In Scio-REPL, use runAndCollect() to execute the pipeline and materialize the contents of an SCollection","title":"How to inspect the content of an SCollection?"},{"location":"FAQ.html#how-do-i-improve-side-input-performance-","text":"By default, Dataflow workers allocate 100MB (see DataflowWorkerHarnessOptions#getWorkerCacheMb) of memory for caching side inputs, and falls back to disk or network. Therefore jobs with large side inputs may be slow. To override this default, register DataflowWorkerHarnessOptions before parsing command line arguments and then pass --workerCacheMb=N when submitting the job.\nimport com.spotify.scio._\nimport org.apache.beam.sdk.options.PipelineOptionsFactory\nimport org.apache.beam.runners.dataflow.options.DataflowWorkerHarnessOptions\n\ndef main(cmdlineArgs: Array[String]): Unit = {\n  PipelineOptionsFactory.register(classOf[DataflowWorkerHarnessOptions])\n  val (sc, args) = ContextAndArgs(cmdlineArgs)\n  // ...\n}","title":"How do I improve side input performance?"},{"location":"FAQ.html#how-do-i-control-concurrency-number-of-dofn-threads-in-dataflow-workers","text":"By default, Google Cloud Dataflow will use as many threads (concurrent DoFns) per worker as appropriate (precise definition is an implementation detail), in some cases you might want to control this. Use NumberOfWorkerHarnessThreads option from DataflowPipelineDebugOptions. For example to use a single thread per worker on 8 vCPU machine, simply specify 8 vCPU worker machine type, and --numberOfWorkerHarnessThreads=1 in CLI or set corresponding option in DataflowPipelineDebugOptions.","title":"How do I control concurrency (number of DoFn threads) in Dataflow workers"},{"location":"FAQ.html#how-to-manually-investigate-a-cloud-dataflow-worker","text":"First find the VM of the worker, the easiest place is through the GCE instance groups:\ngcloud compute ssh --project=<project> --zone=<zone> <VM>\nTo find the id of batch (for batch job) container:\ndocker ps | grep \"batch\\|streaming\" | awk '{print $1}'\nTo get into the harness container:\ndocker exec -it <container-id> /bin/bash\nTo install java jdk tools:\napt-get update\napt-get install default-jdk -y\nTo find java process:\njps\nTo get GC stats:\njstat -gcutil <pid> 1000 1000\nTo get stacktrace:\njstack <pid>","title":"How to manually investigate a Cloud Dataflow worker"}]}