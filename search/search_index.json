{"docs":[{"location":"/paradox.json","text":"","title":""},{"location":"/index.html","text":"Ecclesiastical Latin IPA: /ˈʃi.o/, [ˈʃiː.o], [ˈʃi.i̯o] Verb: I can, know, understand, have knowledge.","title":"Scio"},{"location":"/index.html#scio","text":"Scio is a Scala API for Apache Beam and Google Cloud Dataflow inspired by Apache Spark and Scalding.\nGetting Started is the best place to start with Scio. If you are new to Apache Beam and distributed data processing, check out the Beam Programming Guide first for a detailed explanation of the Beam programming model and concepts. If you have experience with other Scala data processing libraries, check out this comparison between Scio, Scalding and Spark. Finally check out this document about the relationship between Scio, Beam and Dataflow.\nExample Scio pipelines and tests can be found under scio-examples. A lot of them are direct ports from Beam’s Java examples. See this page for some of them with side-by-side explanation. Also see Big Data Rosetta Code for common data processing code snippets in Scio, Scalding and Spark.\nSee Scio Scaladocs for current API documentation.","title":"Scio"},{"location":"/index.html#getting-help","text":"","title":"Getting help"},{"location":"/index.html#documentation","text":"Getting Started - current API documentation Scio REPL - tutorial for the interactive Scio REPL Scio, Beam and Dataflow - how Scio concepts map to Beam and Dataflow Scio, Scalding and Spark - comparison of these frameworks Runners - how Scio handles Beam runners and runner specific logic Scio data guideline - guideline for common problems Apache Beam - notes on Apache Beam compatibility Changelog - breaking changes in each release FAQ - frequently asked questions Powered By - see who is using Scio in production","title":"Documentation"},{"location":"/index.html#io","text":"Type safe BigQuery - tutorial for the type safe BigQuery API Bigtable - using Scio with Bigtable Avro - using Scio with Avro files Protobuf - using Scio with Protobuf Parquet - using Scio with Parquet files","title":"IO"},{"location":"/index.html#extras","text":"Algebird Sort Merge Bucket","title":"Extras"},{"location":"/index.html#internals","text":"ScioIO - new IO system to simplify implementation and stubbing in JobTest OverrideTypeProvider - custom mappings for type-safe BigQuery Kryo - Kryo data serialization Coders - new Magnolia based Coders derivation","title":"Internals"},{"location":"/index.html#further-readings","text":"Big Data Processing at Spotify: The Road to Scio (Part 1) Big Data Processing at Spotify: The Road to Scio (Part 2) The world beyond batch: Streaming 101 The world beyond batch: Streaming 102 Dataflow/Beam & Spark: A Programming Model Comparison VLDB paper on the Dataflow Model","title":"Further Readings"},{"location":"/index.html#presentations","text":"Scio - Big Data on Google Cloud with Scala and Scio - Apache Beam Summit London 2018 Talk Sorry - How Bieber broke Google Cloud at Spotify (slides) - Scala Up North 2017 Talk Scio - Moving to Google Cloud A Spotify Story (slides) - Philly ETE 2017 Talk Scio - A Scala API for Google Cloud Dataflow & Apache Beam (slides) - Scala by the Bay 2016 Talk From stream to recommendation with Cloud Pub/Sub and Cloud Dataflow - GCP NEXT 16 Talk Apache Beam Presentation Materials","title":"Presentations"},{"location":"/index.html#projects-using-or-related-to-scio","text":"Featran - A Scala feature transformation library for data science and machine learning Big Data Rosetta Code - Code snippets for solving common big data problems in various platforms. Inspired by Rosetta Code Ratatool - A tool for random data sampling and generation, which includes BigDiffy, a Scio library for pairwise field-level statistical diff of data sets (slides) scio-deep-dive - Building Scio from scratch step by step for an internal training session scala-flow - A lightweight Scala wrapper for Google Cloud Dataflow from Zendesk clj-headlights - Clojure API for Apache Beam, also from Zendesk datasplash - A Clojure API for Google Cloud Dataflow","title":"Projects using or related to Scio"},{"location":"/Getting-Started.html","text":"","title":"Getting Started"},{"location":"/Getting-Started.html#getting-started","text":"First install the Google Cloud SDK and create a Google Cloud Storage bucket for your project, e.g. gs://my-bucket. Make sure it’s in the same region as the BigQuery datasets you want to access and where you want Dataflow to launch workers on GCE.\nScio may need Google Cloud’s application default credentials for features like BigQuery. Run the following command to set it up.\ngcloud auth application-default login","title":"Getting Started"},{"location":"/Getting-Started.html#building-scio","text":"Scio is built using SBT. To build Scio and publish artifacts locally, run:\ngit clone git@github.com:spotify/scio.git\ncd scio\n# 'sbt +publishLocal' to cross build for all Scala versions\n# 'sbt ++$SCALA_VERSION publishLocal' to build for a specific Scala version\nsbt publishLocal\nYou can also specify sbt heap size with -mem, e.g. sbt -mem 8192. Use the SBT_OPTS environment variable for more fine grained settings.\nexport SBT_OPTS=\"-Xmx8G -Xms8G -Xss1M -XX:MaxMetaspaceSize=1G -XX:+CMSClassUnloadingEnabled -XX:ReservedCodeCacheSize=128m\"\nTo ensure the project loads and builds successfully, run the following sbt command so that all custom tasks are executed\nsbt compile test:compile","title":"Building Scio"},{"location":"/Getting-Started.html#running-the-examples","text":"You can execute the examples locally from SBT. By default pipelines will be executed using the `DirectRunner` and local filesystem will be used for input and output. Take a look at the examples to find out more.\nneville@localhost scio $ sbt\n[info] ...\n> project scio-examples\n[info] ...\n> runMain com.spotify.scio.examples.WordCount --input=<FILE PATTERN> --output=<DIRECTORY>\nNote Unlike Hadoop, Scio or Dataflow input should be file patterns and not directories, i.e. gs://bucket/path/part-*.txt and not gs://bucket/path. Output on the other hand should be directories just like Hadoop, so gs://bucket/path will produce files like gs://bucket/path/part-00000-of-00005.txt.\nUse the `DataflowRunner` to execute pipelines on Google Cloud Dataflow service using managed resources in the Google Cloud Platform.\nneville@localhost scio $ sbt\n[info] ...\n> project scio-examples\n[info] ...\n> set beamRunners := \"DataflowRunner\"\n[info] ...\n> runMain com.spotify.scio.examples.WordCount\n--project=<PROJECT ID>\n--zone=<GCE AVAILABILITY ZONE> --runner=DataflowRunner\n--input=<FILE PATTERN> --output=<DIRECTORY>\nThe Cloud Platform project refers to its name (not number). GCE availability zone should be in the same region as the BigQuery datasets and GCS bucket.\nBy default only DirectRunner is in the library dependencies list. Use set beamRunners := \"<runners>\" to specify additional runner dependencies as a comma separated list, i.e. “DataflowRunner,FlinkRunner”.","title":"Running the Examples"},{"location":"/Getting-Started.html#sbt-project-setup","text":"To create a new SBT project using Giter8 scio-template, simply:\nsbt new spotify/scio-template.g8\nOr add the following to your build.sbt. Replace the direct and Dataflow runner with ones you wish to use. The compiler plugin dependency is only needed for the type safe BigQuery API.\nlibraryDependencies ++= Seq(\n  \"com.spotify\" %% \"scio-core\" % \"0.6.1\",\n  \"com.spotify\" %% \"scio-test\" % \"0.6.1\" % \"test\",\n  \"org.apache.beam\" % \"beam-runners-direct-java\" % \"2.6.0\",\n  \"org.apache.beam\" % \"beam-runners-google-cloud-dataflow-java\" % \"2.6.0\"\n)\n\naddCompilerPlugin(\"org.scalamacros\" % \"paradise\" % \"2.1.1\" cross CrossVersion.full)","title":"SBT project setup"},{"location":"/Getting-Started.html#bigquery-settings","text":"You may need a few extra settings to use BigQuery queries as pipeline input.\nsbt -Dbigquery.project=<PROJECT-ID>\nbigquery.project: GCP project to make BigQuery requests with at compile time. bigquery.secret: By default the credential in Google Cloud SDK will be used. A JSON secret file can be used instead with -Dbigquery.secret=secret.json.","title":"BigQuery Settings"},{"location":"/Getting-Started.html#options","text":"The following options should be specified when running a job on Google Cloud Dataflow service.\n--project - The project ID for your Google Cloud Project. This is required if you want to run your pipeline using the Cloud Dataflow managed service. --zone - The Compute Engine availability zone for launching worker instances to run your pipeline.\nFor pipeline execution parameters and optimization, see the following documents.\nSpecifying Pipeline Execution Parameters Service Optimization and Execution\nThe defaults should work well for most cases but we sometimes tune the following parameters manually. - --workerMachineType - start with smaller types like n1-standard-1 and go up if you run into memory problem. n1-standard-4 works well for a lot of our memory hungry jobs. - --maxNumWorkers - avoid setting it to too high, i.e. 1000 or close to quota, since that reduces available instances for other jobs and more workers means more expensive shuffle. - --diskSizeGb - increase this if you run into disk space problem during shuffle, or alternatively optimize code by replacing groupByKey with reduceByKey or sumByKey. - --workerDiskType - specify SSD for jobs with really expensive shuffles. See a list of disk types here. Also see this page about persistent disk size and type. - --network - specify this if you use VPN to communicate with external services, e.g. HDFS on an on-premise cluster.\nMore Dataflow pipeline specific options available can be found in `DataflowPipelineOptions` and super interfaces. Some more useful ones are from `DataflowPipelineWorkerPoolOptions`.\n`DataflowWorkerHarnessOptions#getWorkerCacheMb` affects side input performance but needs an extra step to enable. See this FAQ item.\nThere are a few more experimental settings that might help specific scenarios: - --experiments=shuffle_mode=service - use external shuffle service instead of local disk - --experiments=enable_custom_bigquery_sink - new custom sink that works around certain limitations when writing to BigQuery - --experiments=worker_region-<REGION> - use specified Google Cloud region instead of zone for more flexible capacity","title":"Options"},{"location":"/examples.html","text":"","title":""},{"location":"/io/index.html","text":"","title":"IO"},{"location":"/io/index.html#io","text":"BigQuery Bigtable Avro Protobuf Parquet","title":"IO"},{"location":"/io/Type-Safe-BigQuery.html","text":"","title":"BigQuery"},{"location":"/io/Type-Safe-BigQuery.html#bigquery","text":"","title":"BigQuery"},{"location":"/io/Type-Safe-BigQuery.html#background","text":"NOTE that there are currently two BigQuery dialects, the legacy query syntax and the new SQL 2011 standard. The SQL standard is highly recommended since it generates dry-run schemas consistent with actual result and eliminates a lot of edge cases when working with records in a type-safe manner. To use standard SQL, prefix your query with #standardsql.","title":"Background"},{"location":"/io/Type-Safe-BigQuery.html#tablerow","text":"BigQuery rows are represented as TableRow in the BigQuery Java API which is basically a Map<String, Object>. Fields are accessed by name strings and values must be cast or converted to the desired type, both of which are error prone process.","title":"TableRow"},{"location":"/io/Type-Safe-BigQuery.html#type-safe-bigquery","text":"The type safe BigQuery API in Scio represents rows as case classes and generates TableSchema converters automatically at compile time with the following mapping logic:\nNullable fields are mapped to Option[T]s Repeated fields are mapped to List[T]s Records are mapped to nested case classes Timestamps are mapped to Joda Time Instant\nSee documentation for BigQueryType for the complete list of supported types.","title":"Type safe BigQuery"},{"location":"/io/Type-Safe-BigQuery.html#type-annotations","text":"There are 5 annotations for type safe code generation.","title":"Type annotations"},{"location":"/io/Type-Safe-BigQuery.html#bigquerytype-fromstorage","text":"This expands a class with output fields from a BigQuery Storage API read. Note that class Row has no body definition and is expanded by the annotation at compile time based on actual table schema.\nStorage API provides fast access to BigQuery-managed storage by using an rpc-based protocol. It is preferred over @BigQueryType.fromTable and @bigQueryType.fromQuery. For comparison:\nfromTable exports the entire table to Avro files on GCS and reads from them. This incurs export cost and export quota. It can also be wasteful if only a fraction of the columns/rows are needed. fromQuery executes the query and saves result as a temporary table before reading it like fromTable. This incurs both query and export cost plus export quota. fromStorage accesses the underlying BigQuery storage directly, reading only columns and rows based on selectedFields and rowRestriction. No query, export cost or quota hit.\nimport com.spotify.scio.bigquery.types.BigQueryType\n\n@BigQueryType.fromStorage(\n    \"bigquery-public-data:samples.gsod\",\n    selectedFields = List(\"tornado\", \"month\"),\n    rowRestriction = \"tornado = true\"\n  )\n  class Row","title":"BigQueryType.fromStorage"},{"location":"/io/Type-Safe-BigQuery.html#bigquerytype-fromtable","text":"This expands a class with fields that map to a BigQuery table.\nimport com.spotify.scio.bigquery.types.BigQueryType\n\n@BigQueryType.fromTable(\"bigquery-public-data:samples.gsod\")\nclass Row","title":"BigQueryType.fromTable"},{"location":"/io/Type-Safe-BigQuery.html#bigquerytype-fromquery","text":"This expands a class with output fields from a SELECT query. A dry run is executed at compile time to get output schema and does not incur additional cost.\nimport com.spotify.scio.bigquery.types.BigQueryType\n\n@BigQueryType.fromQuery(\"SELECT tornado, month FROM [bigquery-public-data:samples.gsod]\")\nclass Row\nThe query string may also contain \"%s\"s and additional arguments for parameterized query. This could be handy for log type data.\nimport com.spotify.scio.bigquery.types.BigQueryType\n\n// generate schema at compile time from a specific date\n@BigQueryType.fromQuery(\"SELECT user, url FROM [my-project:logs.log_%s]\", \"20160101\")\nclass Row\n\n// generate new query strings at runtime\nval newQuery = Row.query(args(0))\nThere’s also a $LATEST placeholder for table partitions. The latest common partition for all tables with the placeholder will be used.\nimport com.spotify.scio.bigquery.types.BigQueryType\n\n// generate schema at compile time from the latest date available in both my-project:log1.log_* and my-project:log2.log_*\n@BigQueryType.fromQuery(\n  \"SELECT user, url, action FROM [my-project:log1.log_%s] JOIN [my-project:log2.log_%s] USING user\",\n  \"$LATEST\", \"$LATEST\")\nclass Row\n\n// generate new query strings at runtime\nval newQuery = Row.query(args(0), args(0))","title":"BigQueryType.fromQuery"},{"location":"/io/Type-Safe-BigQuery.html#bigquerytype-fromschema","text":"This annotation gets schema from a string parameter and is useful in tests.\nimport com.spotify.scio.bigquery.types.BigQueryType\n\n@BigQueryType.fromSchema(\n  \"\"\"\n    |{\n    |  \"fields\": [\n    |    {\"mode\": \"REQUIRED\", \"name\": \"f1\", \"type\": \"INTEGER\"},\n    |    {\"mode\": \"REQUIRED\", \"name\": \"f2\", \"type\": \"FLOAT\"},\n    |    {\"mode\": \"REQUIRED\", \"name\": \"f3\", \"type\": \"BOOLEAN\"},\n    |    {\"mode\": \"REQUIRED\", \"name\": \"f4\", \"type\": \"STRING\"},\n    |    {\"mode\": \"REQUIRED\", \"name\": \"f5\", \"type\": \"TIMESTAMP\"}\n    |    ]\n    |}\n  \"\"\".stripMargin)\nclass Row","title":"BigQueryType.fromSchema"},{"location":"/io/Type-Safe-BigQuery.html#bigquerytype-totable","text":"This annotation works the other way around. Instead of generating class definition from a BigQuery schema, it generates BigQuery schema from a case class definition.\nimport com.spotify.scio.bigquery.types.BigQueryType\n\n@BigQueryType.toTable\ncase class Result(user: String, url: String, time: Long)\nFields in the case class and the class itself can also be annotated with @description which propagates to BigQuery schema.\nimport com.spotify.scio.bigquery.types.BigQueryType\nimport com.spotify.scio.bigquery.description\n\n@BigQueryType.toTable\n@description(\"A list of users mapped to the urls they visited\")\ncase class Result(user: String,\n                  url: String,\n                  @description(\"Milliseconds since Unix epoch\") time: Long)","title":"BigQueryType.toTable"},{"location":"/io/Type-Safe-BigQuery.html#annotation-parameters","text":"Note that due to the nature of Scala macros, only string literals and multi-line strings with optional .stripMargin are allowed as parameters to BigQueryType.fromTable, BigQueryType.fromQuery and BigQueryType.fromSchema.\nThese are OK:\nimport com.spotify.scio.bigquery.types.BigQueryType\n\n@BigQueryType.fromTable(\"project-id:dataset-id.table-id\")\nclass Row1\n\n@BigQueryType.fromQuery(\n  \"\"\"\n    |SELECT user, url\n    |FROM [project-id:dataset-id.table-id]\n  \"\"\".stripMargin)\nclass Row2\nAnd these are not:\nimport com.spotify.scio.bigquery.types.BigQueryType\n\n@BigQueryType.fromQuery(\"SELECT \" + args(1) + \" FROM [\" + args(2) + \"]\")\nclass Row1\n\nval sql = \"SELECT \" + args(1) + \" FROM [\" + args(2) + \"]\"\n@BigQueryType.fromQuery(sql)\nclass Row2\n// error: Unsupported argument \"SELECT \".$plus(args(1)).$plus(\" FROM [\").$plus(args(2)).$plus(\"]\")\n// @BigQueryType.fromQuery(\"SELECT \" + args(1) + \" FROM [\" + args(2) + \"]\")\n//  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n// error: Unsupported argument sql\n// @BigQueryType.fromQuery(sql)\n//  ^^^^^^^^^^^^^^^^^^^^^^^^^^^","title":"Annotation parameters"},{"location":"/io/Type-Safe-BigQuery.html#companion-objects","text":"Classes annotated with the type safe BigQuery API have a few more convenience methods.\nschema: TableSchema - BigQuery schema fromTableRow: (TableRow => T) - TableRow to case class converter toTableRow: (T => TableRow) - case class to TableRow converter toPrettyString(indent: Int = 0) - pretty string representation of the schema\nimport com.spotify.scio.bigquery.types.BigQueryType\n\n@BigQueryType.fromTable(\"bigquery-public-data:samples.gsod\")\nclass Row\n\nRow.toPrettyString(2)\n// res9: String = \"\"\"@BigQueryType.toTable\n// case class Row(\n//   station_number: Long,\n//   wban_number: Option[Long],\n//   year: Long,\n//   month: Long,\n//   day: Long,\n//   mean_temp: Option[Double],\n//   num_mean_temp_samples: Option[Long],\n//   mean_dew_point: Option[Double],\n//   num_mean_dew_point_samples: Option[Long],\n//   mean_sealevel_pressure: Option[Double],\n//   num_mean_sealevel_pressure_samples: Option[Long],\n//   mean_station_pressure: Option[Double],\n//   num_mean_station_pressure_samples: Option[Long],\n//   mean_visibility: Option[Double],\n//   num_mean_visibility_samples: Option[Long],\n//   mean_wind_speed: Option[Double],\n//   num_mean_wind_speed_samples: Option[Long],\n//   max_sustained_wind_speed: Option[Double],\n//   max_gust_wind_speed: Option[Double],\n//   max_temperature: Option[Double],\n//   max_temperature_explicit: Option[Boolean],\n//   min_temperature: Option[Double],\n//   min_temperature_explicit: Option[Boolean],\n//   total_precipitation: Option[Double],\n//   snow_depth: Option[Double],\n//   fog: Option[Boolean],\n//   rain: Option[Boolean],\n//   snow: Option[Boolean],\n//   hail: Option[Boolean],\n//   thunder: Option[Boolean],\n//   tornado: Option[Boolean])\"\"\"\nIn addition, BigQueryType.fromTable and BigQueryTable.fromQuery generate table: String and query: String respectively that refers to parameters in the original annotation.\nimport com.spotify.scio.bigquery.types.BigQueryTypeUser defined companion objects may interfere with macro code generation so for now do not provide one to a case class annotated with @BigQueryType.toTable, i.e. object Row.","title":"Companion objects"},{"location":"/io/Type-Safe-BigQuery.html#using-type-safe-bigquery","text":"","title":"Using type safe BigQuery"},{"location":"/io/Type-Safe-BigQuery.html#type-safe-bigquery-with-scio","text":"To enable type safe BigQuery for ScioContext:\nimport com.spotify.scio._\nimport com.spotify.scio.bigquery._\nimport com.spotify.scio.bigquery.types.BigQueryType\n\n@BigQueryType.fromQuery(\"SELECT tornado, month FROM [bigquery-public-data:samples.gsod]\")\nclass Row\n\n@BigQueryType.toTable\ncase class Result(month: Long, tornado_count: Long)\n\ndef main(cmdlineArgs: Array[String]): Unit = {\n  val (sc, args) = ContextAndArgs(cmdlineArgs)\n  sc.typedBigQuery[Row]()  // query string from Row.query\n    .flatMap(r => if (r.tornado.getOrElse(false)) Seq(r.month) else Nil)\n    .countByValue\n    .map(kv => Result(kv._1, kv._2))\n    .saveAsTypedBigQueryTable(Table.Spec(args(\"output\")))  // schema from Row.schema\n  sc.run()\n  ()\n}","title":"Type safe BigQuery with Scio"},{"location":"/io/Type-Safe-BigQuery.html#type-safe-bigqueryclient","text":"Annotated classes can be used with the BigQueryClient directly too.\nimport com.spotify.scio.bigquery.types.BigQueryType\nimport com.spotify.scio.bigquery.client.BigQuery\n\n@BigQueryType.fromQuery(\"SELECT tornado, month FROM [bigquery-public-data:samples.gsod]\")\nclass Row\n\ndef bq = BigQuery.defaultInstance()\ndef rows = bq.getTypedRows[Row]()\ndef result = bq.writeTypedRows(\"project-id:dataset-id.table-id\", rows.toList)","title":"Type safe BigQueryClient"},{"location":"/io/Type-Safe-BigQuery.html#using-type-safe-bigquery-directly-with-beams-io-library","text":"If there are any BigQuery I/O operations supported in the Apache Beam client but not exposed in Scio, you may choose to use the Beam transform directly using Scio’s .saveAsCustomOutput() option:\nimport com.spotify.scio.values.SCollection\nimport com.spotify.scio.bigquery.types.BigQueryType\nimport org.apache.beam.sdk.io.gcp.bigquery.BigQueryIO\n\n@BigQueryType.fromQuery(\"SELECT tornado, month FROM [bigquery-public-data:samples.gsod]\")\nclass Foo\n\ndef bigQueryType = BigQueryType[Foo]\ndef tableRows: SCollection[Foo] = ???\n\ndef result =\n  tableRows\n    .map(bigQueryType.toTableRow)\n    .saveAsCustomOutput(\n      \"custom bigquery IO\",\n      BigQueryIO\n        .writeTableRows()\n        .to(\"my-project:my-dataset.my-table\")\n        .withSchema(bigQueryType.schema)\n        .withCreateDisposition(???)\n        .withWriteDisposition(???)\n        .withFailedInsertRetryPolicy(???)\n      )","title":"Using type safe BigQuery directly with Beam’s IO library"},{"location":"/io/Type-Safe-BigQuery.html#bigquerytype-and-intellij-idea","text":"See the FAQ for making IntelliJ happy with type safe BigQuery.","title":"BigQueryType and IntelliJ IDEA"},{"location":"/io/Type-Safe-BigQuery.html#custom-types-and-validation","text":"See OverrideTypeProvider for details about the custom types and validation mechanism.","title":"Custom types and validation"},{"location":"/io/Bigtable.html","text":"","title":"Bigtable"},{"location":"/io/Bigtable.html#bigtable","text":"First please read Google’s official doc.","title":"Bigtable"},{"location":"/io/Bigtable.html#bigtable-example","text":"This depends on APIs from scio-bigtable and imports from com.spotify.scio.bigtable._.\nLook at example here.","title":"Bigtable example"},{"location":"/io/Bigtable.html#common-issues","text":"","title":"Common issues"},{"location":"/io/Bigtable.html#size-of-the-cluster-vs-dataflow-cluster","text":"As a general note when writing to Bigtable from Dataflow you should at most use the # of Bigtable nodes you have * 3 cpus. Otherwise Bigtable will be overwhelmed and throttle the writes (and the reads)","title":"Size of the cluster vs Dataflow cluster"},{"location":"/io/Bigtable.html#cell-compression","text":"Bigtable doesn’t compress cell values > 1Mb","title":"Cell compression"},{"location":"/io/Bigtable.html#jetty-alpn-npn-has-not-been-properly-configured","text":"Check that your versions of grpc-netty, netty-handler, and netty-tcnative-boringssl-static are compatible.","title":"Jetty ALPN/NPN has not been properly configured"},{"location":"/io/Bigtable.html#bigtableio","text":"The BigtableIO included in the Dataflow SDK is not recommended for use. It is not written by the Bigtable team and is significantly less performant than the HBase Bigtable Dataflow connector. Please see the example above for the recommended API.","title":"BigtableIO"},{"location":"/io/Bigtable.html#key-structure","text":"Your row key should not contain common parts at the beginning of the key, doing so would overload specific Bigtable nodes. For example, if your row is identifiable by user-id and date key - do NOT use date,user-id, instead use user-id,date or even better in case of date use Bigtable version/timestamp. Read more about row key design over here.","title":"Key structure"},{"location":"/io/Bigtable.html#performance","text":"Read Google doc.","title":"Performance"},{"location":"/io/Bigtable.html#bigtable-vs-datastore","text":"If you require replacement for Cassandra, Bigtable is probable the most straightforward replacement in GCP. Bigtable white paper. To quote the paper - think of Bigtable as:\na sparse, distributed, persistent multidimensional sorted map. The map is indexed by a row key, column key, and a timestamp; each value in the map is an uninterpreted array of bytes.\nBigtable is replicated only within a single zone. Bigtable does not support transactions, that said all operations are atomic at the row level.\nThink of Datastore as distributed, persistent, fully managed key-value store, with support for transactions. Datastore is replicated across multiple datacenters thus making it theoretically more available than Bigtable (as of today).\nRead more about Bigtable here, and more about Datastore over here.","title":"Bigtable vs Datastore"},{"location":"/io/Avro.html","text":"","title":"Avro"},{"location":"/io/Avro.html#avro","text":"","title":"Avro"},{"location":"/io/Avro.html#read-avro-files","text":"Scio comes with support for reading Avro files. Avro supports generic or specific records, Scio supports both via the same method (avroFile), but depending on the type parameter.","title":"Read Avro files"},{"location":"/io/Avro.html#read-specific-records","text":"import com.spotify.scio.ScioContext\nimport com.spotify.scio.avro._\n\nimport org.apache.avro.specific.SpecificRecord\n\ndef sc: ScioContext = ???\n\n// SpecificRecordClass is compiled from Avro schema files\ndef result = sc.avroFile[SpecificRecord](\"gs://path-to-data/lake/part-*.avro\")","title":"Read Specific records"},{"location":"/io/Avro.html#read-generic-records","text":"import com.spotify.scio.ScioContext\nimport com.spotify.scio.avro._\n\nimport org.apache.avro.generic.GenericRecord\nimport org.apache.avro.Schema\n\ndef yourAvroSchema: Schema = ???\n\ndef sc: ScioContext = ???\n\ndef result = sc.avroFile(\"gs://path-to-data/lake/part-*.avro\", yourAvroSchema)\n// `record` is of GenericRecord type","title":"Read Generic records"},{"location":"/io/Avro.html#write-avro-files","text":"Scio comes with support for writing Avro files. Avro supports generic or specific records, Scio supports both via the same method (saveAsAvroFile), but depending on the type of the content of SCollection.","title":"Write Avro files"},{"location":"/io/Avro.html#write-specific-records","text":"import com.spotify.scio.values.SCollection\nimport com.spotify.scio.avro._\n\nimport org.apache.avro.specific.SpecificRecord\n\ncase class Foo(x: Int, s: String)\ndef sc: SCollection[Foo] = ???\n\n// convert to avro SpecificRecord\ndef fn(f: Foo): SpecificRecord = ???\n\n// type of Avro specific records will hold information about schema,\n// therefor Scio will figure out the schema by itself\n\ndef result =  sc.map(fn).saveAsAvroFile(\"gs://path-to-data/lake/output\")","title":"Write Specific records"},{"location":"/io/Avro.html#write-generic-records","text":"import com.spotify.scio.values.SCollection\nimport com.spotify.scio.avro._\n\nimport org.apache.avro.generic.GenericRecord\nimport org.apache.avro.Schema\n\ncase class Foo(x: Int, s: String)\ndef sc: SCollection[Foo] = ???\n\ndef yourAvroSchema: Schema = ???\n\n// convert to avro SpecificRecord\ndef fn(f: Foo): GenericRecord = ???\n\n// writing Avro generic records requires additional argument `schema`\ndef result =  sc.map(fn).saveAsAvroFile(\"gs://path-to-data/lake/output\", schema = yourAvroSchema)","title":"Write Generic records"},{"location":"/io/Avro.html#rules-for-schema-evolution","text":"Unless impossible, provide default values for your fields. New field must have a default value. You can only delete field which has default value. Do not change data type of an existing fields. If needed add a new field to the schema. Do not rename existing fields. If needed use aliases.","title":"Rules for schema evolution"},{"location":"/io/Avro.html#common-issues-guidelines","text":"Follow Avro guidelines, especially the one about schema evolution Wherever possible use specific records Use Builder pattern to construct Avro records","title":"Common issues/guidelines"},{"location":"/io/Protobuf.html","text":"","title":"Protobuf"},{"location":"/io/Protobuf.html#protobuf","text":"","title":"Protobuf"},{"location":"/io/Protobuf.html#read-protobuf-files","text":"Scio comes with custom and efficient support for reading Protobuf files via protobufFile method, for example:\nimport com.spotify.scio.avro._\n\n// FooBarProto is a Protobuf generated class (must be a subclass of Protobuf's `Message`)\nsc.protobufFile[FooBarProto](\"gs://path-to-data/lake/part-*.protobuf.avro\")\n  .map( message => ??? )\n// `message` is of type FooBarProto\nImportant: in most cases the input files should have been previously written by Scio. The reason is that Scio assumes that serialized Protobuf message is stored inside bytes Avro record.\nIf you want to read serialized protobuf messages directly from a file, one solution is to use textFile followed by a map to parse your messages.","title":"Read Protobuf files"},{"location":"/io/Protobuf.html#write-protobuf-files","text":"Scio comes with custom and efficient support for writing Protobuf files via saveAsProtobufFile method, for example:\n// FooBarProto is a Protobuf generated class (must be a subclass of Protobuf's `Message`)\nval data: SCollection[FooBarProto] = ???\ndata.saveAsProtobufFile(\"gs://path-to-data/lake/protos-out\")","title":"Write Protobuf files"},{"location":"/io/Protobuf.html#file-format","text":"Scio’s Protobuf file is backed by Avro with the following schema:\n{\n  \"type\" : \"record\",\n  \"name\" : \"AvroBytesRecord\",\n  \"fields\" : [ {\n    \"name\" : \"bytes\",\n    \"type\" : \"bytes\"\n  } ]\n}\nAvro gives us a block based file format with compression, split and combine support. Protobuf binary is stored in the bytes field of AvroBytesRecord.\nStarting with Scio 0.2.6, the Protobuf schema also is stored as a JSON string in the Avro file metadata under the key protobuf.generic.schema. You can get the schema or JSON records using the proto-tools command line tool from gcs-tools (available in our homebrew tap). Conversion between Protobuf schema, binary and JSON is done via the protobuf-generic library.\nbrew tap spotify/public\nbrew install gcs-proto-tools\nproto-tools getschema data.protobuf.avro\nproto-tools tojson data.protobuf.avro","title":"File format"},{"location":"/io/Protobuf.html#common-issues","text":"","title":"Common issues"},{"location":"/io/Protobuf.html#scalapb","text":"If you end up using ScalaPB, make sure to use java based message class as input/output type, Scala based message class does not inherit from Protobuf’s Message class. To generate both Scala and Java classes add (to your build.sbt):\nimport com.trueaccord.scalapb.{ScalaPbPlugin => PB}\nPB.javaConversions in PB.protobufConfig := true","title":"ScalaPB"},{"location":"/io/Parquet.html","text":"","title":"Parquet"},{"location":"/io/Parquet.html#parquet","text":"Scio supports reading and writing Parquet files as Avro records. It also includes parquet-avro-extra macros for generating column projections and row predicates using idiomatic Scala syntax. Also see Avro page on reading and writing regular Avro files.","title":"Parquet"},{"location":"/io/Parquet.html#read-avro-parquet-files","text":"When reading Parquet files, only Avro specific records are supported.\nTo read a Parquet file with column projections and row predicates:\nimport com.spotify.scio._\nimport com.spotify.scio.parquet.avro._\nimport com.spotify.scio.avro.TestRecord\n\nobject ParquetJob {\n  def main(cmdlineArgs: Array[String]): Unit = {\n\n    val (sc, args) = ContextAndArgs(cmdlineArgs)\n\n    // Macros for generating column projections and row predicates\n    val projection = Projection[TestRecord](_.getIntField, _.getLongField, _.getBooleanField)\n    val predicate = Predicate[TestRecord](x => x.getIntField > 0 && x.getBooleanField)\n\n    sc.parquetAvroFile[TestRecord](\"input.parquet\", projection, predicate)\n      // Map out projected fields right after reading\n      .map(r => (r.getIntField, r.getStringField, r.getBooleanField))\n\n    sc.run()\n    ()\n  }\n}\nNote that the result TestRecords are not complete Avro objects. Only the projected columns (intField, stringField, booleanField) are present while the rest are null. These objects may fail serialization and it’s recommended that you map them out to tuples or case classes right after reading.\nAlso note that predicate logic is only applied when reading actual Parquet files but not in JobTest. To retain the filter behavior while using mock input, it’s recommend that you do the following.\nimport com.spotify.scio._\nimport com.spotify.scio.parquet.avro._\nimport com.spotify.scio.avro.TestRecord\n\nobject ParquetJob {\n  def main(cmdlineArgs: Array[String]): Unit = {\n\n    val (sc, args) = ContextAndArgs(cmdlineArgs)\n\n    val projection = Projection[TestRecord](_.getIntField, _.getLongField, _.getBooleanField)\n    // Build both native filter function and Parquet FilterPredicate\n    // case class Predicates[T](native: T => Boolean, parquet: FilterPredicate)\n    val predicate = Predicate.build[TestRecord](x => x.getIntField > 0 && x.getBooleanField)\n\n    sc.parquetAvroFile[TestRecord](\"input.parquet\", projection, predicate.parquet)\n      // filter natively with the same logic in case of mock input in `JobTest`\n      .filter(predicate.native)\n\n    sc.run()\n    ()\n  }\n}","title":"Read Avro Parquet files"},{"location":"/io/Parquet.html#write-avro-parquet-files","text":"Both Avro generic and specific records are supported when writing.\nType of Avro specific records will hold information about schema, therefore Scio will figure out the schema by itself:\nimport com.spotify.scio.values._\nimport com.spotify.scio.parquet.avro._\nimport com.spotify.scio.avro.TestRecord\n\ndef input: SCollection[TestRecord] = ???\ndef result = input.saveAsParquetAvroFile(\"gs://path-to-data/lake/output\")\nWriting Avro generic records requires additional argument schema:\nimport com.spotify.scio.values._\nimport com.spotify.scio.parquet.avro._\nimport org.apache.avro.generic.GenericRecord\n\ndef input: SCollection[GenericRecord] = ???\ndef yourAvroSchema: org.apache.avro.Schema = ???\n\ndef result = input.saveAsParquetAvroFile(\"gs://path-to-data/lake/output\", schema = yourAvroSchema)","title":"Write Avro Parquet files"},{"location":"/Scio-Unit-Tests.html","text":"","title":"Testing"},{"location":"/Scio-Unit-Tests.html#testing","text":"To write Scio unit tests you will need to add the following dependency to your build.sbt\nlibraryDependencies ++= Seq(\n  // .......\n  \"com.spotify\" %% \"scio-test\" % scioVersion % Test,\n  // .......\n)\nTo run the test, you can run the following commands. You can skip the first two lines if you already ran them and are still in the sbt shell.\n$ sbt\n> project scio-examples\n> test\nClick on this link for more Scala testing tasks.","title":"Testing"},{"location":"/Scio-Unit-Tests.html#test-entire-pipeline","text":"We will use the WordCountTest to explain how Scio tests work. WordCount is the pipeline under test. Full example code for WordCountTest and other test examples can be found here.\nLet’s walk through the details of the test: The test class should extend the PipelineSpec which is a trait for unit testing pipelines.\nThe inData variable holds the input data for your test and the expected variable contains the expected results after your pipeline processes the inData. The WordCount pipeline counts the occurrence of each word, the given the input data , we should expect a count of a=3, b=3, c=1 etc\nval inData = Seq(\"a b c d e\", \"a b a b\", \"\")\n// inData: Seq[String] = List(\"a b c d e\", \"a b a b\", \"\")\n val expected = Seq(\"a: 3\", \"b: 3\", \"c: 1\", \"d: 1\", \"e: 1\")\n// expected: Seq[String] = List(\"a: 3\", \"b: 3\", \"c: 1\", \"d: 1\", \"e: 1\")\nUsing JobTest, you can test the entire pipeline. Specify the type of the class under test, in this case it is com.spotify.scio.examples.WordCount.type . The args function takes the list of command line arguments passed to the main function of WordCount. The WordCount’s main function expects input and output arguments passed to it.\n\"WordCount\" should \"work\" in {\n  JobTest[com.spotify.scio.examples.WordCount.type]\n    .args(\"--input=in.txt\", \"--output=out.txt\")\n    .input(TextIO(\"in.txt\"), inData)\n    .output(TextIO(\"out.txt\"))(coll => coll should containInAnyOrder(expected))\n    .run()\n}\nThe input function injects your input test data. Note that the TestIO[T] should match the input source used in the pipeline e.g. TextIO for sc.textFile, AvroIO for sc.avro. The TextIO id (“in.txt”) should match the one specified in the args.\nThe output function evaluates the output of the pipeline using the provided assertion from the SCollectionMatchers. More info on SCollectionMatchers can be found here. In this example, we are asserting that the output of the pipeline should contain an SCollection with elements that in the expected variable in any order. Also, note that the TestIO[T] should match the output used in the pipeline e.g. TextIO for sc.saveAsTextFile\nThe run function will run the pipeline.","title":"Test entire pipeline"},{"location":"/Scio-Unit-Tests.html#test-for-pipeline-with-sideinput","text":"We will use the SideInputJoinExamples test in JoinExamplesTest to illustrate how to write a test for pipelines with sideinputs. The SideInputJoinExamples pipeline has two input sources, one for eventsInfo and the other for countryInfo. CountryInfo is used as a sideinput to join with eventInfo.\nSince we have two input sources, we have to specify both in the JobTest. Note that the injected data type should match one expected by the sink.\n\"SideInputJoinExamples\" should \"work\" in {\n  JobTest[com.spotify.scio.examples.cookbook.SideInputJoinExamples.type]\n    .args(\"--output=out.txt\")\n    .input(BigQueryIO(ExampleData.EVENT_TABLE), eventData)\n    .input(BigQueryIO(ExampleData.COUNTRY_TABLE), countryData)\n    .output(TextIO(\"out.txt\"))(coll => coll should containInAnyOrder(expected))\n    .run()\n}","title":"Test for pipeline with sideinput"},{"location":"/Scio-Unit-Tests.html#test-for-pipeline-with-sideoutput","text":"SideInOutExampleTest shows an example of how to test pipelines with sideoutputs. Each sideoutput is evaluated using the output function. The ids for TextIO e.g. “out1.txt” should match the ones specified in the args.\nval inData = Seq(\"The quick brown fox jumps over the lazy dog.\")\n\n\"SideInOutExample\" should \"work\" in {\n  JobTest[SideInOutExample.type]\n    .args(\n      \"--input=in.txt\",\n      \"--stopWords=stop.txt\",\n      \"--output1=out1.txt\",\n      \"--output2=out2.txt\",\n      \"--output3=out3.txt\",\n      \"--output4=out4.txt\"\n    )\n    .input(TextIO(\"in.txt\"), inData)\n    .input(TextIO(\"stop.txt\"), Seq(\"the\"))\n    .output(TextIO(\"out1.txt\"))(coll => coll should containInAnyOrder(Seq.empty[String]))\n    .output(TextIO(\"out2.txt\"))(coll => coll should containInAnyOrder(Seq.empty[String]))\n    .output(TextIO(\"out3.txt\"))(coll => coll should containInAnyOrder(Seq(\"dog: 1\", \"fox: 1\")))\n    .output(TextIO(\"out4.txt\")) {\n      _ should containInAnyOrder(Seq(\"brown: 1\", \"jumps: 1\", \"lazy: 1\", \"over: 1\", \"quick: 1\"))\n    }\n    .run()\n}","title":"Test for pipeline with sideoutput"},{"location":"/Scio-Unit-Tests.html#test-partial-pipeline","text":"To test a section of a pipeline, use runWithContext. The TriggerExample.extractFlowInfo test in TriggerExampleTest tests only the extractFlowInfo part of the pipeline.\nThe data variable hold the test data and sc.parallelize will transform the input iterable to a SCollection of strings. TriggerExample.extractFlowInfo will be executed using the ScioContext and you can then specify assertions against the result of the pipeline.\n\"TriggerExample.extractFlowInfo\" should \"work\" in {\n  val data = Seq(\n    \"01/01/2010 00:00:00,1108302,94,E,ML,36,100,29,0.0065,66,9,1,0.001,74.8,1,9,3,0.0028,71,1,9,\"\n      + \"12,0.0099,67.4,1,9,13,0.0121,99.0,1,,,,,0,,,,,0,,,,,0,,,,,0\",\n    \"01/01/2010 00:00:00,\"\n      + \"1100333,5,N,FR,9,0,39,,,9,,,,0,,,,,0,,,,,0,,,,,0,,,,,0,,,,,0,,,,,0,,,,\"\n  )\n  runWithContext { sc =>\n    val r = TriggerExample.extractFlowInfo(sc.parallelize(data))\n    r should haveSize(1)\n    r should containSingleValue((\"94\", 29))\n  }\n}","title":"Test partial pipeline"},{"location":"/Scio-Unit-Tests.html#test-for-pipeline-with-windowing","text":"We will use the LeaderBoardTest to explain how to test Windowing in Scio. The full example code is found here. LeaderBoardTest also extends PipelineSpec. The function under test is the LeaderBoard.calculateTeamScores. This function calculates teams scores within a fixed window with the following the window options: * Calculate the scores every time the window ends * Calculate an early/“speculative” result from partial data, 5 minutes after the first element in our window is processed (withEarlyFiring) * Accept late entries (and recalculates based on them) only if they arrive within the allowedLateness duration.\nIn this test, we are testing calculateTeamScores for when all of the elements arrive on time, i.e. before the watermark.\nFirst, we have to create an input stream representing an unbounded SCollection of type GameActionInfo using the testStreamOf. Each element is assigned a timestamp representing when each event occurred. In the code snippet above, we start at epoch equal zero, by setting watermark to 0 in the advanceWatermarkTo.\nWe add GameActionInfo elements with varying timestamps, and we advanced the watermark to 3 minutes. At this point, all elements are on time because they came before the watermark advances to 3 minutes.\nval stream = testStreamOf[GameActionInfo]\n// Start at the epoch\n  .advanceWatermarkTo(baseTime)\n  // add some elements ahead of the watermark\n  .addElements(\n    event(blueOne, 3, Duration.standardSeconds(3)),\n    event(blueOne, 2, Duration.standardMinutes(1)),\n    event(redTwo, 3, Duration.standardSeconds(22)),\n    event(blueTwo, 5, Duration.standardSeconds(3))\n  )\nWe then more GameActionInfo elements and advance the watermark to infinity by calling the advanceWatermarkToInfinity. Similarly, these elements are also on time because the watermark is infinity.\n// The watermark advances slightly, but not past the end of the window\n.advanceWatermarkTo(baseTime.plus(Duration.standardMinutes(3)))\n.addElements(\n  event(redOne, 1, Duration.standardMinutes(4)),\n  event(blueOne, 2, Duration.standardSeconds(270))\n)\n// The window should close and emit an ON_TIME pane\n.advanceWatermarkToInfinity\nTo run the test, we use the runWithContext, this will run calculateTeamScores using the ScioContext. In calculateTeamScores, we pass the SCollection we created above using testStreamOf. The IntervalWindow specifies the window for which we want to assert the SCollection of elements created by calculateTeamScores. We want to assert that elements with initial window of 0 to 20 minutes were on time. Next we assert, using inOnTimePane that the SCollection elements are equal to the expected sums.\nrunWithContext { sc =>\n  val teamScores =\n    LeaderBoard.calculateTeamScores(sc.testStream(stream), teamWindowDuration, allowedLateness)\n\n  val window = new IntervalWindow(baseTime, teamWindowDuration)\n  teamScores should inOnTimePane(window) {\n    containInAnyOrder(Seq((blueOne.team, 12), (redOne.team, 4)))\n  }\n}\nScio provides more SCollection assertions such as inWindow, inCombinedNonLatePanes, inFinalPane, and inOnlyPane. You can find the full list here. More information on testing unbounded pipelines can be found here.","title":"Test for pipeline with windowing"},{"location":"/Scio-REPL.html","text":"","title":"REPL"},{"location":"/Scio-REPL.html#repl","text":"The Scio REPL is an extension of the Scala REPL, with added functionality that allows you to interactively experiment with Scio. Think of it as a playground to try out things.","title":"REPL"},{"location":"/Scio-REPL.html#quick-start","text":"You can either install Scio REPL via our Homebrew tap on a Mac or download the pre-built jar on other platforms.","title":"Quick start"},{"location":"/Scio-REPL.html#homebrew","text":"brew tap spotify/public\nbrew install scio\nscio-repl","title":"Homebrew"},{"location":"/Scio-REPL.html#pre-built-jar","text":"To download pre-built jar of Scio REPL, find version you are interested in on the release page, and download the REPL jar from Downloads section.\n$ java -jar scio-repl-<version>.jar\nWelcome to\n                 _____\n    ________________(_)_____\n    __  ___/  ___/_  /_  __ \\\n    _(__  )/ /__ _  / / /_/ /\n    /____/ \\___/ /_/  \\____/   version 0.7.0\n\nUsing Scala version 2.12.8 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_144)\nType in expressions to have them evaluated.\nType :help for more information.\n\nBigQuery client available as 'bq'\nScio context available as 'sc'\n\nscio>\nA `ScioContext` is created on REPL startup as sc and a starting point to most operations. Use tab completion, history and other REPL goodies to play around.","title":"Pre-built jar"},{"location":"/Scio-REPL.html#start-from-sbt-console-scala-2-11-x-only-","text":"$ git clone git@github.com:spotify/scio.git\nCloning into 'scio'...\nremote: Counting objects: 9336, done.\nremote: Compressing objects: 100% (275/275), done.\nremote: Total 9336 (delta 139), reused 0 (delta 0), pack-reused 8830\nReceiving objects: 100% (9336/9336), 1.76 MiB | 0 bytes/s, done.\nResolving deltas: 100% (3509/3509), done.\nChecking connectivity... done.\n$ cd scio\n$ sbt scio-repl/run","title":"Start from SBT console (Scala 2.11.x+ only)"},{"location":"/Scio-REPL.html#build-repl-jar-manually","text":"You can also build REPL jar from source.\n$ git clone git@github.com:spotify/scio.git\nCloning into 'scio'...\nremote: Counting objects: 9336, done.\nremote: Compressing objects: 100% (275/275), done.\nremote: Total 9336 (delta 139), reused 0 (delta 0), pack-reused 8830\nReceiving objects: 100% (9336/9336), 1.76 MiB | 0 bytes/s, done.\nResolving deltas: 100% (3509/3509), done.\nChecking connectivity... done.\n$ cd scio\n$ sbt scio-repl/assembly","title":"Build REPL jar manually"},{"location":"/Scio-REPL.html#sbt-project-from-scio-template","text":"Projects generated from scio-template.g8 have built-in REPL. Run sbt repl/run from the project root.","title":"sbt project from scio-template"},{"location":"/Scio-REPL.html#tutorial","text":"","title":"Tutorial"},{"location":"/Scio-REPL.html#local-pipeline","text":"Let’s start with simple local-mode word count example:\ndef wordCount = sc\n    .textFile(\"README.md\")\n    .flatMap(_.split(\"[^a-zA-Z']+\").filter(_.nonEmpty))\n    .countByValue\n    .map(_.toString)\n    .saveAsTextFile(\"/tmp/local_wordcount\")\n\ndef scioResult = sc.run().waitUntilDone()\n\ndef values = scioResult.tap(wordCount).value.take(3)\nMake sure README.md is in the current directory. This example counts words in local file using a local runner (`DirectRunner` and writes result in a local file. The pipeline and actual computation starts on sc.run(). The last command take 3 lines from results and prints them.","title":"Local pipeline"},{"location":"/Scio-REPL.html#local-pipeline-","text":"In the next example we will spice things up a bit and read data from GCS:\n:newScio\ndef shakespeare = sc.textFile(\"gs://dataflow-samples/shakespeare/hamlet.txt\")\n\ndef wordCount = shakespeare\n    .flatMap(_.split(\"[^a-zA-Z']+\").filter(_.nonEmpty))\n    .countByValue\n    .map(_.toString)\n    .saveAsTextFile(\"/tmp/gcs-wordcount\")\n\ndef result = sc\n    .run()\n    .waitUntilDone()\n    .tap(wordCount)\n    .value\n    .take(3)\nEach Scio context is associated with one and only one pipeline. The previous instance of sc was used for the local pipeline example and cannot be reused anymore. The first magic command, :newScio creates a new context as sc. The pipeline still performs computation locally, but reads data from Google Cloud Storage (it could also be BigQuery, Datastore, etc). This example may take a bit longer due to additional network overhead.","title":"Local pipeline ++"},{"location":"/Scio-REPL.html#dataflow-service-pipeline","text":"To create a Scio context for Google Cloud Dataflow service, add Dataflow pipeline options when starting the REPL. The same options will also be used by :newScio when creating new context. For example:\n$ java -jar scio-repl-0.7.0.jar \\\n> --project=<project-id> \\\n> --stagingLocation=<stagin-dir> \\\n> --runner=DataflowRunner\nWelcome to\n                 _____\n    ________________(_)_____\n    __  ___/  ___/_  /_  __ \\\n    _(__  )/ /__ _  / / /_/ /\n    /____/ \\___/ /_/  \\____/   version 0.7.0\n\nUsing Scala version 2.12.8 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_144)\nType in expressions to have them evaluated.\nType :help for more information.\n\nBigQuery client available as 'bq'\nScio context available as 'sc'\ndef shakespeare = sc.textFile(\"gs://dataflow-samples/shakespeare/*\")\n\ndef wordCount = shakespeare\n    .flatMap(_.split(\"[^a-zA-Z']+\").filter(_.nonEmpty))\n    .countByValue\n    .map(_.toString)\n    .saveAsTextFile(\"gs://<gcs-output-dir>\")\n\ndef result = sc\n    .run()\n    .waitUntilDone()\n    .tap(wordCount)\n    .value\n    .take(3)\nIn this case we are reading data from GCS and performing computation in GCE virtual machines managed by Dataflow service. The last line is an example of reading data from GCS files to local memory after a context is closed. Most write operations in Scio return Future[Tap[T]] where a Tap[T] encapsulates some dataset that can be re-opened in another context or directly.\nUse :scioOpts to view or update Dataflow options inside the REPL. New options will be applied the next time you create a context.","title":"Dataflow service pipeline"},{"location":"/Scio-REPL.html#ad-hoc-local-mode","text":"You may start the REPL in distributed mode and run pipelines to aggregate from large datasets, and play around the results in local mode. You can create a local Scio context any time with :newLocalScio <name> and use it for local computations.\nscio> :newLocalScio lsc\nLocal Scio context available as 'lsc'","title":"Ad-hoc local mode"},{"location":"/Scio-REPL.html#bigquery-example","text":"In this example we will read some data from BigQuery and process it in Dataflow. We shall count number of tornadoes per month from a public sample dataset. Scio will do its best to find your configured Google Cloud project, but you can also specify it explicitly via -Dbigquery.project option.\n$ java -jar -Dbigquery.project=<project-id> scio-repl-0.7.0.jar \\\n> --project=<project-id> \\\n> --stagingLocation=<stagin-dir> \\\n> --runner=DataflowRunner\nWelcome to\n                 _____\n    ________________(_)_____\n    __  ___/  ___/_  /_  __ \\\n    _(__  )/ /__ _  / / /_/ /\n    /____/ \\___/ /_/  \\____/   version 0.7.0\n\nUsing Scala version 2.12.8 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_144)\nType in expressions to have them evaluated.\nType :help for more information.\n\nBigQuery client available as 'bq'\nScio context available as 'sc'\ndef tornadoes = sc.bigQuerySelect(Query(\"SELECT tornado, month FROM [clouddataflow-readonly:samples.weather_stations]\"))\n \ndef counts = tornadoes\n    .flatMap(r => if (r.getBoolean(\"tornado\")) Seq(r.getLong(\"month\")) else Nil)\n    .countByValue\n    .map(kv => TableRow(\"month\" -> kv._1, \"tornado_count\" -> kv._2))\n    .take(3)\n    .materialize\n\ndef result = sc\n    .run()\n    .waitUntilDone()\n    .tap(counts)\n    .value\nIn this example we combine power of BigQuery and flexibility of Dataflow. We first query BigQuery table, perform a couple of transformations and take (take(3)) some data back locally (materialize) to view the results.","title":"BigQuery example"},{"location":"/Scio-REPL.html#bigquery-project-id","text":"Scio REPL will do its best to find your configured Google Cloud project, without the need to explicitly specifying bigquery.project property. It will search for project-id in this specific order:\nbigquery.project java system property GCLOUD_PROJECT java system property GCLOUD_PROJECT environmental variable gcloud config files: scio named configuration default configuration\nThis means that you can always set bigquery.project and it will take precedence over other configurations. Read more about gcloud configuration here.","title":"BigQuery project id"},{"location":"/Scio-REPL.html#i-o-commands","text":"There are few built-in commands for simple file I/O.\nimport scala.reflect._\nimport kantan.csv._\n\n// Read from an Avro, text, CSV or TSV file on local filesystem or GCS.\ndef readAvro[T : ClassTag](path: String): Iterator[T] = ???\ndef readText(path: String): Iterator[String] = ???\ndef readCsv[T: RowDecoder](path: String,\n                           sep: Char = ',',\n                           header: Boolean = false): Iterator[T] = ???\ndef readTsv[T: RowDecoder](path: String,\n                           sep: Char = '\\t',\n                           header: Boolean = false): Iterator[T] = ???\n\n// Write to an Avro, text, CSV or TSV file on local filesystem or GCS.\ndef writeAvro[T: ClassTag](path: String, data: Seq[T]): Unit = ???\ndef writeText(path: String, data: Seq[String]): Unit = ???\ndef writeCsv[T: RowEncoder](path: String, data: Seq[T],\n                            sep: Char = ',',\n                            header: Seq[String] = Seq.empty): Unit = ???\ndef writeTsv[T: RowEncoder](path: String, data: Seq[T],\n                            sep: Char = '\\t',\n                            header: Seq[String] = Seq.empty): Unit = ???","title":"I/O Commands"},{"location":"/Scio-REPL.html#tips","text":"","title":"Tips"},{"location":"/Scio-REPL.html#multi-line-code","text":"While in the REPL, use :paste magic command to paste or write multi-line code\n:paste\n// Entering paste mode (ctrl-D to finish)\nimport com.spotify.scio.io.ClosedTap\nimport com.spotify.scio.values.SCollection\n\ndef evenNumber(x: Int): Boolean = x % 2 == 0\ndef evenNumbers: SCollection[Int] = sc.parallelize(1 to 100).filter(evenNumber)\n\n// Exiting paste mode, now interpreting.\n\ndef tap: ClosedTap[String] = evenNumbers.saveAsTextFile(\"/tmp/even\")\n\ndef result = sc.run()","title":"Multi-line code"},{"location":"/Scio-REPL.html#running-jobs-asynchronously","text":"When using REPL and Dataflow service consider using the non-blocking `DataflowRunner` for a more interactive experience. To start:\njava -jar scio-repl-0.7.0.jar \\\n> --project=<project-id> \\\n> --stagingLocation=<stagin-dir> \\\n> --runner=DataflowRunner\nWelcome to\n                 _____\n    ________________(_)_____\n    __  ___/  ___/_  /_  __ \\\n    _(__  )/ /__ _  / / /_/ /\n    /____/ \\___/ /_/  \\____/   version 0.7.0\n\nUsing Scala version 2.12.8 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_144)\nType in expressions to have them evaluated.\nType :help for more information.\n\nBigQuery client available as 'bq'\nScio context available as 'sc'\nimport com.spotify.scio.io.ClosedTap\n\ndef closedTap: ClosedTap[String] = sc\n    .parallelize(1 to 100)\n    .map( _.toString )\n    .saveAsTextFile(\"gs://<output>\")\n\ndef result = sc.run()\n// [main] INFO org.apache.beam.runners.dataflow.DataflowRunner - Executing pipeline on the Dataflow Service, which will have billing implications related to Google Compute Engine usage and other Google Cloud Services.\n// [main] INFO org.apache.beam.runners.dataflow.util.PackageUtil - Uploading 3 files from PipelineOptions.filesToStage to staging location to prepare for execution.\n// [main] INFO org.apache.beam.runners.dataflow.util.PackageUtil - Uploading PipelineOptions.filesToStage complete: 2 files newly uploaded, 1 files cached\n// Dataflow SDK version: 2.9.0\n\ndef state = result.state\nNote that now sc.run() doesn’t block and wait until job completes and gives back control of the REPL right away. Use `ScioExecutionContext` to check for progress, results and orchestrate jobs.","title":"Running jobs asynchronously"},{"location":"/Scio-REPL.html#multiple-scio-contexts","text":"You can use multiple Scio context objects to work with several pipelines at the same time, simply use magic :newScio <context name>, for example:\nscio> :newScio c1\nScio context available as 'c1'\nscio> :newScio c2\nScio context available as 'c2'\nscio> :newLocalScio lc\nScio context available as 'lc'\nYou can use those in combination with DataflowRunner to run multiple pipelines in the same session or wire them with for comprehension over futures.","title":"Multiple Scio contexts"},{"location":"/Scio-REPL.html#bigquery-client","text":"Whenever possible leverage BigQuery! `@BigQueryType` annotations enable type safe and civilized integration with BigQuery inside Scio. Here is example of using the annotations and BigQuery client to read and write typed data directly without Scio context.\n$ java -jar -Dbigquery.project=<project-id> scio-repl-0.7.0.jar\nWelcome to\n                 _____\n    ________________(_)_____\n    __  ___/  ___/_  /_  __ \\\n    _(__  )/ /__ _  / / /_/ /\n    /____/ \\___/ /_/  \\____/   version 0.7.0\n\nUsing Scala version 2.12.8 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_144)\nType in expressions to have them evaluated.\nType :help for more information.\n\nBigQuery client available as 'bq'\nScio context available as 'sc'\n@BigQueryType.fromQuery(\"SELECT tornado, month FROM [clouddataflow-readonly:samples.weather_stations]\") class Row\n\ndef tornadoes = bq.getTypedRows[Row]()\n\ndef result = tornadoes.next.month\n\ndef write = bq.writeTypedRows(\"project-id:dataset-id.table-id\", tornadoes.take(100).toList)","title":"BigQuery client"},{"location":"/Scio-REPL.html#out-of-memory-exception","text":"In case of OOM exceptions, like for example:\ndef closedTap: ClosedTap[String] = ???\n\ndef result = sc.run().waitUntilDone().tap(closedTap).value.next\nsimply increase the size of the heap - be reasonable about the amount of data and heap size though.\nExample of REPL startup with 2GiB of heap size:\n$ java -Xmx2g -jar scio-repl-0.7.0.jar\nWelcome to\n                 _____\n    ________________(_)_____\n    __  ___/  ___/_  /_  __ \\\n    _(__  )/ /__ _  / / /_/ /\n    /____/ \\___/ /_/  \\____/   version 0.7.0\n\nUsing Scala version 2.12.8 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_144)\n\nType in expressions to have them evaluated.\nType :help for more information.\n\nBigQuery client available as 'bq'\nScio context available as 'sc'\nRuntime.getRuntime().maxMemory()\n// res1: Long = 1908932608","title":"Out of memory exception"},{"location":"/Scio-REPL.html#what-is-the-type-of-an-expression-","text":"Use the built in :t! :t displays the type of an expression without evaluating it. Example:\nscio> :t sc.textFile(\"README\").flatMap(_.split(\"[^a-zA-Z']+\")).filter(_.nonEmpty).map(_.length)\ncom.spotify.scio.values.SCollection[Int]\nLearn more about magic keywords via scio> :help","title":"What is the type of an expression?"},{"location":"/internals/index.html","text":"","title":"Internals"},{"location":"/internals/index.html#internals","text":"Coder Typeclass Kryo OverrideTypeProvider ScioIO","title":"Internals"},{"location":"/internals/Coders.html","text":"","title":"Coder Typeclass"},{"location":"/internals/Coders.html#coder-typeclass","text":"Starting from Scio 0.7.0,","title":"Coder Typeclass"},{"location":"/internals/Coders.html#coder-in-apache-beam","text":"As per Beam’s documentation\nWhen Beam runners execute your pipeline, they often need to materialize the intermediate data in your PCollections, which requires converting elements to and from byte strings. The Beam SDKs use objects called Coders to describe how the elements of a given PCollection may be encoded and decoded.\nSo anytime you create a SCollection[T], Beam needs to know how to go from an instance of T to an array of bytes, and from that array of bytes to an instance of T.\nThe Beam SDK has a class called Coder that roughly looks like this:\npublic abstract class Coder<T> implements Serializable {\n  public abstract void encode(T value, OutputStream outStream);\n  public abstract T decode(InputStream inStream);\n}\nBeam provides built-in Coders for various basic Java types (Integer, Long, Double, etc.). But anytime you create a new class, and that class is used in a SCollection, a beam coder needs to be provided.\nimport com.spotify.scio.values.SCollection\n\ncase class Foo(x: Int, s: String)\ndef sc: SCollection[Foo] = ??? // Beam will need an org.apache.beam.sdk.coders.Coder[Foo]","title":"Coder in Apache Beam"},{"location":"/internals/Coders.html#scio-0-6-x-and-below","text":"In Scio 0.6.x and below, Scio would delegate this serialization process to Kryo. Kryo’s job is to automagically “generate” the serialization logic for any type. The benefit is you don’t really have to care about serialization most of the time when writing pipelines with Scio. Using Beam, you would need to explicitly set the coder every time you use a PTtransform.\nWhile it saves a lot of work, it also has a few drawbacks:\nKryo coders can be really inefficient. Especially if you forget to register your classes using a custom KryoRegistrar. The only way to be sure Kryo coders are correctly registered is to write tests and run them with a specific option: (see kryoRegistrationRequired=true). Kryo coders are very dynamic and it can be hard to know exactly which coder is used for a given class. Kryo coders do not always play well with Beam, and sometime can cause weird runtime exceptions. For example, Beam may sometimes throw an IllegalMutationException because of the default Kryo coder implementation.","title":"Scio 0.6.x and below"},{"location":"/internals/Coders.html#scio-0-7-0-and-above","text":"In Scio 0.7.0 and above, the Scala compiler will try to find the correct instance of Coder at compile time. In most cases, the compiler should be able to either directly find a proper Coder implementation, or derive one automatically.\nPlease note that Scio wraps Beam coders in its own Coder definition: com.spotify.scio.coders.Coder","title":"Scio 0.7.0 and above"},{"location":"/internals/Coders.html#built-in-coder-instances","text":"Here’s an example REPL session that demonstrate it:\nimport com.spotify.scio.coders._\nCoder[Int] // Try to find a Coder instance for Int\n// res0: Coder[Int] = Beam(VarIntCoder)\nHere the compiler just found a proper Coder for integers.\nScio also provides Coders for commons collections types:\nCoder[List[String]] // Try to find a Coder instance for List[String]\n// res1: Coder[List[String]] = Transform(Beam(StringUtf8Coder), <function1>)","title":"Built-in Coder instances"},{"location":"/internals/Coders.html#automatically-derived-coder-instances","text":"If you define a case class, the compiler can automatically derive a Coder for that class\ncase class Demo(i: Int, s: String, xs: List[Double])\nCoder[Demo]\n// res2: Coder[Demo] = Ref(repl.Session.App.Demo)\nsealed class hierarchy are also supported:\nsealed trait Top\nfinal case class TA(anInt: Int, aString: String) extends Top\nfinal case class TB(anDouble: Double) extends Top\n\nCoder[Top]\n// res3: Coder[Top] = Disjunction(\n//   \"repl.Session.App.Top\",\n//   Beam(BooleanCoder),\n//   <function1>,\n//   Map(false -> Ref(repl.Session.App.TA), true -> Ref(repl.Session.App.TB))\n// )","title":"Automatically derived Coder instances"},{"location":"/internals/Coders.html#coder-fallbacks","text":"Sometimes, no Coder instance can be found, and it’s impossible to automatically derive one. In that case, Scio will fallback to a Kryo coder for that specific type, and if the scalac flag -Xmacro-settings:show-coder-fallback=true is set, a warning message will be displayed at compile time. This message should help you fix the warning.\nWhile compiling the following with -Xmacro-settings:show-coder-fallback=true\nimport com.spotify.scio.coders._\nval localeCoder = Coder[java.util.Locale]\n// localeCoder: Coder[java.util.Locale] = Fallback(java.util.Locale)\nScalac will output:\nWarning: No implicit Coder found for the following type:\n\n   >> java.util.Locale\n\n using Kryo fallback instead.\n\n\n  Scio will use a fallback Kryo coder instead.\n\n  If a type is not supported, consider implementing your own implicit Coder for this type.\n  It is recommended to declare this Coder in your class companion object:\n\n       object Locale {\n         import com.spotify.scio.coders.Coder\n         import org.apache.beam.sdk.coders.AtomicCoder\n\n         implicit def coderLocale: Coder[Locale] =\n           Coder.beam(new AtomicCoder[Locale] {\n             def decode(in: InputStream): Locale = ???\n             def encode(ts: Locale, out: OutputStream): Unit = ???\n           })\n       }\n\n  If you do want to use a Kryo coder, be explicit about it:\n\n       implicit def coderLocale: Coder[Locale] = Coder.kryo[Locale]\n\n  Additional info at:\n   - https://spotify.github.io/scio/internals/Coders\nHere for example, the compiler could not find a proper instance of Coder[Locale], and suggest you implement one yourself.\nNote that this message is not limited to direct invocation of fallback. For example, if you declare a case class that uses Locale internally, the compiler will show the same warning:\nimport com.spotify.scio.coders._\ncase class Demo2(i: Int, s: String, xs: List[java.util.Locale])\nval demoCoder = Coder[Demo2]\n// demoCoder: Coder[Demo2] = Ref(repl.Session.App5.Demo2)\nScio will still use a “proper” Coder for Int, String and List. Only the serialization of Locale instances is delegated to Kryo.","title":"Coder fallbacks"},{"location":"/internals/Coders.html#upgrading-to-v0-7-0-or-above-migrating-to-static-coder","text":"Migrating to Scio 0.7.x from an older version is likely to break a few things at compile time in your project. See the complete v0.7.0 Migration Guide for more information.","title":"Upgrading to v0.7.0 or above: Migrating to static coder"},{"location":"/internals/Kryo.html","text":"","title":"Kryo"},{"location":"/internals/Kryo.html#kryo","text":"Scio uses a framework called Kryo to serialize objects that need to be shuffled between workers. Network throughput can easily become a bottleneck for your pipeline, so optimizing serialization is an easy win. If you use Dataflow’s shuffler service, you pay per GB shuffled so you can save money even if shuffling is not a bottleneck.\nBy registering your classes at compile time, Kryo can serialize far more efficiently that doing it on the fly. The default serializer includes the full classpath of each class you serialize. By pre-registering the classes you want to serialize, Kryo can replace this with an int as identifier. For some pipelines we observed a 30-40% reduction of bytes shuffled.","title":"Kryo"},{"location":"/internals/Kryo.html#how-to-enable-kryoregistrar","text":"Add the following class. You can rename it, but its name has to end in KryoRegistrar. Also make sure that the Macro Paradise plugin is enabled for your project.\nimport com.spotify.scio.coders.KryoRegistrar\nimport com.twitter.chill._\n\n@KryoRegistrar\nclass MyKryoRegistrar extends IKryoRegistrar {\n  override def apply(k: Kryo): Unit = {\n    // Take care of common Scala classes; tuples, Enumerations, ...\n    val reg = new AllScalaRegistrar\n    reg(k)\n\n    k.registerClasses(List(\n      // All classes that might be shuffled, e.g.:\n      classOf[foo.bar.MyClass],\n\n      // Class that takes type parameters:\n      classOf[_root_.java.util.ArrayList[_]],\n      // But you can also explicitly do:\n      classOf[Array[Byte]],\n\n      // Private class; cannot use classOf:\n      Class.forName(\"com.spotify.scio.extra.sparkey.LocalSparkeyUri\"),\n\n      // Some common Scala objects\n      None.getClass,\n      Nil.getClass\n    ))\n  }\n}\nNote: since Dataflow may shuffle data at any point, you not only have to include classes that are explicitly shuffled (through join or groupBy), but also those returned by map, flatMap, etc.","title":"How to enable KryoRegistrar"},{"location":"/internals/Kryo.html#verifying-it-works","text":"You can add the following class to your test folder; it will enforce registration of classes during your tests. It only works if you actually run your job in tests, so be sure to include a JobTest or so for each pipeline you run.\nimport com.esotericsoftware.kryo.Kryo\nimport com.spotify.scio.coders.KryoRegistrar\nimport com.twitter.chill.IKryoRegistrar\n\n/** Makes sure we don't forget to register encoders, enabled only in tests not to crash production. */\n@KryoRegistrar\nclass TestKryoRegistrar extends IKryoRegistrar {\n  def apply(k: Kryo): Unit = {\n    k.setRegistrationRequired(true)\n  }\n}\nIf you missed registering any classes, you’ll get an error that looks like this:\n[info]   java.lang.IllegalArgumentException: Class is not registered: org.apache.avro.generic.GenericData$Record\n[info] Note: To register this class use: kryo.register(org.apache.avro.generic.GenericData$Record.class);\nWhich you solve by adding classOf[GenericData.Record] or Class.forName(\"org.apache.avro.generic.GenericData$Record\") in MyKryoRegistrar.","title":"Verifying it works"},{"location":"/internals/OverrideTypeProvider.html","text":"","title":"OverrideTypeProvider"},{"location":"/internals/OverrideTypeProvider.html#overridetypeprovider","text":"The `OverrideTypeProvider` trait allows the user to provide custom mappings from BigQuery types to custom Scala types.\nThis can be used for a number of use cases: * Using higher level types in Scio in order to be explicit about what your data is * Custom code can be run when you create new objects to do things like data validation or simple transformation\nThe methods in the Scala trait allow you to inspect the incoming types from BigQuery and decide if you’d like to provide an alternative type mapping to your own custom type. You also must tell Scio how to convert your types back into BigQuery data types.","title":"OverrideTypeProvider"},{"location":"/internals/OverrideTypeProvider.html#setup","text":"Once you implement the OverrideTypeProvider with your own custom types you can supply it to the OverrideTypeProviderFinder by specifying a JVM System property as below.\nSystem.setProperty(\n  \"override.type.provider\",\n  \"com.spotify.scio.bigquery.validation.SampleOverrideTypeProvider\")\nSince this feature uses Scala macros you must do this at initialization time. One easy way to do this is in the build.sbt file for your project. This would look like below.\ninitialize in Test ~= { _ => System.setProperty(\n  \"override.type.provider\",\n  \"com.spotify.scio.bigquery.validation.SampleOverrideTypeProvider\")\n}\nCurrently only one OverrideTypeProvider is allowed per sbt project.\nThis provider is loaded using Reflection at macro expansion time and at runtime as well.\nIf this System property isn’t specified then Scio falls back to the normal default behavior.","title":"Setup"},{"location":"/internals/OverrideTypeProvider.html#implementation","text":"Custom implementations of the OverrideTypeProvider should implement the methods as described below.\ndef shouldOverrideType(tfs: TableFieldSchema)\nThis is the first point of entry and is called when we use macros to create case classes for fromQuery, fromSchema, and fromTable.\ndef getScalaType(c: blackbox.Context)(tfs: TableFieldSchema)\nThis is called when the above shouldOverrideType returns true. Expected return value is a c.Tree representing the Scala type you’d like to use for this mapping.\ndef shouldOverrideType(c: blackbox.Context)(tpe: c.Type)\nThis is called when we do conversions to and from a TableRow internally and your generated case class.\ndef createInstance(c: blackbox.Context)(tpe: c.Type, tree: c.Tree)\nThis is called when the above shouldOverrideType returns true. Expected return value is a c.Tree representing how to create a new instance of your custom Scala type.\ndef shouldOverrideType(tpe: Type)\nThis is called at runtime when we do any operations on the schema directly.\ndef getBigQueryType(tpe: Type)\nThis is called when the above shouldOverrideType returns true. It should return the String representation for the BigQuery column type for your class and field now.\ndef initializeToTable(c: blackbox.Context)(modifiers: c.universe.Modifiers,\n                                           variableName: c.universe.TermName,\n                                           tpe: c.universe.Tree)\nThis is called once per field when we extend the case classes for toTable examples.","title":"Implementation"},{"location":"/internals/ScioIO.html","text":"","title":"ScioIO"},{"location":"/internals/ScioIO.html#scioio","text":"Scio 0.7.0 introduces a new `ScioIO[T]` trait to simplify IO implementation and stubbing in JobTest. This page lists some major changes to this new API.","title":"ScioIO"},{"location":"/internals/ScioIO.html#dependencies","text":"Avro and BigQuery logic was decoupled from scio-core as part of the refactor.\nBefore 0.7.0 scio-core depends on scio-avro and scio-bigquery ScioContext and SCollection[T] include Avro, object, Protobuf and BigQuery IO methods out of the box After 0.7.0 scio-core no longer depends on scio-avro and scio-bigquery Import com.spotify.scio.avro._ to get Avro, object, Protobuf IO methods on ScioContext and SCollection[T] Import com.spotify.scio.bigquery._ to get BigQuery IO methods on ScioContext and SCollection[T]","title":"Dependencies"},{"location":"/internals/ScioIO.html#scioio-t-for-jobtest","text":"As part of the refactor TestIO[T] was replaced by ScioIO[T] for JobTest. Some of them were moved to different packages for consistency but most test code should work with minor import changes. Below is a list of ScioIO[T] implementations.\ncom.spotify.scio.avro AvroIO[T] ObjectFileIO[T] ProtobufIO[T] com.spotify.scio.bigquery BigQueryIO[T] TableRowJsonIO where T =:= TableRow com.spotify.scio.io DatastoreIO where T =:= Entity PubsubIO[T] TextIO where T =:= String CustomIO[T] for use with ScioContext#customInput and SCollection#customOutput com.spotify.scio.bigtable BigtableIO[T] where T =:= Row for input and T =:= Mutation for output This replaces BigtableInput and BigtableOutput com.spotify.scio.cassandra CassandraIO[T] com.spotify.scio.elasticsearch ElasticsearchIO[T] com.spotify.scio.extra.json JsonIO[T] com.spotify.scio.jdbc JdbcIO[T] com.spotify.scio.parquet.avro ParquetAvroIO[T] com.spotify.scio.spanner SpannerIO[T] com.spotify.scio.tensorflow TFRecordIO where T =:= Array[Byte] TFExampleIO where T =:= Example","title":"ScioIO[T] for JobTest"},{"location":"/internals/ScioIO.html#using-scioio-t-directly","text":"2 methods, ScioContext#read and SCollection#write were added to leverage ScioIO[T] directly without needing the extra ScioContext#{textFile,AvroFile,...} and SCollection#saveAs{TextFile,AvroFile,...} syntactic sugar. See WordCountScioIO and WordCountScioIOTest for concrete examples.","title":"Using ScioIO[T] directly"},{"location":"/extras/index.html","text":"","title":"Extras"},{"location":"/extras/index.html#extras","text":"Algebird Sort Merge Bucket","title":"Extras"},{"location":"/extras/Algebird.html","text":"","title":"Algebird"},{"location":"/extras/Algebird.html#algebird","text":"Algebird is Twitter’s abstract algebra library. It has a lot of reusable modules for parallel aggregation and approximation. One can use any Algebird Aggregator or Semigroup with: - aggregate and sum on `SCollection[T]` - aggregateByKey and sumByKey on `SCollection[(K, V)]`\nSee AlgebirdSpec.scala and Algebird wiki for more details. Also see these slides on semigroups.","title":"Algebird"},{"location":"/extras/Algebird.html#algebird-in-repl","text":"scio> import com.twitter.algebird._\nscio> import com.twitter.algebird.CMSHasherImplicits._\nscio> val words = sc.textFile(\"README.md\").\n     | flatMap(_.split(\"[^a-zA-Z0-9]+\")).\n     | filter(_.nonEmpty).\n     | aggregate(CMS.aggregator[String](0.001, 1E-10, 1)).\n     | materialize\nscio> sc.run()\nscio> val cms = words.waitForResult().value.next\nscio> cms.frequency(\"scio\").estimate\nres2: Long = 19\n\nscio> // let's validate:\nscio> import sys.process._\nscio> \"grep -o scio README.md\"  #| \"wc -l\"!\n      19","title":"Algebird in REPL"},{"location":"/extras/Sort-Merge-Bucket.html","text":"","title":"Sort Merge Bucket"},{"location":"/extras/Sort-Merge-Bucket.html#sort-merge-bucket","text":"Sort Merge Bucket is a technique for writing data to file system in deterministic file locations, sorted according by some pre-determined key, so that it can later be read in as key groups with no shuffle required. Since each element is assigned a file destination (bucket) based on a hash of its join key, we can use the same technique to cogroup multiple Sources as long as they’re written using the same key and hashing scheme.\nFor example, given these input records, and SMB write will first extract the key, assign the record to a bucket, sort values within the bucket, and write these values to a corresponding file.\nInput Key Bucket File Assignment {key:“b”, value: 1} “b” 0 bucket-00000-of-00001.avro {key:“b”, value: 2} “b” 0 bucket-00000-of-00001.avro {key:“a”, value: 3} “a” 1 bucket-00001-of-00001.avro\nTwo sources can be joined by opening file readers on corresponding buckets of eachT source and merging key-groups as we go.","title":"Sort Merge Bucket"},{"location":"/extras/Sort-Merge-Bucket.html#what-are-smb-transforms-","text":"scio-smb provides three PTransforms, as well as corresponding Scala API bindings, for SMB operations:\nSortedBucketSink writes data to file system in SMB format. Scala APIs (see: SortedBucketSCollection): SCollection[T: Coder]#saveAsSortedBucket sc.parallelize(250 until 750)\n  .map { i =>\n    Account\n      .newBuilder()\n      .setId(i)\n      .setName(s\"user$i\")\n      .setType(s\"type${i % 5}\")\n      .setAmount(Random.nextDouble() * 1000)\n      .build()\n  }\n  .saveAsSortedBucket(\n    AvroSortedBucketIO\n      .write[Integer, Account](classOf[Integer], \"id\", classOf[Account])\n      .to(args(\"accountOutput\"))\n      .withSorterMemoryMb(128)\n      .withTempDirectory(sc.options.getTempLocation)\n      .withCodec(CodecFactory.snappyCodec())\n      .withHashType(HashType.MURMUR3_32)\n      .withNumBuckets(1)\n      .withNumShards(1)\n  ) Note the use of Integer for parameterized key type instead of a Scala Int. The key class must have a Coder available in the default Beam (Java) coder registry. Also note that the number of buckets specified must be a power of 2. This allows sources of different bucket sizes to still be joinable. SortedBucketSource reads data that has been written to file system using SortedBucketSink into a collection of CoGbkResults. Scala APIs (see: SortedBucketScioContext): ScioContext#sortMergeGroupByKey (1 source) ScioContext#sortMergeJoin (2 sources) ScioContext#sortMergeCoGroup (1-4 sources) sc.sortMergeJoin(\n    classOf[Integer],\n    AvroSortedBucketIO\n      .read(new TupleTag[GenericRecord](\"lhs\"), SortMergeBucketExample.UserDataSchema)\n      .from(args(\"lhsInput\")),\n    AvroSortedBucketIO\n      .read(new TupleTag[Account](\"rhs\"), classOf[Account])\n      .from(args(\"rhsInput\"))\n  )\n  .map(mapFn) // Apply mapping function\n  .saveAsTextFile(args(\"output\")) SortedBucketTransform reads data that has been written to file system using SortedBucketSink, transforms each CoGbkResult using a user-supplied function, and immediately rewrites them using the same bucketing scheme. Scala APIs (see: SortedBucketScioContext): ScioContext#sortMergeTransform (1-3 sources) sc.sortMergeTransform(\n    classOf[Integer],\n    AvroSortedBucketIO\n      .read(new TupleTag[GenericRecord](\"lhs\"), SortMergeBucketExample.UserDataSchema)\n      .from(args(\"lhsInput\")),\n    AvroSortedBucketIO\n      .read(new TupleTag[Account](\"rhs\"), classOf[Account])\n      .from(args(\"rhsInput\"))\n  )\n  .to(\n    AvroSortedBucketIO\n      .write(classOf[Integer], \"userId\", classOf[Account])\n      .to(args(\"output\"))\n      .withNumBuckets(2)\n      .withNumShards(1)\n      .withHashType(HashType.MURMUR3_32)\n  )\n  .via {\n    case (key, (users, accounts), outputCollector) =>\n      users.foreach { user =>\n        outputCollector.accept(\n          Account\n            .newBuilder()\n            .setId(key)\n            .setName(user.get(\"userId\").toString)\n            .setType(\"combinedAmount\")\n            .setAmount(accounts.foldLeft(0.0)(_ + _.getAmount))\n            .build()\n        )\n      }\n  }","title":"What are SMB transforms?"},{"location":"/extras/Sort-Merge-Bucket.html#what-kind-of-data-can-i-write-using-smb-","text":"SMB writes are supported for Avro (GenericRecord and SpecificRecord), JSON, and Tensorflow records. See API bindings in:\nAvroSortedBucketIO JsonSortedBucketIO TensorFlowBucketIO","title":"What kind of data can I write using SMB?"},{"location":"/extras/Sort-Merge-Bucket.html#tuning-parameters-for-smb-transforms","text":"SMB reads should be more performant and less resource-intensive than regular joins or groupBys. However, SMB writes are more expensive than their regular counterparts, since they involve an extra group-by (bucketing) and sorting step.\nAdditionally, non-SMB writes (i.e. implementations of FileBasedSink) use hints from the runner to determine an optimal number of output files. With SMB, you must specify the number of buckets and shards (numBuckets * numShards = total # of files) up front.\nA good starting point is to look at your output data as it has been written by a non-SMB sink, and pick the closest power of 2 as your initial numBuckets, and set numShards to 1. If you anticipate having hot keys, try increasing numShards to randomly split data within a bucket. If your job gets stuck in the sorting phase (since the GroupByKey and SortValues transforms may get fused–you can reference the Counters SortedBucketSink-bucketsInitiatedSorting and SortedBucketSink-bucketsCompletedSorting to get an idea of where your job fails), you can increase sorter memory (default is 128MB):\ndata.saveAsSortedBucket(\n  AvroSortedBucketIO\n    .write[K, V](classOf[K], \"keyField\", classOf[V])\n    .to(...)\n    .withSorterMemoryMb(256)\n)\nYou can also tweak the --workerMachineType pipeline option see: machine specs for Google Cloud Dataflow–although even the smallest machine type has several GB of RAM.","title":"Tuning parameters for SMB transforms"},{"location":"/extras/Sort-Merge-Bucket.html#testing","text":"Currently, mocking data for SMB transforms is not supported in the com.spotify.scio.testing.JobTest framework. See SortMergeBucketExampleTest for an example of using local temp directories to test SMB reads and writes.","title":"Testing"},{"location":"/migrations/index.html","text":"","title":"Migration Guides"},{"location":"/migrations/index.html#migration-guides","text":"Scio v0.7.0 Scio v0.8.0 Scio v0.9.0","title":"Migration Guides"},{"location":"/migrations/v0.7.0-Migration-Guide.html","text":"","title":"Scio v0.7.0"},{"location":"/migrations/v0.7.0-Migration-Guide.html#scio-v0-7-0","text":"Scio 0.7.0 comes with major improvements over previous versions. The overall goal is to make Scio safer, faster, more consistent and easier to extend by leveraging Scala’s type system more and refactoring its internals.\nThe new milestone has been profiled more than ever and will improve the performances of your pipeline. In some cases, the improvement can be very significant.\nScio 0.7.0 also includes, like every release, a number of bugfixes.","title":"Scio v0.7.0"},{"location":"/migrations/v0.7.0-Migration-Guide.html#whats-new-","text":"","title":"What’s new ?"},{"location":"/migrations/v0.7.0-Migration-Guide.html#new-ios-api","text":"In this version, we’ve refactored the implementation of IOs. Scio now provides a new class ScioIO that you can extend to support new types of IOs. ScioContext now has a new method called read and SCollection now has a new method write. Both take an instance of a class extending ScioIO as a parameter and may read from any source, or write to any target.\nAll existing IOs (GCS, BigQuery, BigTable, etc.) have been rewritten to use the new IO API.\nRead more: ScioIO","title":"New IOs api"},{"location":"/migrations/v0.7.0-Migration-Guide.html#new-coders","text":"Scio 0.7.0 also ship with a new Coder implementation that statically resolve the correct Coder for a given type at compile time. In previous versions, Scio would infer the correct coder implementation at runtime, which could lead to poor performances and occasionally, exceptions at runtime.\nRead more: Coders.","title":"New “static” coders"},{"location":"/migrations/v0.7.0-Migration-Guide.html#performances-improvements-benchmarks","text":"Thanks to the new static coders implementation, and because of the time we spend on profiling, Scio 0.7.0 should overall be more efficient than previous versions.","title":"Performances improvements & benchmarks"},{"location":"/migrations/v0.7.0-Migration-Guide.html#automated-migration","text":"Scio 0.7 comes with a set of scalafix rules that will do most of the heavy lifting automatically. Before you go through any manual step, we recommend you start by applying those rules.\nStart by adding the scalafix sbt plugin to your project/plugins.sbt file\naddSbtPlugin(\"ch.epfl.scala\" % \"sbt-scalafix\" % \"0.9.0\")\nlaunch sbt and run scalafixEnable\n> scalafixEnable\n[info] Set current project to my-amazing-scio-pipeline (in build file:/Users/julient/Documents/my-amazing-scio-pipeline/)","title":"Automated migration"},{"location":"/migrations/v0.7.0-Migration-Guide.html#prepare-your-tests","text":"Warning RUN THIS BEFORE UPGRADING SCIO\nYou’ll need to prepare your tests code for migration. For this to run properly, you code needs to compile.\nRun the following command in the sbt shell:\n> test:scalafix github:spotify/scio/FixAvroIO\n[info] Running scalafix on 78 Scala sources\n[success] Total time: 7 s, completed Oct 17, 2018 12:49:31 PM\nOnce FixAvroIO has been applied, you can go ahead and upgrade Scio to 0.7.x in your build file. After you have set Scio’s version in your build.sbt, make sure to either restart or reload sbt.\nYou can now run the automated migration rules. At the moment, we support 4 rules:\nName Description AddMissingImports Add the required imports to access sources / sinks on ScioContext and SCollection RewriteSysProp Replace sys.call(...) by the new syntax FixAvroIO Fix uses of AvroIO in tests BQClientRefactoring Automatically migrate from BigQueryClient to the new BigQuery client\nYou can see all the rules here.\nIn your sbt shell, you can now apply the 3 other rules:\n> scalafix github:spotify/scio/AddMissingImports\n[info] Running scalafix on 173 Scala sources\n[success] Total time: 16 s, completed Oct 17, 2018 12:01:31 PM\n\n> scalafix github:spotify/scio/RewriteSysProp\n[info] Running scalafix on 173 Scala sources\n[success] Total time: 6 s, completed Oct 17, 2018 12:34:00 PM\n\n> scalafix github:spotify/scio/BQClientRefactoring\n[info] Running scalafix on 173 Scala sources\n[success] Total time: 3 s, completed Oct 17, 2018 12:34:20 PM\nAt that point you can try to compile your code and fix the few compilation errors left. The next sections of this guide should contain all the information you need to fix everything.","title":"Prepare your tests"},{"location":"/migrations/v0.7.0-Migration-Guide.html#migration-guide","text":"The following section will detail errors you may encounter while migrating from scio 0.6.x to Scio 0.7.x, and help you fix them. If you’ve run the automated migration fixes, you can jump directly to the Add missing context bounds section.","title":"Migration guide"},{"location":"/migrations/v0.7.0-Migration-Guide.html#method-xxx-is-not-a-member-of-com-spotify-scio-sciocontext","text":"When using read methods from ScioContext the compiler may issue an error of type method xxx is not a member of com.spotify.scio.ScioContext.\nIOs have been refactored in Scio 0.7.0. Each IO type now lives in the appropriate project and package. It means 2 things:","title":"method xxx is not a member of com.spotify.scio.ScioContext"},{"location":"/migrations/v0.7.0-Migration-Guide.html#you-may-need-to-explicitly-add-a-dependency-one-of-scios-subprojects-in-your-build-file","text":"For example, Scio used to pull dependencies on BigQuery IOs even if your pipeline did not use BigQuery at all. With the new IOs, Scio will limit its dependencies to packages you actually use.\nIf your pipeline is using BigQuery, you now need to add scio-bigquery as a dependency of your project:\nlibraryDependencies += \"com.spotify\" %% \"scio-bigquery\" % scioVersion","title":"You may need to explicitly add a dependency one of Scio’s subprojects in your build file"},{"location":"/migrations/v0.7.0-Migration-Guide.html#youll-need-to-import-the-appropriate-package-to-gain-access-to-the-io-methods-","text":"For example while migrating a job that reads data from an Avro file, you may see the following compiler error:\n[error] value avroFile is not a member of com.spotify.scio.ScioContext\n[error]     val coll = sc.avroFile[SomeType](uri)\n[error]                            ^\nAll you have to do to fix it is to import IOs from the correct package:\nimport com.spotify.scio.avro._","title":"You’ll need to import the appropriate package to gain access to the IO methods."},{"location":"/migrations/v0.7.0-Migration-Guide.html#avroio-or-other-type-of-io-not-found","text":"IOs have been moved out of the com.spotify.scio.testing package. To use them in unit tests (or elsewhere), you’ll need to change the import:\ncom.spotify.scio.testing.BigQueryIO -> com.spotify.scio.bigquery.BigQueryIO com.spotify.scio.testing.{AvroIO, ProtobufIO} -> com.spotify.scio.avro.{AvroIO, ProtobufIO} com.spotify.scio.testing.TextIO -> com.spotify.scio.io.TextIO\nA complete list of IO packages can be found here.\nAdditionally, some IOs are now parameterized. For example, AvroIO must now be parameterized with the Avro record type (either GenericRecord or an extension of SpecificRecordBase). In previous versions of Scio, it was possible in some cases to omit that type. For example:\nimport com.spotify.scio.io._\nimport com.spotify.scio.avro._\nimport com.spotify.scio.testing._\n\nclass MyScioJobTest extends PipelineSpec {\n\n  \"MyScioJob\" should \"work\" in {\n    JobTest[MyScioJob.type]\n      .args(s\"--inputAUri=${inputAUri}\")\n      .args(s\"--inputBUri=${inputBUri}\")\n      .input(AvroIO[GenericRecord](inputAUri), Seq(Schemas.record1))\n      .input(AvroIO[GenericRecord](inputBUri), Seq(Schemas.record2))\n      .output(TextIO(output)){ coll =>\n        coll should haveSize (1)\n        ()\n      }\n      .run()\n  }\n\n  // more tests\n}","title":"AvroIO (or other type of IO) not found"},{"location":"/migrations/v0.7.0-Migration-Guide.html#avro-type-inference-issue-","text":"If you use AvroIO you may see the compilation of your tests failing with an error looking like\n[error] <path>/SomeTest.scala:42:20: diverging implicit expansion for type com.spotify.scio.coders.Coder[K]\n[error] starting with macro method gen in trait LowPriorityCoderDerivation\n[error]       .input(AvroIO(inputUri), inputs)\n[error]\nThe problem is that line does not explicitly set the type of the IO:\n.input(AvroIO(inputUri), inputs)\nIn Scio <= 0.6.x this works, but in Scio 0.7.x, you’ll need to be explicit about the types. For example in that case:\n.input(AvroIO[GenericRecord](inputUri), inputs)","title":"Avro type inference issue: “diverging implicit expansion”"},{"location":"/migrations/v0.7.0-Migration-Guide.html#bigquery-client","text":"Client was renamed from BigQueryClient to BigQueryand relocated! Now you need to:\nimport com.spotify.scio.bigquery.client.BigQuery\nThe new client offers now methods namespaced according to their resposabilities.\nMethods typically fall into 4 categories of operations query, table, export and load. i.e:\n-client.extractLocation\n+client.query.extracLocation\n\n-client.getQuerySchema(...)\n+client.query.schema(...)\n\n-client.getTableRows(...)\n+client.table.rows(...)\n\n// the same thing applies for the other formats\n-client.loadTableFromCsv(...)\n+client.load.csv(...)\n\n// the same thing applies for the other formats\n-client.exportTableAsCsv(...)\n+client.export.asCsv(...)","title":"BigQuery client"},{"location":"/migrations/v0.7.0-Migration-Guide.html#not-enough-arguments-for-method-top-topbykey-approxquantilesbykey-implicit-ord-ordering-","text":"Explicit Ordering functions for SCollection reducers are no longer curried. Methods that used to look like:\n.top(10)(Ordering.by(...)\nshould be changed to:\n.top(10, Ordering.by(...))","title":"Not enough arguments for method (top|topByKey|approxQuantilesByKey): (implicit ord: Ordering[…])"},{"location":"/migrations/v0.7.0-Migration-Guide.html#add-missing-context-bounds","text":"In the process of upgrading Scio, you may encounter the following error:\nCannot find a Coder instance for type T\nIf you’ve defined a generic function that uses a SCollection, this function is likely to need a Coder[T]. Scio will require you to provide an implicit Coder[T]. You can read about Scala implicit parameters here\nLet’s see a simple example. Say I created the following method doSomething:\ndef doSomething[T](coll: SCollection[T]): SCollection[T] =\n  coll.map { t =>\n    // do something that returns a T\n    val result: T = ???\n    result\n  }\nIf I try to compile this method the compiler will return the following error:\nCannot find a Coder instance for type:\n\n  >> T\n\n  This can happen for a few reasons, but the most common case is that a data\n  member somewhere within this type doesn't have a Coder instance in scope. Here are\n  some debugging hints:\n    - For Option types, ensure that a Coder instance is in scope for the non-Option version.\n    - For List and Seq types, ensure that a Coder instance is in scope for a single element.\n    - You can check that an instance exists for Coder in the REPL or in your code:\n        scala> Coder[Foo]\n    And find the missing instance and construct it as needed.\n\n  coll.map { t =>\n           ^\nWhat this message says is that calling .map { ... } requires a Coder[T]. You can fix this very easily by adding a new implicit parameter to your method:\nimport com.spotify.scio.coders.Coder\nimport com.spotify.scio.values.SCollection\n\ndef doSomething[T](coll: SCollection[T])(implicit coder: Coder[T]): SCollection[T] =\n  coll.map { t =>\n    // do something that returns a T\n    val result: T = ???\n    result\n  }\nAlternatively, the same result can be achieved using Scala’s context bound syntax:\nimport com.spotify.scio.coders.Coder\nimport com.spotify.scio.values.SCollection\n\ndef doSomething[T: Coder](coll: SCollection[T]): SCollection[T] =\n  coll.map { t =>\n    // do something that returns a T\n    val result: T = ???\n    result\n  }","title":"Add missing context bounds"},{"location":"/migrations/v0.7.0-Migration-Guide.html#replacing-kryo-fallbacks-with-your-own-coders-","text":"Most of the time, the compiler will be able to find or derive an appropriate Coder automatically. Sometimes, it may not be able to find one automatically. This will typically happen for:\nClasses defined in Java Scala classes that are not case classes Classes with a private constructor\nIn those cases, Scio will fallback to using Kryo.","title":"Replacing Kryo fallbacks with your own coders."},{"location":"/migrations/v0.7.0-Migration-Guide.html#showing-all-fallback-at-compile-time","text":"The compiler can show a message each time a fallback is used. To activate that feature, just the the following scalac option: -Xmacro-settings:show-coder-fallback=true.\nYou can fix this warning in two ways:\nImplement a proper Coder for this type Make it explicit that the Kryo coder is in fact the one you want to use.\nIn both cases you want to define a Coder in your own code. The only difference is how you’ll implement it.\nLet’s say you are using a SCollection[java.util.Locale]:\nimport com.spotify.scio.values.SCollection\n\ndef doSomething(coll: SCollection[String]): SCollection[java.util.Locale] =\n  coll.map { t =>\n    // do something that returns a Locale\n    val result: java.util.Locale = ???\n    result\n  }","title":"Showing all fallback at compile time"},{"location":"/migrations/v0.7.0-Migration-Guide.html#using-kryo-explicitly","text":"If you want to explicitly use Kryo (which will probably be the case) you can do the following:\nimport java.util.Locale\nimport com.spotify.scio.coders.Coder\n\nobject Coders {\n  implicit val coderLocale: Coder[Locale] = Coder.kryo[Locale]\n}\nNow all you have to do is make that available at call site:\nimport com.spotify.scio.values.SCollection\nimport Coders._\n\ndef doSomething(coll: SCollection[String]): SCollection[java.util.Locale] =\n  coll.map { t =>\n    // do something that returns a Locale\n    val result: java.util.Locale = ???\n    result\n  }","title":"Using Kryo explicitly"},{"location":"/migrations/v0.7.0-Migration-Guide.html#defining-a-custom-coder","text":"If you want to implement custom coders, see Scio’s source code for examples.\nWarning Before implementing custom coders, we recommend that you test your pipeline with the default coders. Implementing custom coders can be tricky, so make sure there’s a clear benefit in doing it. If you implement custom Coders, you need to make sure they are Serializable.","title":"Defining a custom Coder"},{"location":"/migrations/v0.7.0-Migration-Guide.html#wartremover-compatibility-","text":"Automatically derived coders are generated by a macro. Unfortunately, if you use WartRemover in your project, the macro will trigger warnings. There’s not much we can do in the macro to fix the issue right now, so you’ll have to disable a few warts. Here are the warts you’ll need to disable in your project:\n- Any\n- IsInstanceOf\n- Throw\nIf you use sbt-wartremover, you can disable them in your build like this:\nwartremoverErrors in (Compile, compile) := {\n  val disableWarts = Set(\n    Wart.Any,\n    Wart.IsInstanceOf,\n    Wart.Throw\n  )\n  Warts.unsafe.filterNot(disableWarts.contains)\n},\n\nwartremoverErrors in (Test, compile) := {\n  val disableWarts = Set(\n    Wart.Any,\n    Wart.IsInstanceOf,\n    Wart.Throw\n  )\n  Warts.unsafe.filterNot(disableWarts.contains)\n}","title":"WartRemover compatibility."},{"location":"/migrations/v0.8.0-Migration-Guide.html","text":"","title":"Scio v0.8.0"},{"location":"/migrations/v0.8.0-Migration-Guide.html#scio-v0-8-0","text":"","title":"Scio v0.8.0"},{"location":"/migrations/v0.8.0-Migration-Guide.html#tl-dr","text":"BeamSQL integration BigQuery Storage api support Generic case class conversion Remove the usage of Future New BigQuery method signatures New async DoFns Remove tensorflow methods related to schema inference Remove support for lisp-case CLI arguments","title":"TL;DR"},{"location":"/migrations/v0.8.0-Migration-Guide.html#new-features","text":"","title":"New features"},{"location":"/migrations/v0.8.0-Migration-Guide.html#beamsql","text":"Beam SQL integration is added in this release! This integration comes in many flavors, from fluent api to string interpolation with both offering the possibility to typecheck the provided query at compile time.\nA simple use case of this api is reflected in the example below. This example uses the fluent api to query the SCollection[User] and extract username and age. `query` return’s Row which Beam’s underlying type that contains the values and the Schema of the extracted data.\nimport com.spotify.scio.sql._\nimport com.spotify.scio.values._\nimport org.apache.beam.sdk.values.Row\n\ncase class User(username: String, email: String, age: Int)\n\ndef users: SCollection[User] = ???\n\ndef result: SCollection[Row] = users.query(\"select username, age from SCOLLECTION\")\nIn the following example we go a little bit further. Using queryAs we can express the expected return type (String, Int) instead of Row. Query typecheck only happens at runtime\nIf the expected return type doesn’t the inferred schema for the given query this will fail at runtime.\nimport com.spotify.scio.sql._\nimport com.spotify.scio.values._\n\ncase class User(username: String, email: String, age: Int)\n\ndef users: SCollection[User] = ???\n\ndef result: SCollection[(String, Int)] = users.queryAs(\"select username, age from SCOLLECTION\")\nString interpolation is another way to express SQL queries. As we can see from the following example it behaves exactly as any other string interpolation with the added expression of the expected return type. As in the previous example the return type is not typechecked at compile time with the inferred query schema. Any mismatch will result in runtime exception.\nimport com.spotify.scio.sql._\nimport com.spotify.scio.values._\n\ncase class User(username: String, email: String, age: Int)\n\ndef users: SCollection[User] = ???\n\ndef result: SCollection[(String, Int)] = sql\"select username, age from $users\".as[(String, Int)]\nTypecheck at compile time is provided by tsql. Here’s the same example but with this new method:\nimport com.spotify.scio.sql._\nimport com.spotify.scio.values._\n\ncase class User(username: String, email: String, age: Int)\n\ndef users: SCollection[User] = ???\n\ndef result: SCollection[(String, Int)] = tsql\"select username, age from $users\".as[(String, Int)]","title":"BeamSQL"},{"location":"/migrations/v0.8.0-Migration-Guide.html#handling-sql-query-errors","text":"SQL query errors can happen! They might have syntax errors, wrong fields and even wrong expected types. To help you out, we have some gorgeous error messages for you!\nUsing the users collection from previous example, here are some error messages you might encounter.\nSelecting wrong field from users:\n- tsql\"select username, age from $users\".as[(String, Int)]\n+ tsql\"select username, foo from $users\".as[(String, Int)]\nSqlValidatorException: Column 'foo' not found in any table\n\nQuery:\nselect username, foo from SCOLLECTION\n\n\nschema of SCOLLECTION:\n┌──────────────────────────────────────────┬──────────────────────┬──────────┐\n│ NAME                                     │ TYPE                 │ NULLABLE │\n├──────────────────────────────────────────┼──────────────────────┼──────────┤\n│ username                                 │ STRING               │ NO       │\n│ email                                    │ STRING               │ NO       │\n│ age                                      │ INT32                │ NO       │\n└──────────────────────────────────────────┴──────────────────────┴──────────┘\n\nQuery result schema (inferred) is unknown.\nExpected schema:\n┌──────────────────────────────────────────┬──────────────────────┬──────────┐\n│ NAME                                     │ TYPE                 │ NULLABLE │\n├──────────────────────────────────────────┼──────────────────────┼──────────┤\n│ _1                                       │ STRING               │ NO       │\n│ _2                                       │ INT32                │ NO       │\n└──────────────────────────────────────────┴──────────────────────┴──────────┘\nProviding a SQL query with syntax error:\n- tsql\"select username, age from $users\".as[(String, Int)]\n+ tsql\"select username, age fom $users\".as[(String, Int)]\nParseException: Encountered \"users\" at line 1, column 27.\nWas expecting one of:\n    <EOF>\n    \"ORDER\" ...\n    \"LIMIT\" ...\n    \"OFFSET\" ...\n    \"FETCH\" ...\n    \"FROM\" ...\n    \",\" ...\n    \"UNION\" ...\n    \"INTERSECT\" ...\n    \"EXCEPT\" ...\n    \"MINUS\" ...\n\n\nQuery:\nselect username, age fom  users\n\n\nschema of users:\n┌──────────────────────────────────────────┬──────────────────────┬──────────┐\n│ NAME                                     │ TYPE                 │ NULLABLE │\n├──────────────────────────────────────────┼──────────────────────┼──────────┤\n│ username                                 │ STRING               │ NO       │\n│ email                                    │ STRING               │ NO       │\n│ age                                      │ INT32                │ NO       │\n└──────────────────────────────────────────┴──────────────────────┴──────────┘\n\nQuery result schema (inferred) is unknown.\nExpected schema:\n┌──────────────────────────────────────────┬──────────────────────┬──────────┐\n│ NAME                                     │ TYPE                 │ NULLABLE │\n├──────────────────────────────────────────┼──────────────────────┼──────────┤\n│ _1                                       │ STRING               │ NO       │\n│ _2                                       │ INT32                │ NO       │\n└──────────────────────────────────────────┴──────────────────────┴──────────┘\nReturn type different from the inferred one:\n- tsql\"select username, age from $users\".as[(String, Int)]\n+ tsql\"select username, age fom $users\".as[String]\nInfered schema for query is not compatible with the expected schema.\n\nQuery:\nselect username, age from  users\n\n\nschema of users:\n┌──────────────────────────────────────────┬──────────────────────┬──────────┐\n│ NAME                                     │ TYPE                 │ NULLABLE │\n├──────────────────────────────────────────┼──────────────────────┼──────────┤\n│ username                                 │ STRING               │ NO       │\n│ email                                    │ STRING               │ NO       │\n│ age                                      │ INT32                │ NO       │\n└──────────────────────────────────────────┴──────────────────────┴──────────┘\n\nQuery result schema (inferred):\n┌──────────────────────────────────────────┬──────────────────────┬──────────┐\n│ NAME                                     │ TYPE                 │ NULLABLE │\n├──────────────────────────────────────────┼──────────────────────┼──────────┤\n│ username                                 │ STRING               │ NO       │\n│ age                                      │ INT32                │ NO       │\n└──────────────────────────────────────────┴──────────────────────┴──────────┘\n\nExpected schema:\n┌──────────────────────────────────────────┬──────────────────────┬──────────┐\n│ NAME                                     │ TYPE                 │ NULLABLE │\n├──────────────────────────────────────────┼──────────────────────┼──────────┤\n│ value                                    │ STRING               │ NO       │\n└──────────────────────────────────────────┴──────────────────────┴──────────┘","title":"Handling SQL query Errors"},{"location":"/migrations/v0.8.0-Migration-Guide.html#bigquery-storage-api","text":"BigQuery Storage API provides fast access to BigQuery managed storage by using an rpc-based protocol.\nIf you already use BigQuery, the BigQuery Storage api that we provide will look very familiar as it provides the standard and the type safe api. Switching to this new strategy should be very straightforward.\nUsing the type safe API is almost the same as the previous provided strategies. We just need to use `@BigQueryType.fromStorage`. The example below retrieves all columns from a given table.\nimport com.spotify.scio.bigquery._\n\n@BigQueryType.fromStorage(\"data-integration-test:storage.nested\")\nclass Example\nHowever if you don’t want to pull everything, you can always specify which fields you want and even set some filtering.\nimport com.spotify.scio.bigquery._\n\n@BigQueryType.fromStorage(\n    \"data-integration-test:storage.%s\",\n    List(\"nested\"),\n    selectedFields = List(\"required\", \"optional.int\"),\n    rowRestriction = \"required.int < 5\"\n)\nclass Example\nUsing the above annotated classes can be done through the following methods.\nimport com.spotify.scio._\nimport com.spotify.scio.bigquery._\nimport com.spotify.scio.values._\n\ndef sc: ScioContext = ???\n\ndef below: SCollection[Example] = sc.typedBigQuery[Example]()\n\n// or\n\ndef above: SCollection[Example] = sc.typedBigQueryStorage[Example](rowRestriction = \"required.int > 5\")\nWhen not using the type safe api you can always read as:\nimport com.spotify.scio._\nimport com.spotify.scio.values._\nimport com.spotify.scio.bigquery._\n\ndef sc: ScioContext = ???\n\ndef result: SCollection[TableRow] = sc.bigQueryStorage(\n    Table.Spec(\"clouddataflow-readonly:samples.weather_stations\"),\n    selectedFields = List(\"tornado\", \"month\"),\n    rowRestriction = \"tornado = true\"\n  )","title":"BigQuery Storage API"},{"location":"/migrations/v0.8.0-Migration-Guide.html#generic-case-class-type-conversion","text":"With the introduction of automatic schema derivation for data types becomes really easy to convert between “compatible” generic case class’es.\nimport com.spotify.scio.values._\nimport com.spotify.scio.schemas._\n\ncase class FromExample(i: Int, s: String)\n\ncase class ToExample(s: String)\n\ndef examples: SCollection[FromExample] = ???\nTo convert FromExample => ToExample we can use two methods unsafe and safe. The main difference between them is when the conversion compatibility check happens. With unsafe it happens at runtime while with safe it’s at compile time.\ndef unsafe: SCollection[ToExample] = examples.to[ToExample](To.unsafe)\n\ndef safe: SCollection[ToExample] = examples.to[ToExample](To.safe)","title":"Generic case class type conversion"},{"location":"/migrations/v0.8.0-Migration-Guide.html#deprecations-and-breaking-changes","text":"","title":"Deprecations and Breaking changes"},{"location":"/migrations/v0.8.0-Migration-Guide.html#scala-concurrent-future-removed-from-scioios","text":"The removal of Future led to some semantic and behaviour changes.\nScioIOs no longer return Future\ntrait ScioIO[T] {\n  ....\n- protected def write(data: SCollection[T], params: WriteP): Future[Tap[tapT.T]]\n+ protected def write(data: SCollection[T], params: WriteP): Tap[tapT.T]\n  ...\n}\nSCollection#write returns ClosedTap[T]\nsealed trait SCollection[T] extends PCollectionWrapper[T] {\n    ...\n- def write(io: ScioIO[T])(params: io.WriteP)(implicit coder: Coder[T]): Future[Tap[io.tapT.T]]\n+ def write(io: ScioIO[T])(params: io.WriteP)(implicit coder: Coder[T]): ClosedTap[io.tapT.T]\n    ...\n}\n`ClosedTap[T]` encapsulates the IO `Tap[T]` and it’s only possible to read from it once the pipeline execution is done. This is demonstrated in the following example:\nimport com.spotify.scio._\nimport com.spotify.scio.io._\n\ndef sc: ScioContext = ???\ndef closedTap: ClosedTap[String] =\n     sc\n      .parallelize(1 to 100)\n      .sum\n      .map(_.toString)\n      .saveAsTextFile(\"...\")\n\ndef scioResult: ScioResult = sc.run().waitUntilDone()\n\n// open tap for read\ndef openedTap: Tap[String] = scioResult.tap(closedTap)","title":"scala.concurrent.Future removed from ScioIOs"},{"location":"/migrations/v0.8.0-Migration-Guide.html#sciocontext","text":"ScioContext#close changed it’s return type to ScioExecutionContext.\n- def close(): ScioResult\n+ def close(): ScioExecutionContext\nScioContext#close is also being deprecated in favor of ScioContext#run. With this change, --blocking flag is also deprecated.\nTo achieve the same behaviour with the new ScioContext#run you can do the following:\nimport com.spotify.scio._\nimport scala.concurrent.duration._\n\ndef scioResult(sc: ScioContext): ScioResult = sc.run().waitUntilDone(Duration.Inf)","title":"ScioContext"},{"location":"/migrations/v0.8.0-Migration-Guide.html#remove-tensorflow-methods-related-to-schema-inference","text":"In scio 0.7.0 scio-tensorflow saw some of it’s operations being deprecated. They are no longer available in this version and we recommend users to use TensorFlow Data Validation instead.\nRemoved operations:\nsaveExampleMetadata saveAsTfExampleFileWithSchema","title":"Remove tensorflow methods related to schema inference"},{"location":"/migrations/v0.8.0-Migration-Guide.html#bigquery","text":"If you are using BigQuery you will see some @deprecated warnings:\n// method bigQueryTable in class ScioContextOps is deprecated (since Scio 0.8):\n// this method will be removed; use bigQueryTable(Table.Spec(table)) instead\nAll BigQuery io operations now support a new data type Table to reference a specific table and every method that uses tableSpec: String or tableReference: TableReference has been deprecated.\n- def bigQueryTable(tableSpec: String): SCollection[TableRow]\n- def bigQueryTable(tableReference: TableReference): SCollection[TableRow]\n\n+ def bigQueryTable(table: Table): SCollection[TableRow]\nThe new Table type can be created according to the following examples:\ndef tableSpecString: String = ???\n\ndef table: Table = Table.Spec(tableSpecString)\nor:\ndef tableReference: TableReference = ???\n\ndef table: Table = Table.Ref(tableReference)\nThe advantage of this over the previous usage of String or TableReference is that now we are able to safely disambiguate between table spec and table reference.","title":"BigQuery"},{"location":"/migrations/v0.8.0-Migration-Guide.html#async-dofns","text":"Async DoFns were refactored.\nAsyncLookupDoFn was renamed to `BaseAsyncLookupDoFn` and we now have better support for Guava, Java 8 and scala Future lookup DoFn’s through the following implementations `GuavaAsyncLookupDoFn`, `JavaAsyncLookupDoFn` and `ScalaAsyncLookupDoFn`.","title":"Async DoFns"},{"location":"/migrations/v0.8.0-Migration-Guide.html#remove-support-for-lisp-case-cli-arguments","text":"In order to be consistent with Beam’s way of passing arguments into the application and construct `PipelineOptions`, we decided to drop support for lisp-case arguments.\nWhat this means is that if you were passing arguments like --foo-bar now you need to pass it as --fooBar.","title":"Remove support for lisp-case CLI arguments"},{"location":"/migrations/v0.9.0-Migration-Guide.html","text":"","title":"Scio v0.9.0"},{"location":"/migrations/v0.9.0-Migration-Guide.html#scio-v0-9-0","text":"","title":"Scio v0.9.0"},{"location":"/migrations/v0.9.0-Migration-Guide.html#tl-dr","text":"Bloom Filter IO’s ScioContext Scala 2.11 drop\nWarning For all the details, refer to the release notes on GitHub.","title":"TL;DR"},{"location":"/migrations/v0.9.0-Migration-Guide.html#bloom-filter","text":"In 0.9.0 we switched from our custom Bloom Filter implementation to Guava Bloom Filter for sparse transforms, e.g. sparseJoin, sparseLookup. As a result, we also switched from Algebird Hash128[K] to Guava Funnel[K] type class for hashing items into the Bloom Filter. Implicit Funnel[K] instances are available through magnolify-guava and need to be imported like this:\nimport magnolify.guava.auto._\nIf for error messages like this:\ncould not find implicit value for parameter hash: com.spotify.scio.hash.BloomFilter.Hash[T]\nThe switch also adds the following benefits:\nPreviously Hash128[K] only provides instances for Int, Long, String, Array[Byte], Array[Int] and Array[Long], while magnolify-guava can derive Funnel[K] for most common types including tuples, case classes, etc.\nWe also added an ApproxFilter abstraction to allow extensible approximate filter implementations. BloomFilter extends ApproxFilter and allows us to create filters & side inputs from Iterable[T] & SCollection[T]. The result filter instances are serializable. For example:\nimport com.spotify.scio._\nimport com.spotify.scio.coders.Coder\nimport com.spotify.scio.hash._\nimport com.spotify.scio.values._\nimport magnolify.guava._\n\nval bf: BloomFilter[String] = Seq(\"a\", \"b\", \"c\").asApproxFilter(BloomFilter)\n\nval sc = ScioContext()\nval data = sc.parallelize(Seq(\"a\", \"b\", \"c\"))\nval bfs: SCollection[BloomFilter[String]] = data.asApproxFilter(BloomFilter)\nval bfsi: SideInput[BloomFilter[String]] = data.asApproxFilterSideInput(BloomFilter)\n\nval bfCoder: Coder[BloomFilter[String]] = BloomFilter.filterCoder","title":"Bloom Filter"},{"location":"/migrations/v0.9.0-Migration-Guide.html#ios","text":"","title":"IO’s"},{"location":"/migrations/v0.9.0-Migration-Guide.html#avro","text":"ReflectiveRecordIO was removed in this release and this means that we no longer need to pass a type param when reading GenericRecord making things a little bit cleaner. This unfortunately means that you will need to update your code by removing the type param from avroFile.\nval sc: ScioContext = ???\n\n- sc.avroFile[GenericRecord](path, schema)\n+ sc.avroFile(path, schema)","title":"Avro"},{"location":"/migrations/v0.9.0-Migration-Guide.html#tensorflow","text":"Removed saveAsTfExampleFile in favor of saveAsTfRecordFile as they express better underlying format in each Example’s are being written.\nval coll: SCollection[T <: Example] = ???\n\n- coll.saveAsTfExampleFile(...)\n+ coll.saveAsTfRecordFile(...)","title":"Tensorflow"},{"location":"/migrations/v0.9.0-Migration-Guide.html#end-of-life","text":"scio-cassandra2 and scio-elasticsearch2 reached end-of-life and were removed.","title":"End-of-Life"},{"location":"/migrations/v0.9.0-Migration-Guide.html#sciocontext","text":"All the deprecated behavior around execution and pipeline result in 0.8.x was removed!\nThis means that to start your pipeline you need to:\nval sc: ScioContext = ???\n\n- val result: ScioResult = sc.close()\n+ val execution: ScioExecutionContext = sc.run()\nand to get a ScioResult you need to:\nval sc: ScioContext = ???\n\n- val result: ScioResult = sc.close()\n+ val result: ScioResult = sc.run().waitUntilDone(Duration.Inf)","title":"ScioContext"},{"location":"/migrations/v0.9.0-Migration-Guide.html#scala-2-11-drop","text":"2.11 served us well! The ecosystem is moving on and so are we! From this version forward we will only support 2.12 and 2.13!\nMigrating from 2.11 to 2.12 should not imply any code update, it should be as easy as updating your build.sbt:\n- scalaVersion := \"2.11.12\"\n+ scalaVersion := \"2.12.11\"\nHowever, migrating to 2.13 might require some changes, especially around collections! we advise you to look at the Scala migration guide for an in-depth overview of the most important changes.","title":"Scala 2.11 drop"},{"location":"/dev/index.html","text":"","title":"Development"},{"location":"/dev/index.html#development","text":"Build Style Guide How to Release Design Philosophy","title":"Development"},{"location":"/dev/build.html","text":"","title":"Build"},{"location":"/dev/build.html#build","text":"","title":"Build"},{"location":"/dev/build.html#getting-the-source","text":"git clone https://github.com/spotify/scio.git","title":"Getting the source"},{"location":"/dev/build.html#compiling","text":"Build and test the code.\ncd scio\nsbt test\nSome examples depend on Google Cloud Platform and are excluded by default if GCP credentials are missing. To enable them, authenticate yourself for GCP, set up default credentials and restart sbt.\ngcloud auth application-default login\nsbt test\nAlternatively you can populate pre-generated cache for BigQuery schemas to bypass GCP access. Define bigquery.project as a system property. The value can by anything since we’ll hit cache instead.\n./scripts/gen_schemas.sh\nsbt -Dbigquery.project=dummy-project test\nTasks within the ‘it’ (integration testing) configuration it:{compile,test} currently require access to datasets hosted in an internal Spotify project. External users must authenticate against their own GCP project, through the steps outlined in Getting Started.","title":"Compiling"},{"location":"/dev/build.html#intellij-idea","text":"When opening the project in IntelliJ IDEA, tick “Use sbt shell:” both “for imports” and “for builds”.","title":"IntelliJ IDEA"},{"location":"/dev/Style-Guide.html","text":"","title":"Style Guide"},{"location":"/dev/Style-Guide.html#style-guide","text":"","title":"Style Guide"},{"location":"/dev/Style-Guide.html#general-guidelines","text":"","title":"General Guidelines"},{"location":"/dev/Style-Guide.html#scalafmt","text":"We use scalafmt to format code automatically and keep the code style consistent.","title":"Scalafmt"},{"location":"/dev/Style-Guide.html#sbt-plugin","text":"Run the following command to format the entire codebase.\nsbt scalafmt test:scalafmt scalafmtSbt it:scalafmt","title":"Sbt plugin"},{"location":"/dev/Style-Guide.html#intellij-idea","text":"Most of us write Scala code in IntelliJ IDEA and it’s wise to let the IDE do most of the work including managing imports and formatting code. To use scalafmt you need an additional plugin. Follow this link to install it.\nWe also want to avoid custom settings as much as possible to make on-boarding new developers easier. Hence we use IntelliJ IDEA’s default settings with the following exceptions:\nSet Code Style → Scala → Formatter → scalafmt Under Copyright → Copyright Profiles, add the following template.\nCopyright $today.year Spotify AB.\n\n  Licensed under the Apache License, Version 2.0 (the \"License\");\n  you may not use this file except in compliance with the License.\n  You may obtain a copy of the License at\n\n      http://www.apache.org/licenses/LICENSE-2.0\n\n  Unless required by applicable law or agreed to in writing,\n  software distributed under the License is distributed on an\n  \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n  KIND, either express or implied.  See the License for the\n  specific language governing permissions and limitations\n  under the License.","title":"IntelliJ IDEA"},{"location":"/dev/Style-Guide.html#scalastyle","text":"We use ScalaStyle to cover other rules that we don’t cover with scalafmt and the entire code base should pass. In case of exceptions, on way of temporarily suppress the warnings is to wrap the violating code with a pair of comments:\n// scalastyle:off regex\nprintln(\"hello\")\n// scalastyle:on regex\nCheck ScalaStyle configuration doc for other ways of using comment filters.","title":"ScalaStyle"},{"location":"/dev/Style-Guide.html#scalafix","text":"Scalafix is a refactoring and linting tool that we rely on as well to keep our code tidy. Currently we use the following rules:\nrules = [\n  RemoveUnused,\n  LeakingImplicitClassVal\n]","title":"Scalafix"},{"location":"/dev/Style-Guide.html#references","text":"We want to adhere to the styles of well known Scala projects and use the following documents as references when scalafmt needs a little bit of help. We follow the Databricks Scala Guide mainly with a few differences described in the next section.\nDatabricks Scala Guide The Official Scala Style Guide Twitter’s Effective Scala","title":"References"},{"location":"/dev/Style-Guide.html#differences-from-databricks-scala-guide","text":"","title":"Differences from Databricks Scala Guide"},{"location":"/dev/Style-Guide.html#spacing-and-indentation","text":"For method declarations, align parameters when they don’t fit in a single line. Return types on the same line as the last parameter.\ndef saveAsBigQuery(\n    table: TableReference,\n    schema: TableSchema,\n    writeDisposition: WriteDisposition,\n    createDisposition: CreateDisposition,\n    tableDescription: String,\n    timePartitioning: TimePartitioning)(implicit ev: T <:< TableRow): ClosedTap[TableRow] = {\n   // method body\n  }\n\n  def saveAsTypedBigQuery(\n    tableSpec: String,\n    writeDisposition: WriteDisposition = TableWriteParam.DefaultWriteDisposition,\n    createDisposition: CreateDisposition = TableWriteParam.DefaultCreateDisposition,\n    timePartitioning: TimePartitioning = TableWriteParam.DefaultTimePartitioning)(\n    implicit tt: TypeTag[T],\n    ev: T <:< HasAnnotation,\n    coder: Coder[T]): ClosedTap[T] = {\n      // method body\n  }\nFor classes whose header doesn’t fit in a single line and exceed the line, align the next line and add a blank line after class header.\nclass Foo(val param1: String,\n          val param2: String,\n          val param3: Array[Byte])\n  extends FooInterface  // 2 space indent here\n  with Logging {\n\n  def firstMethod(): Unit = { /* body */ }  // blank line above\n}","title":"Spacing and Indentation"},{"location":"/dev/Style-Guide.html#blank-lines-vertical-whitespace-","text":"A single blank line appears: Between consecutive members (or initializers) of a class: fields, constructors, methods, nested classes, static initializers, instance initializers. Within method bodies, as needed to create logical groupings of statements. Before the first member and after the last member of the class. A blank line is optional: Between consecutive one-liner fields or methods of a class that have no ScalaDoc. Before the first member and after the last member of a short class with one-liner members only. Use one blank line to separate class definitions. Excessive number of blank lines is discouraged.\nclass Foo {\n\n  val x: Int  // blank line before the first member\n  val y: Int\n  val z: Int  // no blank line between one-liners that have no ScalaDoc\n\n  def hello(): {\n    // body\n  }  // blank line after the last member\n\n}\n\n// no blank line before the first member and after the last member\nclass Bar {\n  def x = { /* body */ }\n  def y = { /* body */ }\n}","title":"Blank Lines (Vertical Whitespace)"},{"location":"/dev/Style-Guide.html#curly-braces","text":"Put curly braces even around one-line conditional or loop statements. The only exception is if you are using if/else as an one-line ternary operator that is also side-effect free.\n// the only exception for omitting braces\nval x = if (true) expression1 else expression2","title":"Curly Braces"},{"location":"/dev/Style-Guide.html#documentation-style","text":"Use Java docs style instead of Scala docs style. One-liner ScalaDoc is acceptable. Annotations like @tparam, @param, @return are optional if they are obvious to the user.\nScalaDoc /** */ should only be used for documenting API to end users. Use regular comments e.g. /* */ and // for explaining code to developers.\n/** This is a correct one-liner, short description. */\n\n/**\n * This is correct multi-line JavaDoc comment. And\n * this is my second line, and if I keep typing, this would be\n * my third line.\n */\n\n/** In Spark, we don't use the ScalaDoc style so this\n  * is not correct.\n  */\n\n// @param xs, @tparam T and @return are obvious and no need to document\n/** Sum up a sequence with an Algebird Semigroup. */\ndef sum[T: Semigroup](xs: Seq[T]): T = xs.reduce(implicitly[Semigroup[T]].plus)","title":"Documentation Style"},{"location":"/dev/Style-Guide.html#ordering-within-a-class","text":"If a class is long and has many methods, group them logically into different sections, and use comment headers to organize them.\nclass ScioContext {\n\n  // =======================================================================\n  // Read operations\n  // =======================================================================\n\n  // =======================================================================\n  // Accumulators\n  // =======================================================================\n\n}\nOf course, the situation in which a class grows this long is strongly discouraged, and is generally reserved only for building certain public APIs.","title":"Ordering within a Class"},{"location":"/dev/Style-Guide.html#imports","text":"Mutable collections Always prefix a mutable collection type with M when importing, e.g. import scala.collection.mutable.{Map => MMap} Or import scala.collection.mutable package and use mutable.Map Sort imports in IntelliJ IDEA’s default order: java.* All other imports scala.* It should look like this in Imports → Import Layout\njava\n_______ blank line _______\nall other imports\n_______ blank line _______\nscala","title":"Imports"},{"location":"/dev/How-to-Release.html","text":"","title":"How to Release"},{"location":"/dev/How-to-Release.html#how-to-release","text":"","title":"How to Release"},{"location":"/dev/How-to-Release.html#prerequisites","text":"Sign up for a Sonatype account here Ask for permissions to push to com.spotify domain like in this ticket Add Sonatype credentials to ~/.sbt/1.0/credentials.sbt credentials ++= Seq(\n  Credentials(\n\"Sonatype Nexus Repository Manager\",\n\"oss.sonatype.org\",\n\"$USERNAME\",\n\"$PASSWORD\"))\n Create a PGP key, for example on keybase.io, and distribute it to a public keyserver","title":"Prerequisites"},{"location":"/dev/How-to-Release.html#release-procedure","text":"Run the slow integration tests with SLOW=true sbt it:test Run release skip-tests in sbt console and follow the instructions Go to oss.sonatype.org, find the staging repository, “close” and “release” Run ./scripts/make-site.sh to update documentation Pick a release name from here, here, here or other interesting sources* Update the list of release names below When the tag build completes, update release notes with name and change log If the release includes a Beam version bump, update the version matrix Run scripts/bump_scio.sh to update homebrew formula and scioVersion in downstream repos including scio.g8, featran, etc. Bump version in the internal scio-cookie and monorepo Send internal announcement to scio-users@spotify.com and flatmap-announce@spotify.com Send external announcement to scio-users@googlegroups.com and user@beam.apache.org Announce on internal Slack and public Gitter room Announce on Twitter\n*Starting with 0.4.0 all release names are scientific names of animals with genus and species starting with the same letter, in ascending alphabetical order. Starting with 0.8.0 all release names are Harry Potter spells.","title":"Release procedure"},{"location":"/dev/How-to-Release.html#past-release-names","text":"","title":"Past release names"},{"location":"/dev/How-to-Release.html#0-9-x","text":"v0.9.0 - “Furnunculus”","title":"0.9.x"},{"location":"/dev/How-to-Release.html#0-8-x","text":"v0.8.4 - “Expecto Patronum” v0.8.3 - “Draconifors” v0.8.2 - “Capacious Extremis” v0.8.1 - “Bombarda Maxima” v0.8.0 - “Amato Animo Animato Animagus”","title":"0.8.x"},{"location":"/dev/How-to-Release.html#0-7-x","text":"v0.7.4 - “Watsonula wautieri” v0.7.3 - “Vulpes Vulpes” v0.7.2 - “Ursus t. Ussuricus” v0.7.1 - “Taxidea Taxus” v0.7.0 - “Suricata suricatta”","title":"0.7.x"},{"location":"/dev/How-to-Release.html#0-6-x","text":"v0.6.1 - “Rhyncholestes raphanurus” v0.6.0 - “Quelea Quelea”","title":"0.6.x"},{"location":"/dev/How-to-Release.html#0-5-x","text":"v0.5.7 - “Panthera pardus” v0.5.6 - “Orcinus orca” v0.5.5 - “Nesolagus netscheri” v0.5.4 - “Marmota monax” v0.5.3 - “Lasiorhinus latifrons” v0.5.2 - “Kobus kob” v0.5.1 - “Jaculus jerboa” v0.5.0 - “Ia io”","title":"0.5.x"},{"location":"/dev/How-to-Release.html#0-4-x","text":"v0.4.7 - “Hydrochoerus hydrochaeris” v0.4.6 - “Galago gallarum” v0.4.5 - “Felis ferus” v0.4.4 - “Erinaceus europaeus” v0.4.3 - “Dendrohyrax dorsalis” v0.4.2 - “Castor canadensis” v0.4.1 - “Blarina brevicauda” v0.4.0 - “Atelerix albiventris”","title":"0.4.x"},{"location":"/dev/How-to-Release.html#0-3-x","text":"v0.3.6 - “Veritas odit moras” v0.3.5 - “Unitas, veritas, carnitas” v0.3.4 - “Sectumsempra” v0.3.3 - “Petrificus totalus” v0.3.2 - “Ut tensio sic vis” v0.3.1 - “Expecto patronum” v0.3.0 - “Lux et veritas”","title":"0.3.x"},{"location":"/dev/How-to-Release.html#0-2-x","text":"v0.2.13 - “Ex luna scientia” v0.2.12 - “In extremo” v0.2.11 - “Saltatio mortis” v0.2.10 - “De Mysteriis Dom Sathanas” v0.2.9 - “Hoc tempore atque nunc et semper” v0.2.8 - “Consummatum est” v0.2.7 - “Crescat scientia vita excolatur” v0.2.6 - “Sensu lato” v0.2.5 - “Imperium in imperio” v0.2.4 - “Ab imo pectore” v0.2.3 - “Aurea mediocritas” v0.2.2 - “Intelligenti pauca” v0.2.1 - “Sedes incertae” v0.2.0 - “Nulli secundus”","title":"0.2.x"},{"location":"/dev/How-to-Release.html#0-1-x","text":"v0.1.11 - “In silico” v0.1.10 - “Memento vivere” v0.1.9 - “Lucem sequimur” v0.1.8 - “Nemo saltat sobrius” v0.1.7 - “Spem gregis” v0.1.6 - “Sic infit” v0.1.5 - “Ad astra” v0.1.4 - “Ad arbitrium” v0.1.3 - “Ut cognoscant te” v0.1.2 - “Sapere aude” v0.1.1 - “Festina lente” v0.1.0 - “Scio me nihil scire”","title":"0.1.x"},{"location":"/dev/Design-Philosophy.html","text":"","title":"Design Philosophy"},{"location":"/dev/Design-Philosophy.html#design-philosophy","text":"We learned a lot building and improving Scio. The project was inspired by Spark and Scalding from the beginning, and we improved it over time working with customers of diverse background, including backend, data and ML. The design philosophy behind Scio can be summarized in a few points.\nMake it easy to do the right thing Scala made this possible for the most part. We have a fluent API and it’s easy to find the right transformation without going through lengthy documentation or source code. The most obvious thing is usually the best. .countByValue is clearer and more efficient than .map((_, 1L)).sumByKey than .map((_, 1L)).reduceByKey(_+_). Case classes and Options are much safer and easier than JSON-based TableRows with Objects and nulls, despite the effort we went through to make it work. One can .sum types with built-in Semigroups easily and correctly. Conversely there is no .groupAll since it could incur huge performance penalty and is essentially .groupBy(_ => ()). It’s easier to ask than making the wrong assumption and use it wrong (_“because it’s there”_). Make common use cases simple We have syntactic sugar for most common IO modules e.g. ScioContext#textFile, SCollection#saveAsBigQuery but don’t cover all possible parameters. There’s a trade-off between covering more use cases and keeping the API simple. We opted for a more flexible boilerplate free Args instead of the more type-safe PipelineOptions for command line arguments parsing. Mistakes in these parts of the code are easier to catch and less damaging than those in the computation logic. Another trade-off we made. We have syntactic sugars for various types of joins (hash, inner, outer, sketch) and side input operations (cross, lookup) that can be easily swapped to fine tune a pipeline. Make complex use cases possible We wrap complex internal APIs but don’t hide them away from users. Most low level Beam APIs (Pipeline, PCollection, PTransform) are still easily accessible. There are shorthands for integrating native Beam API, e.g. ScioContext#customInput, SCollection#saveAsCustomOutput, SCollection#applyTransform. Pipelines can be submitted from main, another process, a backend service, or chained with Futures.","title":"Design Philosophy"},{"location":"/scaladoc.html","text":"","title":""},{"location":"/Scio,-Beam-and-Dataflow.html","text":"","title":"Scio, Beam and Dataflow"},{"location":"/Scio,-Beam-and-Dataflow.html#scio-beam-and-dataflow","text":"Check out the Beam Programming Guide first for a detailed explanation of the Beam programming model and concepts. Also see this comparison between Scio, Scalding and Spark APIs.\nScio aims to be a thin wrapper on top of Beam while offering idiomatic Scala style API.","title":"Scio, Beam and Dataflow"},{"location":"/Scio,-Beam-and-Dataflow.html#basics","text":"`ScioContext` wraps `Pipeline` `SCollection` wraps `PCollection` `ScioResult` wraps `PipelineResult` Most `PTransform` are implemented as idiomatic Scala methods on SCollection e.g. map, flatMap, filter, reduce. `PairSCollectionFunctions` and `DoubleSCollectionFunctions` are specialized version of SCollection implemented via the Scala “pimp my library” pattern. An SCollection[(K, V)] is automatically converted to a PairSCollectionFunctions which provides key-value operations, e.g. groupByKey, reduceByKey, cogroup, join. An SCollection[Double] is automatically converted to a DoubleSCollectionFunctions which provides statistical operations, e.g. stddev, variance.","title":"Basics"},{"location":"/Scio,-Beam-and-Dataflow.html#sciocontext-pipelineoptions-args-and-scioresult","text":"Beam/Dataflow uses `PipelineOptions` and its subclasses to parse command line arguments. Users have to extend the interface for their application level arguments. Scalding uses Args to parse application arguments in a more generic and boilerplate free style. ScioContext has a parseArguments method that takes an Array[String] of command line arguments, parses Beam/Dataflow specific ones into a PipelineOptions, and application specific ones into an Args, and returns the (PipelineOptions, Args). ContextAndArgs is a short cut to create a (ScioContext, Args). ScioResult can be used to access accumulator values and job state.","title":"ScioContext, PipelineOptions, Args and ScioResult"},{"location":"/Scio,-Beam-and-Dataflow.html#io","text":"Most `IO` Read transforms are implemented as methods on ScioContext, e.g. avroFile, textFile, bigQueryTable. Most IO Write transforms are implemented as methods on SCollection, e.g. saveAsAvroFile, saveAsTextFile, saveAsBigQueryTable. These IO operations also detects when the ScioContext is running in a `JobTest` and manages test IO in memory. Write options also return a `ClosedTap`. Once the job completes you can open the `Tap`. Tap abstracts away the logic of reading the dataset directly as an Iterator[T] or re-opening it in another ScioContext. The Future is complete once the job finishes. This can be used to do light weight pipeline orchestration e.g. WordCountOrchestration.scala.","title":"IO"},{"location":"/Scio,-Beam-and-Dataflow.html#bykey-operations","text":"Beam/Dataflow ByKey transforms require PCollection[KV[K, V]] inputs while Scio uses SCollection[(K, V)] Hence every ByKey transform in PairSCollectionFunctions converts Scala (K, V) to KV[K, V] before and vice versa afterwards. However these are lightweight wrappers and the JVM should be able to optimize them. PairSCollectionFunctions also converts java.lang.Iterable[V] and java.util.List[V] to scala.Iterable[V] in some cases.","title":"ByKey operations"},{"location":"/Scio,-Beam-and-Dataflow.html#coders","text":"Beam/Dataflow uses `Coder` for (de)serializing elements in a PCollection during shuffle. There are built-in coders for Java primitive types, collections, and common types in GCP like Avro, ProtoBuf, BigQuery TableRow, Datastore Entity. PCollection uses TypeToken from Guava reflection and to workaround Java type erasure and retrieve type information of elements. This may not always work but there is a PCollection#setCoder method to override. Twitter’s chill library uses kryo to (de)serialize data. Chill includes serializers for common Scala types and cal also automatically derive serializers for arbitrary objects. Scio falls back to KryoAtomicCoder when a built-in one isn’t available. A coder may be non-deterministic if Coder#verifyDeterministic throws an exception. Any data type with such a coder cannot be used as a key in ByKey operations. However KryoAtomicCoder assumes all types are deterministic for simplicity so it’s up to the user’s discretion to not avoid non-deterministic types e.g. tuples or case classes with doubles as keys. Avro GenericRecord requires a schema during deserialization (which is available as GenericRecord#getSchema for serialization) and `AvroCoder` requires that too during initialization. This is not possible in KryoAtomicCoder, i.e. when nesting GenericRecord inside a Scala type. Instead KryoAtomicCoder serializes the schema before every record so that they can roundtrip safely. This is not optimal but the only way without requiring user to handcraft a custom coder.","title":"Coders"},{"location":"/Scio,-Scalding-and-Spark.html","text":"","title":"Scio, Spark and Scalding"},{"location":"/Scio,-Scalding-and-Spark.html#scio-spark-and-scalding","text":"Check out the Beam Programming Guide first for a detailed explanation of the Beam programming model and concepts. Also read more about the relationship between Scio, Beam and Dataflow.\nScio’s API is heavily influenced by Spark with a lot of ideas from Scalding.","title":"Scio, Spark and Scalding"},{"location":"/Scio,-Scalding-and-Spark.html#scio-and-spark","text":"The Dataflow programming model is fundamentally different from that of Spark. Read this Google blog article for more details.\nThe Scio API is heavily influenced by Spark but there are some minor differences.\n`SCollection` is equivalent to Spark’s RDD. `PairSCollectionFunctions` and `DoubleSCollectionFunctions` are specialized versions of SCollection and equivalent to Spark’s PairRDDFunctions and DoubleRDDFunctions. Execution planning is static and happens before the job is submitted. There is no driver node in a Dataflow cluster and one can only perform the equivalent of Spark transformations (RDD → RDD) but not actions (RDD → driver local memory). There is no broadcast either but the pattern of RDD → driver via action and driver → RDD via broadcast can be replaced with SCollection.asSingletonSideInput and SCollection.withSideInputs. There is no DStream (continuous series of RDDs) like in Spark Streaming. Values in a SCollection are windowed based on timestamp and windowing operation. The same API works regardless of batch (single global window by default) or streaming mode. Aggregation type transformations that produce SCollections of a single value under global window will produce one value each window when a non-global window is defined. SCollection has extra methods for side input, side output, and windowing.","title":"Scio and Spark"},{"location":"/Scio,-Scalding-and-Spark.html#scio-and-scalding","text":"Scio has a much simpler abstract data types compared to Scalding.\nScalding has many abstract data types like TypedPipe, Grouped, CoGrouped, SortedGrouped. Many of them are intermediate and enable some optimizations or wrap around Cascading’s data model. As a result many Scalding operations are lazily evaluated, for example in pipe.groupBy(keyFn).reduce(mergeFn), mergeFn is lifted into groupBy to operate on the map side as well. Scio on the other hand, has only one main data type SCollection[T] and SCollection[(K, V)] is a specialized variation when the elements are key-value pairs. All Scio operations are strictly evaluated, for example p.groupBy(keyFn) returns (K, Iterable[T]) where the values are immediately grouped, whereas p.reduceByKey(_ + _) groups (K, V) pairs on K and reduces values.\nSome features may look familiar to Scalding users.\n`Args` is a simple command line argument parser similar to the one in Scalding. Powerful transforms are possible with sum, sumByKey, aggregate, aggregrateByKey using Algebird Semigroups and Aggregators. `MultiJoin` and coGroup of up to 22 sources. `JobTest` for end to end pipeline testing.","title":"Scio and Scalding"},{"location":"/Scio,-Scalding-and-Spark.html#scollection","text":"SCollection has a few variations.\n`SCollectionWithSideInput` for replicating small SCollections to all left-hand side values in a large SCollection. `SCollectionWithSideOutput` for output to multiple SCollections. `WindowedSCollection` for accessing window information. `SCollectionWithFanout` and `SCollectionWithHotKeyFanout` for fanout of skewed data.","title":"SCollection"},{"location":"/Scio,-Scalding-and-Spark.html#additional-features","text":"Scio also offers some additional features.\nEach worker can pull files from Google Cloud Storage via `DistCache` to be used in transforms locally, similar to Hadoop distributed cache. See DistCacheExample.scala. Type safe BigQuery IO via Scala macros. Case classes and converters are generated at compile time based on BQ schema. This eliminates the error prone process of handling generic JSON objects. See TypedBigQueryTornadoes.scala. Sinks (saveAs* methods) return ClosedTap[T] that can be opened either in another pipeline as SCollection[T] or directly as Iterator[T] once the current pipeline completes. This enables complex pipeline orchestration. See WordCountOrchestration.scala.","title":"Additional features"},{"location":"/Runners.html","text":"","title":"Runners"},{"location":"/Runners.html#runners","text":"Starting Scio 0.4.4, Beam runner is completely decoupled from scio-core, which no longer depend on any Beam runner now. Add runner dependencies to enable execution on specific backends. For example, when using Scio 0.4.7 which depends on Beam 2.2.0, you should add the following dependencies to run pipelines locally and on Google Cloud Dataflow.\nlibraryDependencies ++= Seq(\n  \"org.apache.beam\" % \"beam-runners-direct-java\" % \"2.2.0\",\n  \"org.apache.beam\" % \"beam-runners-google-cloud-dataflow-java\" % \"2.2.0\"\n)","title":"Runners"},{"location":"/Runners.html#runner-specific-logic","text":"Dataflow specific logic, e.g. job ID, metrics, were also removed from ScioResult. You can convert between the generic ScioResult and runner specific result types like the example below. Note that currently only DataflowResult is implemented.\nimport com.spotify.scio.{ScioContext, ScioExecutionContext, ScioResult}\n\nobject SuperAwesomeJob {\n  def main(cmdlineArgs: Array[String]): Unit = {\n\n    val sc: ScioContext = ???\n\n    // Job code\n    // ...\n\n    // Generic result only\n    val closedContext: ScioExecutionContext = sc.run()\n    val scioResult: ScioResult = closedContext.waitUntilFinish()\n\n    // Convert to Dataflow specific result\n    import com.spotify.scio.runners.dataflow.DataflowResult\n    val dfResult: DataflowResult = scioResult.as[DataflowResult]\n\n    // Convert back to generic result\n    val scioResult2: ScioResult = dfResult.asScioResult\n\n    ()\n  }\n}\nGiven the Google Cloud project ID and Dataflow job ID, one can also create DataflowResult and ScioResult without running a pipeline. This could be when submitting jobs asynchronously and retrieving metrics later.\nimport com.spotify.scio.runners.dataflow.DataflowResult\n\nobject AnotherAwesomeJob {\n  def main(cmdlineArgs: Array[String]): Unit = {\n    val dfResult = DataflowResult(\"<PROJECT_ID>\", \"<REGION>\", \"<JOB_ID>\")\n    val scioResult = dfResult.asScioResult\n    // Some code\n  }\n}","title":"Runner specific logic"},{"location":"/Scio-data-guideline.html","text":"","title":"Data Guidelines"},{"location":"/Scio-data-guideline.html#data-guidelines","text":"Here are some common guidelines for building efficient, cost-effective, and maintainable pipelines. They apply to most use cases but you might have to tweak based on your needs. Also see the FAQ page for some common performance issues and remedies.","title":"Data Guidelines"},{"location":"/Scio-data-guideline.html#development","text":"Use Scio REPL to get familiar with Scio and perform ad-hoc experiments, but don’t use it as a replacement for unit tests.","title":"Development"},{"location":"/Scio-data-guideline.html#storage","text":"Leverage BigQuery, especially BigQuery SELECT query as input whenever possible. BigQuery has a very efficient columnar storage engine, can scale independently from Scio/Dataflow clusters and probably cheaper and easier to write than handcrafted Scala/Java pipeline code. Use BigQuery as an intermediate storage, especially if downstream jobs do a lot of slicing and dicing on rows and columns. Feel free to de-normalize data and use wide rows. Use Bigtable or Datastore depending on requirements for serving pipeline output to production services.","title":"Storage"},{"location":"/Scio-data-guideline.html#computation","text":"Prefer combine/aggregate/reduce transforms over groupByKey. Keep in mind that a reduce operation must be associative and commutative. Prefer sum/sumByKey over reduce/reduceByKey for basic data types. They use Algebird Semigroups and are often more optimized than hard coded reduce functions. See AlgebirdSpec.scala for more examples. Understand the performance characteristics of different types of joins and the role of side input cache, see the FAQ for more. Understand the Kryo serialization tuning options and use custom Kryo serializers for objects in the critical pass if necessary.","title":"Computation"},{"location":"/Scio-data-guideline.html#execution-parameters","text":"When tuning pipeline execution parameters, start with smaller workerMachineType e.g. default n1-standard-1 to n1-standard-4, and reasonable maxNumWorkers that reflect your input size. Keep in mind that there might be limited availability of large GCE instances and more workers means higher shuffle cost.","title":"Execution parameters"},{"location":"/Scio-data-guideline.html#streaming","text":"For streaming jobs with periodically updated state, i.e. log decoration with metadata, keep (and update) states in Bigtable, and do look ups from the streaming job (read more about Bigtable key structure). Also see BigtableDoFn for an abstraction that handles asynchronous Bigtable requests. For streaming, larger worker machine types and SSD for workerDiskType might be more suitable. A typical job with 5 x n1-standard-4 and 100GB SSDs can handle ~30k peak events per second. Also see this article on disk performance.","title":"Streaming"},{"location":"/Apache-Beam.html","text":"","title":"Versions"},{"location":"/Apache-Beam.html#versions","text":"Starting from version 0.3.0, Scio moved from Google Cloud Dataflow Java SDK to Apache Beam as its core dependencies and introduced a few breaking changes.\nDataflow Java SDK 1.x.x uses com.google.cloud.dataflow.sdk namespace. Apache Beam uses org.apache.beam.sdk namespace. Dataflow Java SDK 2.x.x is also based on Apache Beam 2.x.x and uses org.apache.beam.sdk.\nScio 0.3.x depends on Beam 0.6.0 (last pre-stable release) and Scio 0.4.x depends on Beam 2.0.0 (first stable release). Breaking changes in these releases are documented below.","title":"Versions"},{"location":"/Apache-Beam.html#version-matrices","text":"Early Scio releases depend on Google Cloud Dataflow Java SDK while later ones depend on Apache Beam. Check out the Changelog page for migration guides.\nAlso check out the SDK Version Support Status page. Since a Beam release depends on specific version of Dataflow API, a deprecated Beam version is not guaranteed to work correctly or at all. We strongly recommend upgrading before the deprecation date.\nScio SDK Dependency Description 0.9.x Apache Beam 2.x.x Drop Scala 2.11, add Scala 2.13, Guava based Bloom Filter 0.8.x Apache Beam 2.x.x Beam SQL, BigQuery storage API, ScioExecutionContext, Async DoFns 0.7.x Apache Beam 2.x.x Static coders, new ScioIO 0.6.x Apache Beam 2.x.x Cassandra 2.2 0.5.x Apache Beam 2.x.x Better type-safe Avro and BigQuery IO 0.4.x Apache Beam 2.0.0 Stable release Beam 0.3.x Apache Beam 0.6.0 Pre-release Beam 0.2.x Dataflow Java SDK SQL-2011 support 0.1.x Dataflow Java SDK First releases\nScio Version Beam Version Details 0.9.0 2.20.0 This version will be deprecated on April 15, 2021. 0.8.2 2.19.0 This version will be deprecated on February 4, 2021. 0.8.1 2.18.0 This version will be deprecated on January 23, 2021. 0.8.0 2.17.0 This version will be deprecated on January 6, 2021. 0.7.4 2.11.0 This version will be deprecated on March 1, 2020. 0.7.3 2.10.0 This version will be deprecated on February 11, 2020. 0.7.2 2.10.0 0.7.0+ 2.9.0 This version will be deprecated on December 13, 2019. 0.6.0 2.6.0 This version will be deprecated on August 8, 2019. 0.5.7 2.6.0 0.5.6 2.5.0 This version will be deprecated on June 6, 2019. 0.5.1+ 2.4.0 Deprecated as of March 20, 2019. 0.5.0 2.2.0 Deprecated as of December 2, 2018. 0.4.6+ 2.2.0 0.4.1+ 2.1.0 Deprecated as of August 23, 2018. 0.4.0 2.0.0 Deprecated as of May 17, 2018. 0.3.0+ 0.6.0 Unsupported","title":"Version matrices"},{"location":"/Apache-Beam.html#release-cycle-and-backport-procedures","text":"Scio has a frequent release cycle, roughly every 2-4 weeks, as compared to months for the upstream Apache Beam. We also aim to stay a step ahead by pulling changes from upstream and contributing new ones back.\nLet’s call the Beam version that Scio depends on current, and upstream master latest. Here’re the procedures for backporting changes.\nFor changes available in latest but not in current: - Copy Java files from latest to Scio repo - Rename classes and modify as necessary - Release Scio - Update checklist for the next current version like #633 - Remove change once current is updated\nFor changes we want to make to latest: - Submit pull request to latest - Follow the steps above once merged","title":"Release cycle and backport procedures"},{"location":"/Apache-Beam.html#beam-master-nightly-build","text":"To keep up with upstream changes, beam-master branch is built nightly and depends on latest Beam SNAPSHOT.\nWe should do the following periodically to reduce work when upgrading Beam release version. - rebase beam-master on master - fix for breaking changes in beam-master - rebase master on beam-master when upgrading Beam release version.\nTo work on a breaking change: - checkout beam-master branch - run ./scripts/circleci_snapshot.sh to change beamVersion to the latest SNAPSHOT - run sbt test it:test and fix errors","title":"Beam master nightly build"},{"location":"/Changelog.html","text":"","title":"Changelog"},{"location":"/Changelog.html#changelog","text":"","title":"Changelog"},{"location":"/Changelog.html#breaking-changes-since-scio-0-9-0-","text":"Drop Scala 2.11, add Scala 2.13 support Remove deprecated modules scio-cassandra2 and scio-elasticsearch2 Remove deprecated methods since 0.8.0 Switch from Algebird Hash128[K] to Guava Funnel[K] for Bloom filter and sparse transforms","title":"Breaking changes since Scio 0.9.0 (v0.9.0 Migration Guide)"},{"location":"/Changelog.html#breaking-changes-since-scio-0-8-0-","text":"ScioIOs no longer return Future ScioContext#close returns ScioExecutionContext instead of ScioResult Async DoFn refactor Deprecate scio-cassandra2 and scio-elasticsearch2 ContextAndArgs#typed no longer accepts list-case #2221","title":"Breaking changes since Scio 0.8.0 (v0.8.0 Migration Guide)"},{"location":"/Changelog.html#breaking-changes-since-scio-0-7-0-","text":"New Magnolia based Coders derivation New ScioIO replaces TestIO[T] to simplify IO implementation and stubbing in JobTest","title":"Breaking changes since Scio 0.7.0 (v0.7.0 Migration Guide)"},{"location":"/Changelog.html#breaking-changes-since-scio-0-6-0","text":"scio-cassandra2 now requires Cassandra 2.2 instead of 2.0","title":"Breaking changes since Scio 0.6.0"},{"location":"/Changelog.html#breaking-changes-since-scio-0-5-0","text":"BigQueryIO in JobTest now requires a type parameter which could be either TableRow for JSON or T for type-safe API where T is a type annotated with @BigQueryType. Explicit .map(T.toTableRow) of test data is no longer needed. See changes in BigQueryTornadoesTest and TypedBigQueryTornadoesTest for more. Typed AvroIO now accepts case classes instead of Avro records in JobTest. Explicit .map(T.toGenericRecord) of test data is no longer needed. See this change for more. Package com.spotify.scio.extra.transforms is moved from scio-extra to scio-core, under com.spotify.scio.transforms.","title":"Breaking changes since Scio 0.5.0"},{"location":"/Changelog.html#breaking-changes-since-scio-0-4-0","text":"Accumulators are replaced by the new metrics API, see MetricsExample.scala for more com.spotify.scio.hdfs package and related APIs (ScioContext#hdfs*, SCollection#saveAsHdfs*) are removed, regular file IO API should now support both GCS and HDFS (if scio-hdfs is included as a dependency). Starting Scio 0.4.4, Beam runner is completely decoupled from scio-core. See Runners page for more details.","title":"Breaking changes since Scio 0.4.0"},{"location":"/Changelog.html#breaking-changes-since-scio-0-3-0","text":"See this page for a list of breaking changes from Dataflow Java SDK to Beam Scala 2.10 is dropped, 2.11 and 2.12 are the supported Scala binary versions Java 7 is dropped and Java 8+ is required DataflowPipelineRunner is renamed to DataflowRunner DirectPipelineRunner is renamed to DirectRunner BlockingDataflowPipelineRunner is removed and ScioContext#close() will not block execution; use sc.run().waitUntilDone() to retain the blocking behavior, i.e. if you launch job from an orchestration engine like Airflow or Luigi You should set tempLocation instead of stagingLocation regardless of runner; set it to a local path for DirectRunner or a GCS path for DataflowRunner; if not set, DataflowRunner will create a default bucket for the project Type safe BigQuery is now stable API; use import com.spotify.scio.bigquery._ instead of import com.spotify.scio.experimental._ scio-bigtable no longer depends on HBase and uses Protobuf based Bigtable API; check out the updated example Custom IO, i.e. ScioContext#customInput and SCollection#saveAsCustomOutput require a name: String parameter","title":"Breaking changes since Scio 0.3.0"},{"location":"/FAQ.html","text":"","title":"FAQ"},{"location":"/FAQ.html#faq","text":"General questions What’s the status of Scio? Who’s using Scio? What’s the relationship between Scio and Apache Beam? What’s the relationship between Scio and Google Cloud Dataflow? How does Scio compare to Scalding or Spark? What are GCE availability zone and GCS bucket location? Programming questions How do I setup a new SBT project? How do I deploy Scio jobs to Dataflow? How do I use the SNAPSHOT builds of Scio? How do I unit test pipelines? How do I combine multiple input sources? How do I log in a job? How do I use Beam’s Java API in Scio? What are the different types of joins and performance implication? How to create Dataflow job template? How do I cancel a job after certain time period? Why can’t I have an SCollection inside another SCollection? BigQuery questions What is BigQuery dataset location? How stable is the type safe BigQuery API? How do I work with nested Options in type safe BigQuery? How do I unit test BigQuery queries? How do I stream to a partitioned BigQuery table? How do I invalidate cached BigQuery results or disable cache? How does BigQuery determines job priority? Streaming questions How do I update a streaming job? Other IO components How do I access various files outside of a ScioContext? How do I reduce Datastore boilerplate? How do I throttle Bigtable writes? How do I use custom Kryo serializers? What Kryo tuning options are there? Development environment issues How do I keep SBT from running out of memory? How do I fix SBT heap size error in IntelliJ? How do I fix “Unable to create parent directories” error in IntelliJ? How to make IntelliJ IDEA work with type safe BigQuery classes? Common issues What does “Cannot prove that T1 <:< T2” mean? How do I fix invalid default BigQuery credentials? Why are my typed BigQuery case classes not up to date? How do I fix “SocketTimeoutException” with BigQuery? Why do I see names like “main@{NativeMethodAccessorImpl...}” in the UI? How do I fix “RESOURCE_EXHAUSTED” error? Can I use “scala.App” trait instead of “main” method? How to inspect the content of an SCollection? How do I improve side input performance? How do I control concurrency (number of DoFn threads) in Dataflow workers How to manually investigate a Cloud Dataflow worker","title":"FAQ"},{"location":"/FAQ.html#general-questions","text":"","title":"General questions"},{"location":"/FAQ.html#whats-the-status-of-scio-","text":"Scio is widely being used for production data pipelines at Spotify and is our preferred framework for building new pipelines on Google Cloud. We run Scio on Google Cloud Dataflow service in both batch and streaming modes. However it’s still under heavy development and there might be minor breaking API changes from time to time.","title":"What’s the status of Scio?"},{"location":"/FAQ.html#whos-using-scio-","text":"Spotify uses Scio for all new data pipelines running on Google Cloud Platform, including music recommendation, monetization, artist insights and business analysis. We also use BigQuery, Bigtable and Datastore heavily with Scio. We use Scio in both batch and streaming mode.\nAs of mid 2017, there’re 200+ developers and 700+ production pipelines. The largest batch job we’ve seen uses 800 n1-highmem-32 workers (25600 CPUs, 166.4TB RAM) and processes 325 billion rows from Bigtable (240TB). We also have numerous jobs that process 10TB+ of BigQuery data daily. On the streaming front, we have many jobs with 30+ n1-standard-16 workers (480 CPUs, 1.8TB RAM) and SSD disks for real time machine learning or reporting.\nFor a incomplete list of users, see the Powered By page.","title":"Who’s using Scio?"},{"location":"/FAQ.html#whats-the-relationship-between-scio-and-apache-beam-","text":"Scio is a Scala API built on top of Apache Beam’s Java SDK. Scio aims to offer a concise, idiomatic Scala API for a subset of Beam’s features, plus extras we find useful, like REPL, type safe BigQuery, and IO taps.","title":"What’s the relationship between Scio and Apache Beam?"},{"location":"/FAQ.html#whats-the-relationship-between-scio-and-google-cloud-dataflow-","text":"Scio (version before 0.3.0) was originally built on top of Google Cloud Dataflow’s Java SDK. Google donated the code base to Apache and renamed it Beam. Cloud Dataflow became one of the supported runners, alongside Apache Flink & Apache Spark. Scio 0.3.x is built on top of Beam 0.6.0 and 0.4.x is built on top of Beam 2.x. Many users run Scio on the Dataflow runner today.","title":"What’s the relationship between Scio and Google Cloud Dataflow?"},{"location":"/FAQ.html#how-does-scio-compare-to-scalding-or-spark-","text":"Check out the wiki page on Scio, Scalding and Spark. Also check out Big Data Rosetta Code for some snippets.","title":"How does Scio compare to Scalding or Spark?"},{"location":"/FAQ.html#what-are-gce-availability-zone-and-gcs-bucket-location-","text":"GCE availability zone is where the Google Cloud Dataflow service spins up VM instances for your job, e.g. us-east1-a. Each GCS bucket (gs://bucket) has a storage class and bucket location that affects availability, latency and price. The location should be close to GCE availability zone. Dataflow uses --stagingLocation for job jars, temporary files and BigQuery I/O.","title":"What are GCE availability zone and GCS bucket location?"},{"location":"/FAQ.html#programming-questions","text":"","title":"Programming questions"},{"location":"/FAQ.html#how-do-i-setup-a-new-sbt-project-","text":"Read the documentation.","title":"How do I setup a new SBT project?"},{"location":"/FAQ.html#how-do-i-deploy-scio-jobs-to-dataflow-","text":"When developing locally, you can do sbt \"runMain MyClass ... or just runMain MyClass ... in the SBT console without building any artifacts.\nWhen deploying to the cloud, we recommend using sbt-pack or sbt-native-packager plugin instead of sbt-assembly. Unlike assembly, they pack dependency jars in a directory instead of merging them, so that we don’t have to deal with merge strategy and dependency jars can be cached by Dataflow service.\nAt Spotify we pack jars with sbt-pack, build docker images with sbt-docker together with orchestration components e.g. Luigi or Airflow and deploy them with Styx.","title":"How do I deploy Scio jobs to Dataflow?"},{"location":"/FAQ.html#how-do-i-use-the-snapshot-builds-of-scio-","text":"Commits to Scio master are automatically published to Sonatype via continuous integration. To use the latest SNAPSHOT artifact, add the following line to your build.sbt.\nresolvers += Resolver.sonatypeRepo(\"snapshots\")\nOr you can configure SBT globally by adding the following to ~/.sbt/1.0/global.sbt.\nresolvers ++= Seq(\n  Resolver.sonatypeRepo(\"snapshots\")\n  // other resolvers\n)","title":"How do I use the SNAPSHOT builds of Scio?"},{"location":"/FAQ.html#how-do-i-unit-test-pipelines-","text":"Any Scala or Java unit testing frameworks can be used with Scio but we provide some utilities for ScalaTest.\nPipelineTestUtils - utilities for testing parts of a pipeline JobTest - for testing pipelines end-to-end with complete arguments and IO coverage SCollectionMatchers - ScalaTest matchers for SCollection PipelineSpec - shortcut for ScalaTest FlatSpec with utilities and matchers\nThe best place to find example useage of JobTest and SCollectionMatchers are their respective tests in JobTestTest and SCollectionMatchersTest. For more examples see:\nscio-examples https://github.com/spotify/big-data-rosetta-code/tree/master/src/test/scala/com/spotify/bdrc/testing","title":"How do I unit test pipelines?"},{"location":"/FAQ.html#how-do-i-combine-multiple-input-sources-","text":"How do I combine multiple input sources, e.g. different BigQuery tables, files located in different GCS buckets? You can combine SCollections from different sources into one using the companion method SCollection.unionAll, for example:\nimport com.spotify.scio._\nimport com.spotify.scio.avro._\nimport com.spotify.scio.values._\nimport com.spotify.scio.avro.TestRecord\n\nobject MyJob {\n  def main(cmdlineArgs: Array[String]): Unit = {\n    val (sc, args) = ContextAndArgs(cmdlineArgs)\n\n    val collections =\n      Seq(\"gs://bucket1/data/*.avro\", \"gs://bucket2/data/*.avro\")\n        .map(sc.avroFile[TestRecord](_))\n\n    val all = SCollection.unionAll(collections)\n  }\n}","title":"How do I combine multiple input sources?"},{"location":"/FAQ.html#how-do-i-log-in-a-job-","text":"You can log in a Scio job with most common logging libraries but slf4j is included as a dependency. Define the logger instance as a member of the job object and use it inside a lambda.\nimport com.spotify.scio._\nimport org.slf4j.LoggerFactory\n\nobject MyJob {\n  private val logger = LoggerFactory.getLogger(this.getClass)\n  def main(cmdlineArgs: Array[String]): Unit = {\n    val (sc, args) = ContextAndArgs(cmdlineArgs)\n\n    sc.parallelize(1 to 100)\n      .map { i =>\n        logger.info(s\"Element $i\")\n        i * i\n      }\n    // ...\n  }\n}","title":"How do I log in a job?"},{"location":"/FAQ.html#how-do-i-use-beams-java-api-in-scio-","text":"Scio exposes a few things to allow easy integration with native Beam Java API, notably:\nScioContext#customInput to apply a PTransform[_ >: PBegin, PCollection[T]] (source) and get a SCollection[T]. SCollection#applyTransform to apply a PTransform[_ >: PCollection[T], PCollection[U]] and get a SCollection[U] SCollection#saveAsCustomOutput to apply a PTransform[_ >: PCollection[T], PDone] (sink) and get a ClosedTap[T].\nSee BeamExample.scala for more details. Custom I/O can also be tested via the `JobTest` harness.","title":"How do I use Beam’s Java API in Scio?"},{"location":"/FAQ.html#what-are-the-different-types-of-joins-and-performance-implication-","text":"Inner (a.join(b)), left (a.leftOuterJoin(b)), outer (a.fullOuterJoin(b)) performs better with a large LHS. So a should be the larger data set with potentially more hot keys, i.e. key with many values. Every key-value pair from every input is shuffled. join/leftOuterJoin may be replaced by hashJoin/leftHashJoin if the RHS is small enough to fit in memory (e.g. < 1GB). The RHS is used as a multi-map side input for the LHS. No shuffle is performed. Consider skewedJoin if some keys on the LHS are extremely hot. Consider sparseOuterJoin if you want a full outer join where RHS is much smaller than LHS, but may not fit in memory. Consider cogroup if you need to access value groups of each key. MultiJoin supports inner, left, outer join and cogroup of up to 22 inputs. For multi-joins larger inputs should be on the left, e.g. size(a) >= size(b) >= size(c) >= size(d) in MultiJoin(a, b, c, d). Check out these slides for more information on joins. Also see this section on Cloud Dataflow Shuffle service.","title":"What are the different types of joins and performance implication?"},{"location":"/FAQ.html#how-to-create-dataflow-job-template-","text":"For Apache Beam based Scio (version >= 0.3.0) use DataflowRunner and specify templateLocation option. For example in CLI --templateLocation=gs://<bucket>/job1. Read more about templates here.","title":"How to create Dataflow job template?"},{"location":"/FAQ.html#how-do-i-cancel-a-job-after-certain-time-period-","text":"You can wait on the ScioResult and call the internal PipelineResult#cancel() method if a timeout exception happens.\nimport com.spotify.scio._\nimport scala.concurrent.duration._\n\nobject MyJob {\n  def main(cmdlineArgs: Array[String]): Unit = {\n    val (sc, args) = ContextAndArgs(cmdlineArgs)\n\n    // ...\n    val closedSc: ScioExecutionContext = sc.run()\n    val result: ScioResult = closedSc.waitUntilFinish(1.minute, cancelJob = true)\n  }\n}","title":"How do I cancel a job after certain time period?"},{"location":"/FAQ.html#why-cant-i-have-an-scollection-inside-another-scollection-","text":"You cannot have an SCollection inside another SCollection, i.e. anything with type SCollection[SCollection[T]]. To explain this we have to go back to the relationship between ScioContext and SCollection. Every ScioContext represents a unique pipeline and every SCollection represents a stage in the pipeline execution, i.e. the state of the pipeline after some transforms has be applied. We start a pipeline code with val sc = ..., create new SCollections with methods on sc, e.g. sc.textFile, and transform them with methods like .map, .filter, .join. Therefore each SCollection can trace its root to one single sc. The pipeline is submitted for execution when we call sc.run(). Hence we cannot have an SCollection inside another SCollection just as we cannot have a pipeline inside another pipeline.","title":"Why can’t I have an SCollection inside another SCollection?"},{"location":"/FAQ.html#bigquery-questions","text":"","title":"BigQuery questions"},{"location":"/FAQ.html#what-is-bigquery-dataset-location-","text":"Each BigQuery dataset has a location (e.g. US, EU) and every table inside are stored in the same location. Tables in a JOIN must be from the same region. Also one can only import/export tables to a GCS bucket in the same location. Starting from v0.2.1, Scio will detect the dataset location of a query and create a staging dataset for ScioContext#bigQuerySelect and @BigQueryType.fromQuery. This location should be the same as that of your --stagingLocation GCS bucket. The old -Dbigquery.staging_dataset.location flag is removed.\nBecause of these limitations and performance reasons, make sure --zone, --stagingLocation and -Dbigquery.staging_dataset.location location of BigQuery datasets are consistent.","title":"What is BigQuery dataset location?"},{"location":"/FAQ.html#how-stable-is-the-type-safe-bigquery-api-","text":"Type Safe BigQuery API is considered stable and widely used at Spotify. There are several caveats however:\nBoth legacy and SQL syntax are supported although the SQL syntax is highly recommended The system will detect legacy or SQL syntax and choose the correct one To override auto-detection, start the query with either #legacysql or #standardsql comment line Legacy syntax is less predictable, especially for complex queries and may be disabled in the future Case classes generated by @BigQueryType.fromTable or @BigQueryType.fromQuery are not recognized in IntelliJ IDEA, but see this section for a workaround","title":"How stable is the type safe BigQuery API?"},{"location":"/FAQ.html#how-do-i-work-with-nested-options-in-type-safe-bigquery-","text":"Any nullable field in BigQuery is translated to Option[T] by the type safe BigQuery API and it can be clunky to work with rows with multiple or nested fields. For example:\nimport com.spotify.scio.bigquery.types.BigQueryType\n\n@BigQueryType.fromSchema(\"\"\"{\n    |\"fields\": [{\n    | \"type\":\"RECORD\",\n    | \"mode\": \"NULLABLE\",\n    | \"name\":\"user\",\n    | \"fields\":[\n    |   {\"mode\": \"NULLABLE\", \"name\":\"email\", \"type\": \"STRING\"},\n    |   {\"mode\": \"REQUIRED\",\"name\":\"name\",\"type\":\"STRING\"}]\n    |}]}\"\"\".stripMargin)\nclass Row\n\ndef doSomethingWithRow(row: Row) = {\n  if (row.user.isDefined) {  // Option[User]\n    val email = row.user.get.email  // Option[String]\n    if (email.isDefined) {\n      doSomething(email.get)\n    }\n  }\n}\nFor comprehension is a nicer alternative in these cases:\ndef doSomethingWithRowUsingFor(row: Row) = {\n  val e: Option[String] =\n    for {\n      u <- row.user\n      e <- u.email\n    } yield e\n  e.foreach(doSomething)\n}\nAlso see these slides and this blog article.","title":"How do I work with nested Options in type safe BigQuery?"},{"location":"/FAQ.html#how-do-i-unit-test-bigquery-queries-","text":"BigQuery doesn’t provide a way to unit test query logic locally, but we can query the service directly in an integration test. Take a look at BigQueryIT.scala. MockBigQuery will create temporary tables on the service, feed them with mock data, and substitute table references in your query string with the mocked ones.","title":"How do I unit test BigQuery queries?"},{"location":"/FAQ.html#how-do-i-stream-to-a-partitioned-bigquery-table-","text":"Currently there is no way to create a partitioned BigQuery table via Scio/Beam when streaming, however it is possible to stream to a partitioned table if it is already created.\nThis can be done by using fixed windows and using the window bounds to infer date. As of Scio 0.4.0-beta2 this looks as follows:\nimport com.spotify.scio._\nimport org.apache.beam.sdk.values.ValueInSingleWindow\nimport org.apache.beam.sdk.transforms.SerializableFunction\nimport org.apache.beam.sdk.transforms.windowing.IntervalWindow\nimport com.google.api.services.bigquery.model.TableRow\nimport org.apache.beam.sdk.io.gcp.bigquery.{BigQueryIO, TableDestination}\nimport BigQueryIO.Write.{CreateDisposition, WriteDisposition}\nimport org.joda.time.format.DateTimeFormat\nimport org.joda.time.{DateTimeZone, Duration}\n\nclass DayPartitionFunction() extends SerializableFunction[ValueInSingleWindow[TableRow], TableDestination] {\n  override def apply(input: ValueInSingleWindow[TableRow]): TableDestination = {\n    val partition = DateTimeFormat.forPattern(\"yyyyMMdd\").withZone(DateTimeZone.UTC)\n      .print(input.getWindow.asInstanceOf[IntervalWindow].start())\n    new TableDestination(\"project:dataset.partitioned$\" + partition, \"\")\n  }\n}\n\nobject BQPartitionedJob {\n\n  def myStringToTableRowConversion: String => TableRow = ???\n\n  def main(cmdlineArgs: Array[String]): Unit = {\n    val (sc, args) = ContextAndArgs(cmdlineArgs)\n\n    sc.pubsubSubscription[String](\"projects/data-university/topics/data-university\")\n      .withFixedWindows(Duration.standardSeconds(30))\n      // Convert to `TableRow`\n      .map(myStringToTableRowConversion)\n      .saveAsCustomOutput(\n        \"SaveAsDayPartitionedBigQuery\",\n        BigQueryIO.writeTableRows().to(\n          new DayPartitionFunction())\n          .withWriteDisposition(WriteDisposition.WRITE_APPEND)\n          .withCreateDisposition(CreateDisposition.CREATE_NEVER)\n      )\n\n    sc.run()\n  }\n}\nIn Scio 0.3.X it is possible to achieve the same behaviour using SerializableFunction[BoundedWindow, String] and BigQueryIO.Write.to. It is also possible to stream to separate tables with a Date suffix by modifying DayPartitionFunction, specifying the Schema, and changing the CreateDisposition to CreateDisposition.CREATE_IF_NEEDED.","title":"How do I stream to a partitioned BigQuery table?"},{"location":"/FAQ.html#how-do-i-invalidate-cached-bigquery-results-or-disable-cache-","text":"Scio’s BigQuery client in Scio caches query result in system property bigquery.cache.directory, which defaults to $PWD/.bigquery. Use rm -rf .bigquery to invalidate all cached results. To disable caching, set system property bigquery.cache.enabled to false.","title":"How do I invalidate cached BigQuery results or disable cache?"},{"location":"/FAQ.html#how-does-bigquery-determines-job-priority-","text":"By default Scio runs BigQuery jobs with BATCH priority except when in the REPL where it runs with INTERACTIVE. To override this, set system property bigquery.priority to either BATCH or INTERACTIVE.","title":"How does BigQuery determines job priority?"},{"location":"/FAQ.html#streaming-questions","text":"","title":"Streaming questions"},{"location":"/FAQ.html#how-do-i-update-a-streaming-job-","text":"Dataflow allows streaming jobs to be updated on the fly by specifying --update, along with --jobName=[your_job] on the command line. See https://cloud.google.com/dataflow/pipelines/updating-a-pipeline for detailed docs. Note that for this to work, Dataflow needs to be able to identify which transformations from the original job map to those in the replacement job. The easiest way to do so is to give unique names to transforms in the code itself. In Scio, this can be achieved by calling .withName() before applying the transform. For example:\nimport com.spotify.scio._\n\ndef main(cmdlineArgs: Array[String]): Unit = {\n  val (sc, args) = ContextAndArgs(cmdlineArgs)\n  sc.textFile(args(\"input\"))\n    .withName(\"MakeUpper\").map(_.toUpperCase)\n    .withName(\"BigWords\").filter(_.length > 6)\n}\nIn this example, the map’s transform name is “MakeUpper” and the filter’s is “BigWords”. If we later decided that we want to count 6 letter words as “big” too, then we can change it to _.length > 5, and because the transform name is the same the job can be updated on the fly.","title":"How do I update a streaming job?"},{"location":"/FAQ.html#other-io-components","text":"","title":"Other IO components"},{"location":"/FAQ.html#how-do-i-access-various-files-outside-of-a-sciocontext-","text":"For Scio version >= 0.4.0\nStarting from Scio 0.4.0 you can use Apache Beam’s Filesystems abstraction:\nimport org.apache.beam.sdk.io.FileSystems\n// the path can be any of the supported Filesystems, e.g. local, GCS, HDFS\ndef readmeResource = FileSystems.matchNewResource(\"gs://<bucket>/README.md\", false)\ndef readme = FileSystems.open(readmeResource)\nFor Scio version < 0.4.0\nNote This part is GCS specific.\nYou can get a `GcsUtil` instance from ScioContext, which can be used to open GCS files in read or write mode.\nimport com.spotify.scio.ContextAndArgs\nimport org.apache.beam.sdk.extensions.gcp.options.GcsOptions\n\ndef main(cmdlineArgs: Array[String]): Unit = {\n  val (sc, args) = ContextAndArgs(cmdlineArgs)\n  val gcsUtil = sc.optionsAs[GcsOptions].getGcsUtil\n  // ...\n}","title":"How do I access various files outside of a ScioContext?"},{"location":"/FAQ.html#how-do-i-reduce-datastore-boilerplate-","text":"Datastore Entity class is actually generated from Protobuf which uses the builder pattern and very boilerplate heavy. You can use the Magnolify library to seamlessly convert bewteen case classes and Entitys. See MagnolifyDatastoreExample.scala for an example job and MagnolifyDatastoreExampleTest.scala for tests.","title":"How do I reduce Datastore boilerplate?"},{"location":"/FAQ.html#how-do-i-throttle-bigtable-writes-","text":"Currently Dataflow autoscaling may not work well with large writes BigtableIO. Specifically It does not take into account Bigtable IO rate limits and may scale up more workers and end up hitting the limit and eventually fail the job. As a workaround, you can enable throttling for Bigtable writes in Scio 0.4.0-alpha2 or later.\nimport com.spotify.scio.values._\nimport com.spotify.scio.bigtable._\nimport com.google.cloud.bigtable.config.{BigtableOptions, BulkOptions}\nimport com.google.bigtable.v2.Mutation\nimport com.google.protobuf.ByteString\n\ndef main(cmdlineArgs: Array[String]): Unit = {\n  // ...\n\n  val data: SCollection[(ByteString, Iterable[Mutation])] = ???\n\n  val btOptions =\n    BigtableOptions.builder()\n      .setProjectId(btProjectId)\n      .setInstanceId(btInstanceId)\n      .setBulkOptions(BulkOptions.builder()\n        .enableBulkMutationThrottling()\n        .setBulkMutationRpcTargetMs(10) // lower latency threshold, default is 100\n        .build())\n      .build()\n  data.saveAsBigtable(btOptions, btTableId)\n\n  // ...\n}","title":"How do I throttle Bigtable writes?"},{"location":"/FAQ.html#how-do-i-use-custom-kryo-serializers-","text":"See Kryo for more.\nDefine a registrar class that extends IKryoRegistrar and annotate it with @KryoRegistrar. Note that the class name must ends with KryoRegistrar, i.e. MyKryoRegistrar for Scio to find it.\nimport com.twitter.chill._\nimport com.esotericsoftware.kryo.Kryo\nimport com.spotify.scio.coders.KryoRegistrar\nimport com.twitter.chill.IKryoRegistrar\n\n@KryoRegistrar\nclass MyKryoRegistrar extends IKryoRegistrar {\n  override def apply(k: Kryo): Unit = {\n    // register serializers for additional classes here\n    k.forClass(new UserRecordSerializer)\n    k.forClass(new AccountRecordSerializer)\n    //...\n  }\n}\nRegistering just the classes can also improve Kryo performance. By registering, classes will be serialized as numeric IDs instead of fully qualified class names, hence saving space and network IO while shuffling. make\nimport com.twitter.chill._\nimport com.esotericsoftware.kryo.Kryo\nimport com.spotify.scio.coders.KryoRegistrar\nimport com.twitter.chill.IKryoRegistrar\n\n@KryoRegistrar\nclass MyKryoRegistrar extends IKryoRegistrar {\n  override def apply(k: Kryo): Unit = {\n    k.registerClasses(List(classOf[MyRecord1], classOf[MyRecord2]))\n  }\n}","title":"How do I use custom Kryo serializers?"},{"location":"/FAQ.html#what-kryo-tuning-options-are-there-","text":"See KryoOptions.java for a complete list of available Kryo tuning options. These can be passed via command line, for example:\n--kryoBufferSize=1024 --kryoMaxBufferSize=8192 --kryoReferenceTracking=false --kryoRegistrationRequired=true\nAmong these, --kryoRegistrationRequired=true might be useful when developing to ensure that all data types in the pipeline are registered.","title":"What Kryo tuning options are there?"},{"location":"/FAQ.html#development-environment-issues","text":"","title":"Development environment issues"},{"location":"/FAQ.html#how-do-i-keep-sbt-from-running-out-of-memory-","text":"SBT might run out of memory sometimes and show an OutOfMemoryError: Metaspace error. Override default memory setting with -mem <integer>, e.g. sbt -mem 1024.","title":"How do I keep SBT from running out of memory?"},{"location":"/FAQ.html#how-do-i-fix-sbt-heap-size-error-in-intellij-","text":"If you encounter an SBT error with message “Initial heap size set to a larger value than the maximum heap size”, that is because IntelliJ has a lower default -Xmx for SBT than -Xms in our .jvmopts. To fix that, open Preferences -> Build, Execution, Deployment -> Build Tools -> sbt, and update Maximum heap size, MB to 2048.","title":"How do I fix SBT heap size error in IntelliJ?"},{"location":"/FAQ.html#how-do-i-fix-error-in-intellij-","text":"You might get an error message like java.io.IOException: Unable to create parent directories of /Applications/IntelliJ IDEA CE.app/Contents/bin/.bigquery/012345abcdef.schema.json. This usually happens to people who run IntelliJ IDEA with its bundled JVM. There are two solutions.\nInstall JDK from java.com and switch to it by following the “All platforms: switch between installed runtimes” section in this page. Override the bigquery .cache directory as a JVM compiler parameter. On the bottom right of the IntelliJ window, click the icon that looks like a clock, and then “Configure…”. Then, edit the JVM parameters to include the line -Dbigquery.cache.directory=</path/to/repository>/.bigquery. Then, restart the compile server by clicking on the clock icon -> Stop, and then Start.","title":"How do I fix “Unable to create parent directories” error in IntelliJ?"},{"location":"/FAQ.html#how-to-make-intellij-idea-work-with-type-safe-bigquery-classes-","text":"Due to issue SCL-8834 case classes generated by @BigQueryType.fromTable or @BigQueryType.fromQuery are not recognized in IntelliJ IDEA. There are two workarounds. The first, IDEA plugin solution, is highly recommended.\nIDEA Plugin\nInside IntelliJ, Preferences -> Plugins -> Browse repositories ... and search Scio. Install the plugin, restart IntelliJ, recompile the project (use SBT or IntelliJ). You have to recompile the project each time you add/edit @BigQueryType macro. Plugin requires Scio >= 0.2.2. Documentation.\nUse case class from @BigQueryType.toTable\nFirst start Scio REPL and generate case classes from your query or table.\nimport com.spotify.scio.bigquery.types.BigQueryType\n\n@BigQueryType.fromQuery(\"SELECT tornado, month FROM [bigquery-public-data:samples.gsod]\")\nclass Tornado\nNext print Scala code of the generated classes.\nTornado.toPrettyString()\n// res12: String = \"\"\"@BigQueryType.toTable\n// case class Tornado(tornado: Option[Boolean], month: Long)\"\"\"\nYou can then paste the @BigQueryType.fromQuery code into your pipeline and use it with sc.typedBigQuery.\nimport com.spotify.scio._\nimport com.spotify.scio.values._\nimport com.spotify.scio.bigquery._\n\ndef main(cmdlineArgs: Array[String]): Unit = {\n  val (sc, args) = ContextAndArgs(cmdlineArgs)\n\n  val data: SCollection[Tornado] = sc.typedBigQuery[Tornado]\n  // ...\n}","title":"How to make IntelliJ IDEA work with type safe BigQuery classes?"},{"location":"/FAQ.html#common-issues","text":"","title":"Common issues"},{"location":"/FAQ.html#what-does-mean-","text":"Sometimes you get an error message like Cannot prove that T1 <:< T2 when saving an SCollection. This is because some sink methods have an implicit argument like this which means element type T of SCollection[T] must be a sub-type of TableRow in order to save it to BigQuery. You have to map out elements to the required type before saving.\ndef saveAsBigQuery(tableSpec: String)(implicit ev: T <:< TableRow)\nIn the case of saveAsTypedBigQuery you might get an Cannot prove that T <:< com.spotify.scio.bigquery.types.BigQueryType.HasAnnotation. error message. This API requires an SCollection[T] where T is a case class annotated with @BigQueryType.toTable. For example:\nimport com.spotify.scio._\nimport com.spotify.scio.values._\nimport com.spotify.scio.bigquery._\nimport com.spotify.scio.bigquery.types.BigQueryType\n\n@BigQueryType.toTable\ncase class Result(user: String, score: Int)\n\ndef main(cmdlineArgs: Array[String]): Unit = {\n  val (sc, args) = ContextAndArgs(cmdlineArgs)\n  val p: SCollection[(String, Int)] = ???\n\n  p.map(kv => Result(kv._1, kv._2))\n   .saveAsTypedBigQueryTable(Table.Spec(args(\"output\")))\n}\nNote Scio uses Macro Annotations and Macro Paradise plugin to implement annotations. You need to add Macro Paradise plugin to your scala compiler as described here.","title":"What does “Cannot prove that T1 <:< T2” mean?"},{"location":"/FAQ.html#how-do-i-fix-invalid-default-bigquery-credentials-","text":"If you don’t specify a secret credential file for BigQuery [1], Scio will use your default credentials (via GoogleCredential.getApplicationDefault), which:\nReturns the Application Default Credentials which are used to identify and authorize the whole application. The following are searched (in order) to find the Application Default Credentials: - Credentials file pointed to by the GOOGLE_APPLICATION_CREDENTIALS environment variable - Credentials provided by the Google Cloud SDK - gcloud auth application-default login command - Google App Engine built-in credentials - Google Cloud Shell built-in credentials - Google Compute Engine built-in credentials\nThe easiest way to configure it on your local machine is to use the gcloud auth application-default login command.\n[1] Keep in mind that you can specify your credential file via -Dbigquery.secret.","title":"How do I fix invalid default BigQuery credentials?"},{"location":"/FAQ.html#why-are-my-typed-bigquery-case-classes-not-up-to-date-","text":"Case classes generated by @BigQueryType.fromTable or other macros might not update after table schema change. To solve this problem, remove the cached BigQuery metadata by deleting the .bigquery directory in your project root. If you would rather avoid any issues resulting from caching and schema evolution entirely, you can disable caching by setting the system property bigquery.cache.enabled to false.","title":"Why are my typed BigQuery case classes not up to date?"},{"location":"/FAQ.html#how-do-i-fix-with-bigquery-","text":"BigQuery requests may sometimes timeout, i.e. for complex queries over many tables.\nexception during macro expansion:\n[error] java.net.SocketTimeoutException: Read timed out\nIt can be fixed by increasing the timeout settings (default 20s).\nsbt -Dbigquery.connect_timeout=30000 -Dbigquery.read_timeout=30000","title":"How do I fix “SocketTimeoutException” with BigQuery?"},{"location":"/FAQ.html#why-do-i-see-names-like-in-the-ui-","text":"Scio traverses JVM stack trace to figure out the proper name of each transform, i.e. flatMap@{UserAnalysis.scala:30} but may get confused if your jobs are under the com.spotify.scio package. Move them to a different package, e.g. com.spotify.analytics to fix the issue.","title":"Why do I see names like “main@{NativeMethodAccessorImpl...}” in the UI?"},{"location":"/FAQ.html#how-do-i-fix-error-","text":"You might see errors like RESOURCE_EXHAUSTED: IO error: No space left on disk in a job. They usually indicate that you have allocated insufficient local disk space to process your job. If you are running your job with default settings, your job is running on 3 workers, each with 250 GB of local disk space. Consider modifying the default settings to increase the number of workers available to your job (via --numWorkers), to increase the default disk size per worker (via --diskSizeGb).","title":"How do I fix “RESOURCE_EXHAUSTED” error?"},{"location":"/FAQ.html#can-i-use-trait-instead-of-method-","text":"Your Scio applications should define a main method instead of extending scala.App. Applications extending scala.App due to delayed initialization and closure cleaning may not work properly.","title":"Can I use “scala.App” trait instead of “main” method?"},{"location":"/FAQ.html#how-to-inspect-the-content-of-an-scollection-","text":"There is multiple options here: - Use debug() method on an SCollection to print its content as the data flows through the DAG during the execution (after the run or runAndCollect) - Use a debugger and setup break points - make sure to break inside of your functions to stop control at the execution not the pipeline construction time - In Scio-REPL, use runAndCollect() to execute the pipeline and materialize the contents of an SCollection","title":"How to inspect the content of an SCollection?"},{"location":"/FAQ.html#how-do-i-improve-side-input-performance-","text":"By default Dataflow workers allocate 100MB (see DataflowWorkerHarnessOptions#getWorkerCacheMb) of memory for caching side inputs, and falls back to disk or network. Therefore jobs with large side inputs may be slow. To override this default, register DataflowWorkerHarnessOptions before parsing command line arguments and then pass --workerCacheMb=N when submitting the job.\nimport com.spotify.scio._\nimport org.apache.beam.sdk.options.PipelineOptionsFactory\nimport org.apache.beam.runners.dataflow.options.DataflowWorkerHarnessOptions\n\ndef main(cmdlineArgs: Array[String]): Unit = {\n  PipelineOptionsFactory.register(classOf[DataflowWorkerHarnessOptions])\n  val (sc, args) = ContextAndArgs(cmdlineArgs)\n  // ...\n}","title":"How do I improve side input performance?"},{"location":"/FAQ.html#how-do-i-control-concurrency-number-of-dofn-threads-in-dataflow-workers","text":"By default Google Cloud Dataflow will use as many threads (concurrent DoFns) per worker as appropriate (precise definition is an implementation detail), in same cases you might want to control this. Use NumberOfWorkerHarnessThreads option from DataflowPipelineDebugOptions. For example to use a single thread per worker on 8 vCPU machine, simply specify 8 vCPU worker machine type, and --numberOfWorkerHarnessThreads=1 in CLI or set corresponding option in DataflowPipelineDebugOptions.","title":"How do I control concurrency (number of DoFn threads) in Dataflow workers"},{"location":"/FAQ.html#how-to-manually-investigate-a-cloud-dataflow-worker","text":"First find the VM of the worker, the easiest place is through the GCE instance groups:\ngcloud compute ssh --project=<project> --zone=<zone> <VM>\nTo find the id of batch (for batch job) container:\ndocker ps | grep \"batch\\|streaming\" | awk '{print $1}'\nTo get into the harness container:\ndocker exec -it <container-id> /bin/bash\nTo install java jdk tools:\napt-get update\napt-get install default-jdk -y\nTo find java process:\njps\nTo get GC stats:\njstat -gcutil <pid> 1000 1000\nTo get stacktrace:\njstack <pid>","title":"How to manually investigate a Cloud Dataflow worker"},{"location":"/Powered-By.html","text":"","title":"Powered By"},{"location":"/Powered-By.html#powered-by","text":"Here is a list of organizations using Scio in production.\nOrganization Use Case Code Spotify Everything including music recommendation, monetization, artist insights and business analysis. We also use BigQuery, Bigtable and Datastore heavily with Scio. Big Data Rosetta Code, Ratatool, Featran Big Viking Games Streaming event collection and ETL using Pub/Sub and BigQuery Algolia Log collection and analytics using Bigtable, Cloud Storage & Pub/sub Hypefactors Natural language processing / media monitoring. Also using PubSub, GCS and ElasticSearch with Scio. Discord Streaming event collection, sessionization, and enrichment using Pub/Sub, BigQuery, and Bigtable. Dow Jones Streaming article events, bulk article extractions and ETL using Pub/Sub, GCS and BigQuery for the DNA Platform. Honey Streaming ETL data pipeline from Pub/Sub to Pub/Sub, BigTable, BigQuery. Cabify Streaming data pipelines from Pub/Sub to BigQuery Jobrapido Streaming and Batch ETL using Pub/Sub and BigQuery 9GAG Streaming and Batch ETL using Pub/Sub Cityblock Streaming and Batch ETL using Pub/Sub, BigQuery, and Datastore Arquivei Streaming and Batch ETL using Pub/Sub, BigQuery, GCS, S3, and ElasticSearch Vpon Batch ETL and BigQuery Snowplow Analytics Streaming ETL Beam Enrich, BigQuery Loader and Cloud Storage Loader","title":"Powered By"}]}