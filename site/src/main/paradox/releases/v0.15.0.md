# v0.15.0 Release Blog

## New Features

### Managed IO support

Scio now supports [Dataflow's managed I/O](https://cloud.google.com/dataflow/docs/guides/managed-io). 
See the @ref:[Managed IO documentation](../io/Managed.md) for more details.

### Iceberg support

Scio now supports reading from and writing to [Apache Iceberg](https://iceberg.apache.org/) via Beam's @ref[Managed transforms](../io/Managed.md).
See the @ref:[Iceberg IO documentation](../io/Iceberg.md) for more details.

### New Magnolify-based APIs

0.15.0 adds magnolify-based "typed" APIs for Avro, BigTable, BigQuery, Datastore, Protobuf, and TFRecords.

For Avro, if a `magnolify.avro.AvroType[T]` can be derived for your scala type `T`, you can use `sc.typedAvroFileMagnolify` for reads and `scoll.saveAsAvroFile` for writes.

For BigTable, if a `magnolify.bigtable.BigtableType[T]` can be derived for your scala type `T`, you can use `sc.typedBigtable` for reads and `scoll.saveAsBigtable` for writes.

For BigQuery, if a `magnolify.bigquery.TableRowType[T]` can be derived for your scala type `T`, for reads you can use one of:

* `sc.typedBigQuerySelect(query)`
* `sc.typedBigQueryTable(table)`
* `sc.typedBigQueryStorageMagnolify(query)`
* `sc.typedBigQueryStorageMagnolify(table)`

And for writes you can use `scoll.saveAsBigQueryTable(table)`.

For Datastore, if a `magnolify.datastore.EntityType[T]` can be derived for your scala type `T`, you can use `sc.typedDatastore` for reads and `scoll.saveAsDatastore` for writes.

For Protobuf, if a `magnolify.protobuf.ProtobufType[T, U]` can be derived for your scala type `T` and a protobuf type `U`, you can use `sc.typedProtobufFile` for reads and `scoll.saveAsProtobufFile` for writes.

For TFRecords, if a `magnolify.tensorflow.ExampleType[T]` can be derived for your scala type `T`, you can use `sc.typedTfRecordFile` for reads and `scoll.saveAsTfRecordFile`.

The ugly naming of `typedAvroFileMagnolify` and `typedBigQueryStorageMagnolify` are intended to be easily migrated in future versions of scio when the preferred names no longer collide with the deprecated macro-based API.

### Parquet metadata

Scio now supports writing extra metadata to parquet files via the `metadata` parameter on write. For example:

```scala
scoll.saveAsTypedParquetFile(path, metadata = Map("mykey" -> "myValue"))
```

## Breaking changes

See the @ref:[Migration guide for 0.15.0](migrations/v0.15.0-Migration-Guide.md).