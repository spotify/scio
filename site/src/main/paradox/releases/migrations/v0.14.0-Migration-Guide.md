# Scio v0.14.0

## Avro removed from core

Avro coders are now a part of the `com.spotify.scio.avro` package.
```scala
import com.spotify.scio.avro._
```

Update direct usage:
```diff
- Coder.avroGenericRecordCoder(schema)
+ avroGenericRecordCoder(schema)
- Coder.avroGenericRecordCoder
+ avroGenericRecordCoder
- Coder.avroSpecificRecordCoder[T]
+ avroSpecificRecordCoder[T]
- Coder.avroSpecificFixedCoder[U]
+ avroSpecificFixedCoder[U]
```

Dynamic avro and protobuf writes are now in `com.spotify.scio.avro.dynamic`.
If using `saveAsDynamicAvroFile` or `saveAsDynamicProtobufFile`, add the following:
```scala
import com.spotify.scio.avro.dynamic._
```

Avro schemas are now in `com.spotify.scio.avro.schemas` package:
```scala
import com.spotify.scio.avro.schemas._
```

## Materialize no longer splittable

Materialize was previously implemented using an Avro wrapper around byte arrays.
To keep `materialize` in `scio-core` it has been reimplemented with `saveAsBinaryFile`, which writes a sequence of records with no sub-file blocks, and thus does not support trivially splitting the file on read.
We have found little use of materialize for large datasets that are not also saved permanently, so we expect the impact of this change to be minimal.

## New `binaryFile` read

See the relevant @scaladoc[binaryFile](com.spotify.scio.ScioContext#binaryFile(path:String,reader:com.spotify.scio.io.BinaryIO.BinaryFileReader,compression:org.apache.beam.sdk.io.Compression,emptyMatchTreatment:org.apache.beam.sdk.io.fs.EmptyMatchTreatment,suffix:String):com.spotify.scio.values.SCollection[Array[Byte]]) scaladoc and example @extref[BinaryInOut](example:BinaryInOut).
