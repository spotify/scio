<!DOCTYPE html>
<html lang="en" class="no-js">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<meta http-equiv="x-ua-compatible" content="ie=edge">
<meta name="description" content="Scio - Documentation">
<meta name="generator" content="Paradox, paradox-material-theme=0.7.0, mkdocs-material=3.0.3">

<meta name="lang:clipboard.copy" content="Copy to clipboard">
<meta name="lang:clipboard.copied" content="Copied to clipboard">
<meta name="lang:search.language" content="">
<meta name="lang:search.pipeline.stopwords" content="true">
<meta name="lang:search.pipeline.trimmer" content="true">
<meta name="lang:search.result.none" content="No matching documents">
<meta name="lang:search.result.one" content="1 matching document">
<meta name="lang:search.result.other" content="# matching documents">
<meta name="lang:search.tokenizer" content="[\s\-]+">


<meta name="description" content="Scio - Documentation">
<link rel="shortcut icon" href="../images/favicon.ico">
<title>Parquet Â· Scio</title>
<link rel="stylesheet" href="../assets/stylesheets/application.451f80e5.css">
<link rel="stylesheet" href="../assets/stylesheets/application-palette.22915126.css">
<link rel="stylesheet" href="../lib/material__tabs/dist/mdc.tabs.min.css">
<link rel="stylesheet" href="../lib/prettify/prettify.css">
<script src="../assets/javascripts/modernizr.1aa3b519.js"></script>
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700|Roboto+Mono">
<style>
body,input{font-family:"Roboto","Helvetica Neue",Helvetica,Arial,sans-serif}
code,kbd,pre{font-family:"Roboto Mono","Courier New",Courier,monospace}
</style>
<link rel="stylesheet" href="../assets/fonts/font-awesome.css">
<link rel="stylesheet" href="../assets/fonts/material-icons.css">
<link rel="stylesheet" href="../assets/stylesheets/paradox-material-theme.css">
</head>
<body
data-md-color-primary="white"
data-md-color-accent="indigo"
>
<input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
<input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
<label class="md-overlay" data-md-component="overlay" for="__drawer"></label>
<header class="md-header" data-md-component="header">
<nav class="md-header-nav md-grid">
<div class="md-flex">
<div class="md-flex__cell md-flex__cell--shrink">
<a href="../index.html" title="Scio" class="md-header-nav__button md-logo">
<img src="../images/logo.png" width="24" height="24">
</a>
</div>
<div class="md-flex__cell md-flex__cell--shrink">
<label class="md-icon md-icon--menu md-header-nav__button" for="__drawer"></label>
</div>
<div class="md-flex__cell md-flex__cell--stretch">
<div class="md-flex__ellipsis md-header-nav__title" data-md-component="title">
<span class="md-header-nav__topic">
Scio
</span>
<span class="md-header-nav__topic">
Parquet
</span>
</div>
</div>
<div class="md-flex__cell md-flex__cell--shrink">
<label class="md-icon md-icon--search md-header-nav__button" for="__search"></label>
<div class="md-search" data-md-component="search" role="dialog">
<label class="md-search__overlay" for="__search"></label>
<div class="md-search__inner" role="search">
<form class="md-search__form" name="search">
<input type="text" class="md-search__input" name="query" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="query" data-md-state="active">
<label class="md-icon md-search__icon" for="__search"></label>
<button type="reset" class="md-icon md-search__icon" data-md-component="reset" tabindex="-1">&#xE5CD;</button>
</form>
<div class="md-search__output">
<div class="md-search__scrollwrap" data-md-scrollfix>
<div class="md-search-result" data-md-component="result">
<div class="md-search-result__meta">
Type to start searching
</div>
<ol class="md-search-result__list"></ol>
</div>
</div>
</div>
</div>
</div>

</div>
<div class="md-flex__cell md-flex__cell--shrink">
<div class="md-header-nav__source">
<a href="https://github.com/spotify/scio"
title="Go to repository"
class="md-source"
data-md-source="github">
<div class="md-source__icon">
<i class="fa fa-github"></i>
</div>
<div class="md-source__repository">
spotify/scio
</div>
</a>

</div>
</div>
</div>
</nav>
</header>

<div class="md-container">
<main class="md-main">
<div class="md-main__inner md-grid" data-md-component="container">
<div class="md-sidebar md-sidebar--primary" data-md-component="navigation">
<div class="md-sidebar__scrollwrap">
<div class="md-sidebar__inner">
<nav class="md-nav md-nav--primary" data-md-level="0" style="visibility: hidden">
<label class="md-nav__title md-nav__title--site" for="drawer">
<a href="../index.html" title="Scio" class="md-nav__button md-logo">
<span class="md-nav__button md-logo">
<img src="../images/logo.png" width="24" height="24">
</a>
<a href="../index.html" title="Scio">
Scio
</a>
</label>
<div class="md-nav__source">
<a href="https://github.com/spotify/scio"
title="Go to repository"
class="md-source"
data-md-source="github">
<div class="md-source__icon">
<i class="fa fa-github"></i>
</div>
<div class="md-source__repository">
spotify/scio
</div>
</a>

</div>
<ul>
  <li><a href="../Getting-Started.html" class="page">Getting Started</a></li>
  <li><a href="../Builtin.html" class="page">Built-in Functionality</a></li>
  <li><a href="../Joins.html" class="page">Joins</a></li>
  <li><a href="../SideInputs.html" class="page">Side Inputs</a></li>
  <li><a href="../io/index.html" class="page">IO</a>
  <ul>
    <li><a href="../io/Avro.html" class="page">Avro</a></li>
    <li><a href="../io/Binary.html" class="page">Binary</a></li>
    <li><a href="../io/BigQuery.html" class="page">BigQuery</a></li>
    <li><a href="../io/Bigtable.html" class="page">Bigtable</a></li>
    <li><a href="../io/Cassandra.html" class="page">Cassandra</a></li>
    <li><a href="../io/Csv.html" class="page">CSV</a></li>
    <li><a href="../io/Datastore.html" class="page">Datastore</a></li>
    <li><a href="../io/Grpc.html" class="page">GRPC</a></li>
    <li><a href="../io/Elasticsearch.html" class="page">Elasticsearch</a></li>
    <li><a href="../io/Jdbc.html" class="page">JDBC</a></li>
    <li><a href="../io/Json.html" class="page">Json</a></li>
    <li><a href="../io/Neo4J.html" class="page">Neo4J</a></li>
    <li><a href="../io/Object.html" class="page">Object file</a></li>
    <li><a href="../io/Parquet.html" class="active page">Parquet</a></li>
    <li><a href="../io/Protobuf.html" class="page">Protobuf</a></li>
    <li><a href="../io/Pubsub.html" class="page">PubSub</a></li>
    <li><a href="../io/ReadFiles.html" class="page">ReadFiles</a></li>
    <li><a href="../io/Redis.html" class="page">Redis</a></li>
    <li><a href="../io/Spanner.html" class="page">Spanner</a></li>
    <li><a href="../io/Tensorflow.html" class="page">Tensorflow</a></li>
    <li><a href="../io/Text.html" class="page">Text</a></li>
  </ul></li>
  <li><a href="../examples.html" class="page">Examples</a></li>
  <li><a href="../Scio-Unit-Tests.html" class="page">Testing</a></li>
  <li><a href="../internals/index.html" class="page">Internals</a>
  <ul>
    <li><a href="../internals/Coders.html" class="page">Coder Typeclass</a></li>
    <li><a href="../internals/Kryo.html" class="page">Kryo</a></li>
    <li><a href="../internals/OverrideTypeProvider.html" class="page">OverrideTypeProvider</a></li>
    <li><a href="../internals/ScioIO.html" class="page">ScioIO</a></li>
  </ul></li>
  <li><a href="../extras/index.html" class="page">Extras</a>
  <ul>
    <li><a href="../extras/Algebird.html" class="page">Algebird</a></li>
    <li><a href="../extras/Annoy.html" class="page">Annoy</a></li>
    <li><a href="../extras/AsyncDoFn.html" class="page">AsyncDoFn</a></li>
    <li><a href="../extras/BigQueryAvro.html" class="page">BigQueryAvro</a></li>
    <li><a href="../extras/DistCache.html" class="page">DistCache</a></li>
    <li><a href="../extras/Fanout.html" class="page">Fanout</a></li>
    <li><a href="../extras/HyperLogLog.html" class="page">HyperLogLog</a></li>
    <li><a href="../extras/MutableScalableBloomFilter.html" class="page">MutableScalableBloomFilter</a></li>
    <li><a href="../extras/Sorter.html" class="page">Sorter</a></li>
    <li><a href="../extras/Sort-Merge-Bucket.html" class="page">Sort Merge Bucket</a></li>
    <li><a href="../extras/Sparkey.html" class="page">Sparkey</a></li>
    <li><a href="../extras/Scio-REPL.html" class="page">REPL</a></li>
    <li><a href="../extras/Transforms.html" class="page">Transforms</a></li>
    <li><a href="../extras/Voyager.html" class="page">Voyager</a></li>
  </ul></li>
  <li><a href="../dev/index.html" class="page">Development</a>
  <ul>
    <li><a href="../dev/build.html" class="page">Build</a></li>
    <li><a href="../dev/Style-Guide.html" class="page">Style Guide</a></li>
    <li><a href="../dev/How-to-Release.html" class="page">How to Release</a></li>
    <li><a href="../dev/Design-Philosophy.html" class="page">Design Philosophy</a></li>
  </ul></li>
  <li><a href="../scaladoc.html" class="page">Scaladoc</a></li>
  <li><a href="../Scio,-Scalding-and-Spark.html" class="page">Scio, Spark and Scalding</a></li>
  <li><a href="../Runners.html" class="page">Runners</a></li>
  <li><a href="../Scio-data-guideline.html" class="page">Data Guidelines</a></li>
  <li><a href="../releases/index.html" class="page">Releases</a>
  <ul>
    <li><a href="../releases/Apache-Beam.html" class="page">Apache Beam Compatibility</a></li>
    <li><a href="../releases/migrations/index.html" class="page">Migration Guides</a></li>
    <li><a href="../releases/breaking-changes.html" class="page">Breaking Changelog</a></li>
    <li><a href="../releases/v0.12.0.html" class="page">v0.12.0 Release Blog</a></li>
  </ul></li>
  <li><a href="../FAQ.html" class="page">FAQ</a></li>
</ul>
<nav class="md-nav md-nav--secondary">
<label class="md-nav__title" for="__toc">Table of contents</label>
<ul>
  <li><a href="../io/Parquet.html#parquet" class="header">Parquet</a>
  <ul>
    <li><a href="../io/Parquet.html#avro" class="header">Avro</a></li>
    <li><a href="../io/Parquet.html#case-classes" class="header">Case classes</a></li>
    <li><a href="../io/Parquet.html#configuring-parquet" class="header">Configuring Parquet</a></li>
    <li><a href="../io/Parquet.html#parquet-reads-in-scio-0-12-0-" class="header">Parquet Reads in Scio 0.12.0+</a></li>
    <li><a href="../io/Parquet.html#testing" class="header">Testing</a></li>
  </ul></li>
</ul>
</nav>

</nav>
<ul style="display: none">
<li class="md-nav__item md-version" id="project.version">
<label class="md-nav__link" for="__version">
<i class="md-icon" title="Version">label_outline</i> 0.14.11-4-9e91a20-20250207T190641Z*
</label>
</li>
</ul>
</div>
</div>
</div>
<div class="md-sidebar md-sidebar--secondary" data-md-component="toc">
<div class="md-sidebar__scrollwrap">
<div class="md-sidebar__inner">
<nav class="md-nav md-nav--secondary">
<label class="md-nav__title" for="__toc">Table of contents</label>
<ul>
  <li><a href="../io/Parquet.html#parquet" class="header">Parquet</a>
  <ul>
    <li><a href="../io/Parquet.html#avro" class="header">Avro</a></li>
    <li><a href="../io/Parquet.html#case-classes" class="header">Case classes</a></li>
    <li><a href="../io/Parquet.html#configuring-parquet" class="header">Configuring Parquet</a></li>
    <li><a href="../io/Parquet.html#parquet-reads-in-scio-0-12-0-" class="header">Parquet Reads in Scio 0.12.0+</a></li>
    <li><a href="../io/Parquet.html#testing" class="header">Testing</a></li>
  </ul></li>
</ul>
</nav>

</div>
</div>
</div>
<div class="md-content">
<article class="md-content__inner md-typeset">
<div class="md-content__searchable">
<h1><a href="#parquet" name="parquet" class="anchor"><span class="anchor-link"></span></a>Parquet</h1>
<p>Scio supports reading and writing <a href="https://parquet.apache.org/">Parquet</a> files as Avro records or Scala case classes. Also see <a href="./Avro.html">Avro</a> page on reading and writing regular Avro files.</p>
<h2><a href="#avro" name="avro" class="anchor"><span class="anchor-link"></span></a>Avro</h2>
<h3><a href="#read-parquet-files-as-avro" name="read-parquet-files-as-avro" class="anchor"><span class="anchor-link"></span></a>Read Parquet files as Avro</h3>
<p>When reading Parquet as Avro specific records, one can use <a href="https://github.com/nevillelyh/parquet-extra">parquet-extra</a> macros for generating column projections and row predicates using idiomatic Scala syntax. To read a Parquet file as Avro specific record with column projections and row predicates:</p>
<pre class="prettyprint"><code class="language-scala">import com.spotify.scio._
import com.spotify.scio.parquet.avro._
import com.spotify.scio.avro.TestRecord

object ParquetJob {
  def main(cmdlineArgs: Array[String]): Unit = {

    val (sc, args) = ContextAndArgs(cmdlineArgs)

    // Macros for generating column projections and row predicates
    val projection = Projection[TestRecord](_.getIntField, _.getLongField, _.getBooleanField)
    val predicate = Predicate[TestRecord](x =&gt; x.getIntField &gt; 0 &amp;&amp; x.getBooleanField)

    sc.parquetAvroFile[TestRecord](&quot;input.parquet&quot;, projection, predicate)
      // Map out projected fields right after reading
      .map(r =&gt; (r.getIntField, r.getStringField, r.getBooleanField))

    sc.run()
    ()
  }
}
</code></pre>
<p>Note that the result <code>TestRecord</code>s are not complete Avro objects. Only the projected columns (<code>intField</code>, <code>stringField</code>, <code>booleanField</code>) are present while the rest are null. These objects may fail serialization and it&rsquo;s recommended that you map them out to tuples or case classes right after reading.</p>
<p>Also note that <code>predicate</code> logic is only applied when reading actual Parquet files but not in <code>JobTest</code>. To retain the filter behavior while using mock input, it&rsquo;s recommend that you do the following.</p>
<pre class="prettyprint"><code class="language-scala">import com.spotify.scio._
import com.spotify.scio.parquet.avro._
import com.spotify.scio.avro.TestRecord

object ParquetJob {
  def main(cmdlineArgs: Array[String]): Unit = {

    val (sc, args) = ContextAndArgs(cmdlineArgs)

    val projection = Projection[TestRecord](_.getIntField, _.getLongField, _.getBooleanField)
    // Build both native filter function and Parquet FilterPredicate
    // case class Predicates[T](native: T =&gt; Boolean, parquet: FilterPredicate)
    val predicate = Predicate.build[TestRecord](x =&gt; x.getIntField &gt; 0 &amp;&amp; x.getBooleanField)

    sc.parquetAvroFile[TestRecord](&quot;input.parquet&quot;, projection, predicate.parquet)
      // filter natively with the same logic in case of mock input in `JobTest`
      .filter(predicate.native)

    sc.run()
    ()
  }
}
</code></pre>
<p>You can also read Avro generic records by specifying a reader schema.</p>
<pre class="prettyprint"><code class="language-scala mdoc:reset:silent">import com.spotify.scio._
import com.spotify.scio.parquet.avro._
import com.spotify.scio.avro.TestRecord
import org.apache.avro.generic.GenericRecord

object ParquetJob {
  def main(cmdlineArgs: Array[String]): Unit = {

    val (sc, args) = ContextAndArgs(cmdlineArgs)

    sc.parquetAvroFile[GenericRecord](&quot;input.parquet&quot;, TestRecord.getClassSchema)
      // Map out projected fields into something type safe
      .map(r =&gt; (r.get(&quot;int_field&quot;).asInstanceOf[Int], r.get(&quot;string_field&quot;).toString))

    sc.run()
    ()
  }
}
</code></pre>
<h3><a href="#write-avro-to-parquet-files" name="write-avro-to-parquet-files" class="anchor"><span class="anchor-link"></span></a>Write Avro to Parquet files</h3>
<p>Both Avro <a href="https://avro.apache.org/docs/current/api/java/org/apache/avro/generic/GenericData.Record.html">generic</a> and <a href="https://avro.apache.org/docs/current/api/java/org/apache/avro/specific/package-summary.html">specific</a> records are supported when writing.</p>
<p>Type of Avro specific records will hold information about schema, therefore Scio will figure out the schema by itself:</p>
<pre class="prettyprint"><code class="language-scala mdoc:reset:silent">import com.spotify.scio.values._
import com.spotify.scio.parquet.avro._
import com.spotify.scio.avro.TestRecord

def input: SCollection[TestRecord] = ???
def result = input.saveAsParquetAvroFile(&quot;gs://path-to-data/lake/output&quot;)
</code></pre>
<p>Writing Avro generic records requires additional argument <code>schema</code>:</p>
<pre class="prettyprint"><code class="language-scala mdoc:reset:silent">import com.spotify.scio.values._
import com.spotify.scio.coders.Coder
import com.spotify.scio.avro._
import com.spotify.scio.parquet.avro._
import org.apache.avro.generic.GenericRecord

def input: SCollection[GenericRecord] = ???
lazy val yourAvroSchema: org.apache.avro.Schema = ???
implicit lazy val coder: Coder[GenericRecord] = avroGenericRecordCoder(yourAvroSchema)

def result = input.saveAsParquetAvroFile(&quot;gs://path-to-data/lake/output&quot;, schema = yourAvroSchema)
</code></pre>
<h3><a href="#logical-types" name="logical-types" class="anchor"><span class="anchor-link"></span></a>Logical Types</h3>
<p>As of <strong>Scio 0.14.0</strong> and above, Scio supports specific record logical types in parquet-avro out of the box.</p>
<p>When using generic record you&rsquo;ll need to supply the additional Configuration parameter <code>AvroReadSupport.AVRO_DATA_SUPPLIER</code> for reads or <code>AvroWriteSupport.AVRO_DATA_SUPPLIER</code> for writes to use logical types.</p>
<pre class="prettyprint"><code class="language-scala mdoc:compile-only">import com.spotify.scio._
import com.spotify.scio.avro._
import com.spotify.scio.coders.Coder
import com.spotify.scio.parquet.avro._
import com.spotify.scio.parquet.ParquetConfiguration
import com.spotify.scio.values.SCollection
import org.apache.avro.Conversions
import org.apache.avro.generic.GenericRecord
import org.apache.avro.data.TimeConversions
import org.apache.avro.generic.GenericData
import org.apache.parquet.avro.{AvroDataSupplier, AvroReadSupport, AvroWriteSupport}

val sc: ScioContext = ???
implicit val coder: Coder[GenericRecord] = ???
val data: SCollection[GenericRecord] = ???

class AvroLogicalTypeSupplier extends AvroDataSupplier {
  override def get(): GenericData = {
    val data = GenericData.get()

    // Add conversions as needed
    data.addLogicalTypeConversion(new TimeConversions.TimestampMillisConversion())

    data
  }
}

// Reads
sc.parquetAvroFile(
  &quot;somePath&quot;,
  conf = ParquetConfiguration.of(AvroReadSupport.AVRO_DATA_SUPPLIER -&gt; classOf[AvroLogicalTypeSupplier])
)

// Writes
data.saveAsParquetAvroFile(
  &quot;somePath&quot;,
  conf = ParquetConfiguration.of(AvroWriteSupport.AVRO_DATA_SUPPLIER -&gt; classOf[AvroLogicalTypeSupplier])
)
</code></pre>
<h2><a href="#case-classes" name="case-classes" class="anchor"><span class="anchor-link"></span></a>Case classes</h2>
<p>Scio uses <a href="https://github.com/spotify/magnolify/blob/master/docs/parquet.md">magnolify-parquet</a> to derive Parquet reader and writer for case classes at compile time, similar to how <a href="../internals/Coders.html">coders</a> work. See this <a href="https://github.com/spotify/magnolify/blob/master/docs/mapping.md">mapping table</a> for how Scala and Parquet types map; enum type mapping is also specifically <a href="https://github.com/spotify/magnolify/blob/main/docs/enums.md">documented</a>.</p>
<h3><a href="#read-parquet-files-as-case-classes" name="read-parquet-files-as-case-classes" class="anchor"><span class="anchor-link"></span></a>Read Parquet files as case classes</h3>
<p>When reading Parquet files as case classes, all fields in the case class definition are read. Therefore, it&rsquo;s desirable to construct a case class type with only fields needed for processing.</p>
<p>Starting in Magnolify 0.4.8 (corresponding to Scio 0.11.6 and above), predicates for case classes have Magnolify support at the <em>field level only</em>. You can use Parquet&rsquo;s <code>FilterApi.or</code> and <code>FilterApi.and</code> to chain them:</p>
<pre class="prettyprint"><code class="language-scala mdoc:reset:silent">import com.spotify.scio._
import com.spotify.scio.parquet.types._
import magnolify.parquet._
import org.apache.parquet.filter2.predicate.FilterApi

object ParquetJob {
  case class MyRecord(int_field: Int, string_field: String)

  def main(cmdlineArgs: Array[String]): Unit = {

    val (sc, args) = ContextAndArgs(cmdlineArgs)

    sc.typedParquetFile[MyRecord](&quot;input.parquet&quot;, predicate = FilterApi.and(
      Predicate.onField[String](&quot;string_field&quot;)(_.startsWith(&quot;a&quot;)),
      Predicate.onField[Int](&quot;int_field&quot;)(_ % 2 == 0))
    )

    sc.run()
    ()
  }
}
</code></pre>
<h3><a href="#write-case-classes-to-parquet-files" name="write-case-classes-to-parquet-files" class="anchor"><span class="anchor-link"></span></a>Write case classes to Parquet files</h3>
<p>When writing case classes as Parquet files, the schema is derived from the case class and all fields are written.</p>
<pre class="prettyprint"><code class="language-scala mdoc:reset:silent">import com.spotify.scio.values._
import com.spotify.scio.parquet.types._

case class MyRecord(int_field: Int, string_field: String)
def input: SCollection[MyRecord] = ???

def result = input.saveAsTypedParquetFile(&quot;gs://path-to-data/lake/output&quot;)
</code></pre>
<h3><a href="#compatibility" name="compatibility" class="anchor"><span class="anchor-link"></span></a>Compatibility</h3>
<p>Note that Parquet writes Avro <code>array</code> fields differently than most other Parquet submodules. For example <code>my_field: List[Int]</code> would normally map to something like this:</p>
<pre><code>repeated int32 my_field;
</code></pre>
<p>While <code>parquet-avro</code> would map it to this:</p>
<pre><code>required group my_field {
  repeated int32 array;
}
</code></pre>
<p>Add the following import to handle typed Parquet in a way compatible with Parquet Avro:</p>
<pre class="prettyprint"><code class="language-scala">import magnolify.parquet.ParquetArray.AvroCompat._
</code></pre>
<p>The same Avro schema evolution principles apply to Parquet, i.e. only append <code>OPTIONAL</code> or <code>REPEATED</code> fields with default <code>null</code> or <code>[]</code>. See this <a href="https://github.com/spotify/magnolify/blob/main/parquet/src/test/scala/magnolify/parquet/SchemaEvolutionSuite.scala">test</a> for some common scenarios w.r.t. Parquet schema evolution.</p>
<h2><a href="#configuring-parquet" name="configuring-parquet" class="anchor"><span class="anchor-link"></span></a>Configuring Parquet</h2>
<p>The Parquet Java <a href="https://github.com/apache/parquet-mr">library</a> heavily relies on Hadoop&rsquo;s <code>Job</code> API. Therefore, in both the Parquet library and in scio-parquet, we use Hadoop&rsquo;s <a href="https://hadoop.apache.org/docs/r2.10.2/api/org/apache/hadoop/conf/Configuration.html">Configuration</a> class to manage most Parquet read and write options.</p>
<p>The <code>Configuration</code> class, when initialized, will load default values from the first available <code>core-site.xml</code> found on the classpath. Scio-parquet provides a default <a href="https://github.com/spotify/scio/blob/main/scio-parquet/src/main/resources/core-site.xml"><code>core-site.xml</code> implementation</a>: if your Scio pipeline has a dependency on <code>scio-parquet</code>, these default options will be picked up in your pipeline.</p>
<h3><a href="#overriding-the-default-configuration" name="overriding-the-default-configuration" class="anchor"><span class="anchor-link"></span></a>Overriding the Default Configuration</h3>
<p>You can override the default configuration in two ways:</p>
<ol>
  <li>
  <p>Declare a <code>core-site.xml</code> file of your own in your project&rsquo;s <code>src/main/resources</code> folder. Note that Hadoop can only pick one <code>core-site.xml</code> to read: if you override the file in your project, Hadoop will not read Scio&rsquo;s default <code>core-site.xml</code> at all, and none of its default options will be loaded.</p></li>
  <li>
  <p>Create an in-memory <code>Configuration</code> object for use with scio-parquet&rsquo;s <code>ReadParam</code> and <code>WriteParam</code>. Any options provided this way will be <em>appended</em> to Scio&rsquo;s default configuration.</p></li>
</ol>
<p>You can create and pass in a custom Configuration using our <code>ParquetConfiguration</code> helper, available in Scio 0.12.x and above:</p>
<pre class="prettyprint"><code class="language-scala">import com.spotify.scio.parquet.ParquetConfiguration

data
  .saveAsParquetAvroFile(args(&quot;output&quot;), conf = ParquetConfiguration.of(&quot;parquet.block.size&quot; -&gt; 536870912))
</code></pre>
<p>If you&rsquo;re on Scio 0.11.x or below, you&rsquo;ll have to create a <code>Configuration</code> object directly:</p>
<pre class="prettyprint"><code class="language-scala">import org.apache.hadoop.conf.Configuration

val parquetConf: Configuration = {
  val conf: Configuration = new Configuration()
  conf.setInt(&quot;parquet.block.size&quot;, 536870912)
  conf
}
</code></pre>
<h3><a href="#common-configuration-options" name="common-configuration-options" class="anchor"><span class="anchor-link"></span></a>Common Configuration Options</h3>
<ul>
  <li><code>parquet.block.size</code> - This determines block size for HDFS and row group size. 1 GiB is recommended over the default 128 MiB, although you&rsquo;ll have to weigh the tradeoffs: a larger block size means fewer seek operations on blob storage, at the cost of having to load a larger row group into memory.</li>
  <li><code>fs.gs.inputstream.fadvise</code> - &ldquo;Fadvise allows applications to provide a hint to the Linux kernel with the intended I/O access pattern, indicating how it intends to read a file, whether for sequential scans or random seeks.&rdquo; According to this <a href="https://cloud.google.com/blog/products/data-analytics/new-release-of-cloud-storage-connector-for-hadoop-improving-performance-throughput-and-more">blog post</a> &ldquo;traditional MapReduce jobs&rdquo; are ideal use cases for setting this config to <code>SEQUENTIAL</code>, and Scio jobs fit in this category.</li>
</ul>
<p>Here are some other recommended settings.</p>
<ul>
  <li><code>numShards</code> - This should be explicitly set so that the size of each output file is smaller than but close to <code>parquet.block.size</code>, i.e. 1 GiB. This guarantees that each file contains 1 row group only and reduces seeks.</li>
  <li><code>compression</code> - Parquet defaults to ZSTD compression with a level of 3; compression level can be set to any integer from 1-22 using the configuration option <code>parquet.compression.codec.zstd.level</code>. <code>SNAPPY</code> and <code>GZIP</code> compression types also work out of the box; Snappy is less CPU intensive but has lower compression ratio. In our benchmarks GZIP seem to work better than Snappy on GCS.</li>
</ul>
<p>A full list of Parquet configuration options can be found <a href="https://github.com/apache/parquet-mr/blob/master/parquet-hadoop/README.md">here</a>.</p>
<h2><a href="#parquet-reads-in-scio-0-12-0-" name="parquet-reads-in-scio-0-12-0-" class="anchor"><span class="anchor-link"></span></a>Parquet Reads in Scio 0.12.0+</h2>
<p>Parquet read internals have been reworked in Scio 0.12.0. As of 0.12.0, you can opt-into the new Parquet read implementation, backed by the new Beam <a href="https://beam.apache.org/blog/splittable-do-fn/">SplittableDoFn</a> API, by following the instructions <a href="../releases/migrations/v0.12.0-Migration-Guide.html#parquet-reads">here</a>.</p>
<h2><a href="#testing" name="testing" class="anchor"><span class="anchor-link"></span></a>Testing</h2>
<p>In addition to JobTest support for Avro, Typed, and Tensorflow models, Scio 0.14.5 and above include utilities for testing projections and predicates. Just import the desired module, <code>com.spotify.scio.testing.parquet.{avro|types|tensorflow}._</code>, from the <code>scio-test-parquet</code> artifact. For example, test utilities for Avro are available in <code>com.spotify.scio.testing.parquet.avro._</code>:</p>
<pre class="prettyprint"><code class="language-scala">import com.spotify.scio.testing.parquet.avro._

val projection: Schema = ???
val predicate: FilterPredicate = ???

val records: Iterable[T &lt;: SpecificRecord] = ???
val expected: Iterable[T &lt;: SpecificRecord] = ???

records withProjection projection withPredicate predicate should contain theSameElementsAs expected
</code></pre>
<p>You can also test a case class projection against an Avro writer schema to ensure writer/reader compatibility:</p>
<pre class="prettyprint"><code class="language-scala">import com.spotify.scio.testing.parquet.avro._

case class MyProjection(id: Int)
val records: Iterable[T &lt;: SpecificRecord] = ???
val expected: Iterable[MyRecord] = ???

records withProjection[MyProjection] should contain theSameElementsAs expected
</code></pre>
</div>
<div>
<a href="https://github.com/spotify/scio/tree/master/site/src/main/paradox/io/Parquet.md" title="Edit this page" class="md-source-file md-edit">
Edit this page
</a>
</div>
<div class="print-only">
<span class="md-source-file md-version">
0.14.11-4-9e91a20-20250207T190641Z*
</span>
</div>
</article>
</div>
</div>
</main>
<footer class="md-footer">
<div class="md-footer-nav">
<nav class="md-footer-nav__inner md-grid">
<a href="../io/Object.html" title="Object file" class="md-flex md-footer-nav__link md-footer-nav__link--prev" rel="prev">
<div class="md-flex__cell md-flex__cell--shrink">
<i class="md-icon md-icon--arrow-back md-footer-nav__button"></i>
</div>
<div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
<span class="md-flex__ellipsis">
<span class="md-footer-nav__direction">
Previous
</span>
Object file
</span>
</div>
</a>
<a href="../io/Protobuf.html" title="Protobuf" class="md-flex md-footer-nav__link md-footer-nav__link--next" rel="next">
<div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
<span class="md-flex__ellipsis">
<span class="md-footer-nav__direction">
Next
</span>
Protobuf
</span>
</div>
<div class="md-flex__cell md-flex__cell--shrink">
<i class="md-icon md-icon--arrow-forward md-footer-nav__button"></i>
</div>
</a>
</nav>
</div>
<div class="md-footer-meta md-typeset">
<div class="md-footer-meta__inner md-grid">
<div class="md-footer-copyright">
<div class="md-footer-copyright__highlight">
Copyright (C) 2025 Spotify AB
</div>
Powered by
<a href="https://github.com/lightbend/paradox">Paradox</a>
and
<a href="https://sbt.github.io/sbt-paradox-material-theme/">Paradox Material Theme</a>

</div>
<div class="md-footer-social">
<a href="https://github.com/spotify" class="md-footer-social__link fa fa-github"></a><a href="https://twitter.com/spotifyeng" class="md-footer-social__link fa fa-twitter"></a>
</div>

</div>
</div>
</footer>

</div>
<script src="../assets/javascripts/application.583bbe55.js"></script>
<script src="../assets/javascripts/paradox-material-theme.js"></script>
<script>app.initialize({version:"0.17",url:{base:"../."}})</script>
<script type="text/javascript" src="../lib/prettify/prettify.js"></script>
<script type="text/javascript" src="../lib/prettify/lang-scala.js"></script>
<script type="text/javascript">
document.addEventListener("DOMContentLoaded", function(event) {
window.prettyPrint && prettyPrint();
});
</script>
</body>
</html>
