<!DOCTYPE html>
<html lang="en" class="no-js">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<meta http-equiv="x-ua-compatible" content="ie=edge">
<meta name="description" content="Scio - Documentation">
<meta name="generator" content="Paradox, paradox-material-theme=0.7.0, mkdocs-material=3.0.3">

<meta name="lang:clipboard.copy" content="Copy to clipboard">
<meta name="lang:clipboard.copied" content="Copied to clipboard">
<meta name="lang:search.language" content="">
<meta name="lang:search.pipeline.stopwords" content="true">
<meta name="lang:search.pipeline.trimmer" content="true">
<meta name="lang:search.result.none" content="No matching documents">
<meta name="lang:search.result.one" content="1 matching document">
<meta name="lang:search.result.other" content="# matching documents">
<meta name="lang:search.tokenizer" content="[\s\-]+">


<meta name="description" content="Scio - Documentation">
<link rel="shortcut icon" href="../images/favicon.ico">
<title>Sort Merge Bucket Â· Scio</title>
<link rel="stylesheet" href="../assets/stylesheets/application.451f80e5.css">
<link rel="stylesheet" href="../assets/stylesheets/application-palette.22915126.css">
<link rel="stylesheet" href="../lib/material__tabs/dist/mdc.tabs.min.css">
<link rel="stylesheet" href="../lib/prettify/prettify.css">
<script src="../assets/javascripts/modernizr.1aa3b519.js"></script>
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700|Roboto+Mono">
<style>
body,input{font-family:"Roboto","Helvetica Neue",Helvetica,Arial,sans-serif}
code,kbd,pre{font-family:"Roboto Mono","Courier New",Courier,monospace}
</style>
<link rel="stylesheet" href="../assets/fonts/font-awesome.css">
<link rel="stylesheet" href="../assets/fonts/material-icons.css">
<link rel="stylesheet" href="../assets/stylesheets/paradox-material-theme.css">
</head>
<body
data-md-color-primary="white"
data-md-color-accent="indigo"
>
<input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
<input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
<label class="md-overlay" data-md-component="overlay" for="__drawer"></label>
<header class="md-header" data-md-component="header">
<nav class="md-header-nav md-grid">
<div class="md-flex">
<div class="md-flex__cell md-flex__cell--shrink">
<a href="../index.html" title="Scio" class="md-header-nav__button md-logo">
<img src="../images/logo.png" width="24" height="24">
</a>
</div>
<div class="md-flex__cell md-flex__cell--shrink">
<label class="md-icon md-icon--menu md-header-nav__button" for="__drawer"></label>
</div>
<div class="md-flex__cell md-flex__cell--stretch">
<div class="md-flex__ellipsis md-header-nav__title" data-md-component="title">
<span class="md-header-nav__topic">
Scio
</span>
<span class="md-header-nav__topic">
Sort Merge Bucket
</span>
</div>
</div>
<div class="md-flex__cell md-flex__cell--shrink">
<label class="md-icon md-icon--search md-header-nav__button" for="__search"></label>
<div class="md-search" data-md-component="search" role="dialog">
<label class="md-search__overlay" for="__search"></label>
<div class="md-search__inner" role="search">
<form class="md-search__form" name="search">
<input type="text" class="md-search__input" name="query" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="query" data-md-state="active">
<label class="md-icon md-search__icon" for="__search"></label>
<button type="reset" class="md-icon md-search__icon" data-md-component="reset" tabindex="-1">&#xE5CD;</button>
</form>
<div class="md-search__output">
<div class="md-search__scrollwrap" data-md-scrollfix>
<div class="md-search-result" data-md-component="result">
<div class="md-search-result__meta">
Type to start searching
</div>
<ol class="md-search-result__list"></ol>
</div>
</div>
</div>
</div>
</div>

</div>
<div class="md-flex__cell md-flex__cell--shrink">
<div class="md-header-nav__source">
<a href="https://github.com/spotify/scio"
title="Go to repository"
class="md-source"
data-md-source="github">
<div class="md-source__icon">
<i class="fa fa-github"></i>
</div>
<div class="md-source__repository">
spotify/scio
</div>
</a>

</div>
</div>
</div>
</nav>
</header>

<div class="md-container">
<main class="md-main">
<div class="md-main__inner md-grid" data-md-component="container">
<div class="md-sidebar md-sidebar--primary" data-md-component="navigation">
<div class="md-sidebar__scrollwrap">
<div class="md-sidebar__inner">
<nav class="md-nav md-nav--primary" data-md-level="0" style="visibility: hidden">
<label class="md-nav__title md-nav__title--site" for="drawer">
<a href="../index.html" title="Scio" class="md-nav__button md-logo">
<span class="md-nav__button md-logo">
<img src="../images/logo.png" width="24" height="24">
</a>
<a href="../index.html" title="Scio">
Scio
</a>
</label>
<div class="md-nav__source">
<a href="https://github.com/spotify/scio"
title="Go to repository"
class="md-source"
data-md-source="github">
<div class="md-source__icon">
<i class="fa fa-github"></i>
</div>
<div class="md-source__repository">
spotify/scio
</div>
</a>

</div>
<ul>
  <li><a href="../Getting-Started.html" class="page">Getting Started</a></li>
  <li><a href="../Builtin.html" class="page">Built-in Functionality</a></li>
  <li><a href="../Joins.html" class="page">Joins</a></li>
  <li><a href="../SideInputs.html" class="page">Side Inputs</a></li>
  <li><a href="../io/index.html" class="page">IO</a>
  <ul>
    <li><a href="../io/Avro.html" class="page">Avro</a></li>
    <li><a href="../io/Binary.html" class="page">Binary</a></li>
    <li><a href="../io/BigQuery.html" class="page">BigQuery</a></li>
    <li><a href="../io/Bigtable.html" class="page">Bigtable</a></li>
    <li><a href="../io/Cassandra.html" class="page">Cassandra</a></li>
    <li><a href="../io/Csv.html" class="page">CSV</a></li>
    <li><a href="../io/Datastore.html" class="page">Datastore</a></li>
    <li><a href="../io/Grpc.html" class="page">GRPC</a></li>
    <li><a href="../io/Elasticsearch.html" class="page">Elasticsearch</a></li>
    <li><a href="../io/Jdbc.html" class="page">JDBC</a></li>
    <li><a href="../io/Json.html" class="page">Json</a></li>
    <li><a href="../io/Neo4J.html" class="page">Neo4J</a></li>
    <li><a href="../io/Object.html" class="page">Object file</a></li>
    <li><a href="../io/Parquet.html" class="page">Parquet</a></li>
    <li><a href="../io/Protobuf.html" class="page">Protobuf</a></li>
    <li><a href="../io/Pubsub.html" class="page">PubSub</a></li>
    <li><a href="../io/ReadFiles.html" class="page">ReadFiles</a></li>
    <li><a href="../io/Redis.html" class="page">Redis</a></li>
    <li><a href="../io/Spanner.html" class="page">Spanner</a></li>
    <li><a href="../io/Tensorflow.html" class="page">Tensorflow</a></li>
    <li><a href="../io/Text.html" class="page">Text</a></li>
  </ul></li>
  <li><a href="../examples.html" class="page">Examples</a></li>
  <li><a href="../Scio-Unit-Tests.html" class="page">Testing</a></li>
  <li><a href="../internals/index.html" class="page">Internals</a>
  <ul>
    <li><a href="../internals/Coders.html" class="page">Coder Typeclass</a></li>
    <li><a href="../internals/Kryo.html" class="page">Kryo</a></li>
    <li><a href="../internals/OverrideTypeProvider.html" class="page">OverrideTypeProvider</a></li>
    <li><a href="../internals/ScioIO.html" class="page">ScioIO</a></li>
  </ul></li>
  <li><a href="../extras/index.html" class="page">Extras</a>
  <ul>
    <li><a href="../extras/Algebird.html" class="page">Algebird</a></li>
    <li><a href="../extras/Annoy.html" class="page">Annoy</a></li>
    <li><a href="../extras/AsyncDoFn.html" class="page">AsyncDoFn</a></li>
    <li><a href="../extras/BigQueryAvro.html" class="page">BigQueryAvro</a></li>
    <li><a href="../extras/DistCache.html" class="page">DistCache</a></li>
    <li><a href="../extras/Fanout.html" class="page">Fanout</a></li>
    <li><a href="../extras/HyperLogLog.html" class="page">HyperLogLog</a></li>
    <li><a href="../extras/MutableScalableBloomFilter.html" class="page">MutableScalableBloomFilter</a></li>
    <li><a href="../extras/Sorter.html" class="page">Sorter</a></li>
    <li><a href="../extras/Sort-Merge-Bucket.html" class="active page">Sort Merge Bucket</a></li>
    <li><a href="../extras/Sparkey.html" class="page">Sparkey</a></li>
    <li><a href="../extras/Scio-REPL.html" class="page">REPL</a></li>
    <li><a href="../extras/Transforms.html" class="page">Transforms</a></li>
    <li><a href="../extras/Voyager.html" class="page">Voyager</a></li>
  </ul></li>
  <li><a href="../dev/index.html" class="page">Development</a>
  <ul>
    <li><a href="../dev/build.html" class="page">Build</a></li>
    <li><a href="../dev/Style-Guide.html" class="page">Style Guide</a></li>
    <li><a href="../dev/How-to-Release.html" class="page">How to Release</a></li>
    <li><a href="../dev/Design-Philosophy.html" class="page">Design Philosophy</a></li>
    <li><a href="../dev/Scala-Steward.html" class="page">Scala Steward</a></li>
  </ul></li>
  <li><a href="../scaladoc.html" class="page">Scaladoc</a></li>
  <li><a href="../Scio,-Scalding-and-Spark.html" class="page">Scio, Spark and Scalding</a></li>
  <li><a href="../Runners.html" class="page">Runners</a></li>
  <li><a href="../Scio-data-guideline.html" class="page">Data Guidelines</a></li>
  <li><a href="../releases/index.html" class="page">Releases</a>
  <ul>
    <li><a href="../releases/Apache-Beam.html" class="page">Apache Beam Compatibility</a></li>
    <li><a href="../releases/migrations/index.html" class="page">Migration Guides</a></li>
    <li><a href="../releases/breaking-changes.html" class="page">Breaking Changelog</a></li>
    <li><a href="../releases/v0.12.0.html" class="page">v0.12.0 Release Blog</a></li>
  </ul></li>
  <li><a href="../FAQ.html" class="page">FAQ</a></li>
</ul>
<nav class="md-nav md-nav--secondary">
<label class="md-nav__title" for="__toc">Table of contents</label>
<ul>
  <li><a href="../extras/Sort-Merge-Bucket.html#sort-merge-bucket" class="header">Sort Merge Bucket</a>
  <ul>
    <li><a href="../extras/Sort-Merge-Bucket.html#what-are-smb-transforms-" class="header">What are SMB transforms?</a></li>
    <li><a href="../extras/Sort-Merge-Bucket.html#what-kind-of-data-can-i-write-using-smb-" class="header">What kind of data can I write using SMB?</a></li>
    <li><a href="../extras/Sort-Merge-Bucket.html#secondary-keys" class="header">Secondary keys</a></li>
    <li><a href="../extras/Sort-Merge-Bucket.html#null-keys" class="header">Null keys</a></li>
    <li><a href="../extras/Sort-Merge-Bucket.html#avro-string-keys" class="header">Avro String keys</a></li>
    <li><a href="../extras/Sort-Merge-Bucket.html#parquet" class="header">Parquet</a></li>
    <li><a href="../extras/Sort-Merge-Bucket.html#tuning-parameters-for-smb-transforms" class="header">Tuning parameters for SMB transforms</a></li>
    <li><a href="../extras/Sort-Merge-Bucket.html#testing" class="header">Testing</a></li>
  </ul></li>
</ul>
</nav>

</nav>
<ul style="display: none">
<li class="md-nav__item md-version" id="project.version">
<label class="md-nav__link" for="__version">
<i class="md-icon" title="Version">label_outline</i> 0.14.17-31-b648b4a-20250714T202047Z*
</label>
</li>
</ul>
</div>
</div>
</div>
<div class="md-sidebar md-sidebar--secondary" data-md-component="toc">
<div class="md-sidebar__scrollwrap">
<div class="md-sidebar__inner">
<nav class="md-nav md-nav--secondary">
<label class="md-nav__title" for="__toc">Table of contents</label>
<ul>
  <li><a href="../extras/Sort-Merge-Bucket.html#sort-merge-bucket" class="header">Sort Merge Bucket</a>
  <ul>
    <li><a href="../extras/Sort-Merge-Bucket.html#what-are-smb-transforms-" class="header">What are SMB transforms?</a></li>
    <li><a href="../extras/Sort-Merge-Bucket.html#what-kind-of-data-can-i-write-using-smb-" class="header">What kind of data can I write using SMB?</a></li>
    <li><a href="../extras/Sort-Merge-Bucket.html#secondary-keys" class="header">Secondary keys</a></li>
    <li><a href="../extras/Sort-Merge-Bucket.html#null-keys" class="header">Null keys</a></li>
    <li><a href="../extras/Sort-Merge-Bucket.html#avro-string-keys" class="header">Avro String keys</a></li>
    <li><a href="../extras/Sort-Merge-Bucket.html#parquet" class="header">Parquet</a></li>
    <li><a href="../extras/Sort-Merge-Bucket.html#tuning-parameters-for-smb-transforms" class="header">Tuning parameters for SMB transforms</a></li>
    <li><a href="../extras/Sort-Merge-Bucket.html#testing" class="header">Testing</a></li>
  </ul></li>
</ul>
</nav>

</div>
</div>
</div>
<div class="md-content">
<article class="md-content__inner md-typeset">
<div class="md-content__searchable">
<h1><a href="#sort-merge-bucket" name="sort-merge-bucket" class="anchor"><span class="anchor-link"></span></a>Sort Merge Bucket</h1>
<p>Sort Merge Bucket (SMB) is a technique for writing data to file system in deterministic file locations, sorted according to some pre-determined key, so that it can later be read in as key groups with no shuffle required. Since each element is assigned a file destination (bucket) based on a hash of its join key, we can use the same technique to cogroup multiple Sources as long as they&rsquo;re written using the same key and hashing scheme.</p>
<p>For example, given these input records, and SMB write will first extract the key, assign the record to a bucket, sort values within the bucket, and write these values to a corresponding file.</p>
<table>
  <thead>
    <tr>
      <th>Input </th>
      <th>Key </th>
      <th>Bucket </th>
      <th>File Assignment </th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>{key:&ldquo;b&rdquo;, value: 1} </td>
      <td>&ldquo;b&rdquo; </td>
      <td>0 </td>
      <td>bucket-00000-of-00002.avro </td>
    </tr>
    <tr>
      <td>{key:&ldquo;b&rdquo;, value: 2} </td>
      <td>&ldquo;b&rdquo; </td>
      <td>0 </td>
      <td>bucket-00000-of-00002.avro </td>
    </tr>
    <tr>
      <td>{key:&ldquo;a&rdquo;, value: 3} </td>
      <td>&ldquo;a&rdquo; </td>
      <td>1 </td>
      <td>bucket-00001-of-00002.avro </td>
    </tr>
  </tbody>
</table>
<p>Two sources can be joined by opening file readers on corresponding buckets of each source and merging key-groups as we go.</p>
<h2><a href="#what-are-smb-transforms-" name="what-are-smb-transforms-" class="anchor"><span class="anchor-link"></span></a>What are SMB transforms?</h2>
<p><code>scio-smb</code> provides three <a href="https://beam.apache.org/releases/javadoc/2.66.0/?org/apache/beam/sdk/transforms/PTransform.html" title="org.apache.beam.sdk.transforms.PTransform"><code>PTransform</code></a>s, as well as corresponding Scala API bindings, for SMB operations:</p>
<ul>
  <li>
    <p><a href="https://spotify.github.io/scio/api/org/apache/beam/sdk/extensions/smb/?org/apache/beam/sdk/extensions/smb/SortedBucketSink.html" title="org.apache.beam.sdk.extensions.smb.SortedBucketSink"><code>SortedBucketSink</code></a> writes data to file system in SMB format. Scala APIs (see: <a href="https://spotify.github.io/scio/api/com/spotify/scio/smb/syntax/SortedBucketSCollection.html" title="com.spotify.scio.smb.syntax.SortedBucketSCollection"><code>SortedBucketSCollection</code></a>):</p>
    <ul>
      <li>
      <p><code>SCollection[T: Coder]#saveAsSortedBucket</code></p></li>
    </ul>
    <pre class="prettyprint"><button class="snippet-button copy-snippet" title="Copy snippet to clipboard">copy</button><a class="snippet-button go-to-source" href="https://github.com/spotify/scio/tree/master/scio-examples/src/main/scala/com/spotify/scio/examples/extra/SortMergeBucketExample.scala#L103-L127" target="_blank" title="Go to snippet source">source</a><code class="language-scala">val accountWriteTap = sc
  .parallelize(250 until 750)
  .map { i =&gt;
    Account
      .newBuilder()
      .setId(i % 100)
      .setName(s&quot;name$i&quot;)
      .setType(s&quot;type${i % 5}&quot;)
      .setAmount(Random.nextDouble() * 1000)
      .build()
  }
  .saveAsSortedBucket(
    ParquetAvroSortedBucketIO
      .write[Integer, Account](classOf[Integer], &quot;id&quot;, classOf[Account])
      .to(args(&quot;accounts&quot;))
      .withSorterMemoryMb(128)
      .withTempDirectory(sc.options.getTempLocation)
      .withConfiguration(
        ParquetConfiguration.of(ParquetOutputFormat.BLOCK_SIZE -&gt; 512 * 1024 * 1024)
      )
      .withHashType(HashType.MURMUR3_32)
      .withFilenamePrefix(&quot;part&quot;) // Default is &quot;bucket&quot;
      .withNumBuckets(1)
      .withNumShards(1)
  )</code></pre>
    <p>Note the use of <code>Integer</code> for parameterized key type instead of a Scala <code>Int</code>. The key class must have a Coder available in the default Beam (Java) coder registry.</p>
    <p>Also note that the number of buckets specified must be a power of 2. This allows sources of different bucket sizes to still be joinable.</p>
  </li>
  <li>
    <p><a href="https://spotify.github.io/scio/api/org/apache/beam/sdk/extensions/smb/?org/apache/beam/sdk/extensions/smb/SortedBucketSource.html" title="org.apache.beam.sdk.extensions.smb.SortedBucketSource"><code>SortedBucketSource</code></a> reads data that has been written to file system using <code>SortedBucketSink</code> into a collection of <a href="https://beam.apache.org/releases/javadoc/2.66.0/?org/apache/beam/sdk/transforms/join/CoGbkResult.html" title="org.apache.beam.sdk.transforms.join.CoGbkResult"><code>CoGbkResult</code></a>s. Scala APIs (see: <a href="https://spotify.github.io/scio/api/com/spotify/scio/smb/syntax/SortedBucketScioContext.html" title="com.spotify.scio.smb.syntax.SortedBucketScioContext"><code>SortedBucketScioContext</code></a>):</p>
    <ul>
      <li><code>ScioContext#sortMergeGroupByKey</code> (1 source)</li>
      <li><code>ScioContext#sortMergeJoin</code> (2 sources)</li>
      <li><code>ScioContext#sortMergeCoGroup</code> (1-22 sources)</li>
    </ul>
    <p>Note that each <a href="https://beam.apache.org/releases/javadoc/2.66.0/?org/apache/beam/sdk/values/TupleTag.html" title="org.apache.beam.sdk.values.TupleTag"><code>TupleTag</code></a> used to create the <a href="https://spotify.github.io/scio/api/org/apache/beam/sdk/extensions/smb/?org/apache/beam/sdk/extensions/smb/SortedBucketIO.Read.html" title="org.apache.beam.sdk.extensions.smb.SortedBucketIO.Read"><code>SortedBucketIO.Read</code></a>s needs to have a unique Id.</p>
    <pre class="prettyprint"><button class="snippet-button copy-snippet" title="Copy snippet to clipboard">copy</button><a class="snippet-button go-to-source" href="https://github.com/spotify/scio/tree/master/scio-examples/src/main/scala/com/spotify/scio/examples/extra/SortMergeBucketExample.scala#L178-L205" target="_blank" title="Go to snippet source">source</a><code class="language-scala">sc.sortMergeJoin(
  classOf[Integer],
  ParquetAvroSortedBucketIO
    .read(new TupleTag[GenericRecord](&quot;users&quot;), SortMergeBucketExample.UserDataSchema)
    .withProjection(
      SchemaBuilder
        .record(&quot;UserProjection&quot;)
        .fields
        .requiredInt(&quot;userId&quot;)
        .requiredInt(&quot;age&quot;)
        .endRecord
    )
    // Filter at the Parquet IO level to users under 50
    // Filtering at the IO level whenever possible, as it reduces total bytes read
    .withFilterPredicate(FilterApi.lt(FilterApi.intColumn(&quot;age&quot;), Int.box(50)))
    // Filter at the SMB Cogrouping level to a single record per user
    // Filter at the Cogroup level if your filter depends on the materializing key group
    .withPredicate((xs, _) =&gt; xs.size() == 0)
    .from(args(&quot;users&quot;)),
  ParquetTypeSortedBucketIO
    .read(new TupleTag[AccountProjection](&quot;accounts&quot;))
    .from(args(&quot;accounts&quot;)),
  TargetParallelism.max()
).map { case (_, (userData, account)) =&gt;
  (userData.get(&quot;age&quot;).asInstanceOf[Int], account.amount)
}.groupByKey
  .mapValues(amounts =&gt; amounts.sum / amounts.size)
  .saveAsTextFile(args(&quot;output&quot;))</code></pre>
  </li>
  <li>
    <p><a href="https://spotify.github.io/scio/api/org/apache/beam/sdk/extensions/smb/?org/apache/beam/sdk/extensions/smb/SortedBucketTransform.html" title="org.apache.beam.sdk.extensions.smb.SortedBucketTransform"><code>SortedBucketTransform</code></a> reads data that has been written to file system using <code>SortedBucketSink</code>, transforms each <a href="https://beam.apache.org/releases/javadoc/2.66.0/?org/apache/beam/sdk/transforms/join/CoGbkResult.html" title="org.apache.beam.sdk.transforms.join.CoGbkResult"><code>CoGbkResult</code></a> using a user-supplied function, and immediately rewrites them using the same bucketing scheme. Scala APIs (see: <a href="https://spotify.github.io/scio/api/com/spotify/scio/smb/syntax/SortedBucketScioContext.html" title="com.spotify.scio.smb.syntax.SortedBucketScioContext"><code>SortedBucketScioContext</code></a>):</p>
    <ul>
      <li>
      <p><code>ScioContext#sortMergeTransform</code> (1-22 sources)</p></li>
    </ul>
    <p>Note that each <a href="https://beam.apache.org/releases/javadoc/2.66.0/?org/apache/beam/sdk/values/TupleTag.html" title="org.apache.beam.sdk.values.TupleTag"><code>TupleTag</code></a> used to create the <a href="https://spotify.github.io/scio/api/org/apache/beam/sdk/extensions/smb/?org/apache/beam/sdk/extensions/smb/SortedBucketIO.Read.html" title="org.apache.beam.sdk.extensions.smb.SortedBucketIO.Read"><code>SortedBucketIO.Read</code></a>s needs to have a unique Id.</p>
    <pre class="prettyprint"><button class="snippet-button copy-snippet" title="Copy snippet to clipboard">copy</button><a class="snippet-button go-to-source" href="https://github.com/spotify/scio/tree/master/scio-examples/src/main/scala/com/spotify/scio/examples/extra/SortMergeBucketExample.scala#L235-L257" target="_blank" title="Go to snippet source">source</a><code class="language-scala">sc.sortMergeTransform(
  classOf[Integer],
  ParquetAvroSortedBucketIO
    .read(new TupleTag[GenericRecord](&quot;users&quot;), SortMergeBucketExample.UserDataSchema)
    // Filter at the Parquet IO level to users under 50
    .withFilterPredicate(FilterApi.lt(FilterApi.intColumn(&quot;age&quot;), Int.box(50)))
    .from(args(&quot;users&quot;)),
  ParquetTypeSortedBucketIO
    .read(new TupleTag[AccountProjection](&quot;accounts&quot;))
    .from(args(&quot;accounts&quot;)),
  TargetParallelism.auto()
).to(
  ParquetTypeSortedBucketIO
    .transformOutput[Integer, CombinedAccount](&quot;id&quot;)
    .to(args(&quot;output&quot;))
).via { case (key, (users, accounts), outputCollector) =&gt;
  val sum = accounts.map(_.amount).sum
  users.foreach { user =&gt;
    outputCollector.accept(
      CombinedAccount(key, user.get(&quot;age&quot;).asInstanceOf[Integer], sum)
    )
  }
}</code></pre>
  </li>
</ul>
<h2><a href="#what-kind-of-data-can-i-write-using-smb-" name="what-kind-of-data-can-i-write-using-smb-" class="anchor"><span class="anchor-link"></span></a>What kind of data can I write using SMB?</h2>
<p>SMB writes are supported for multiple formats:</p>
<ul>
  <li>Avro (GenericRecord and SpecificRecord) when also depending on <code>scio-avro</code>.
    <ul>
      <li><a href="https://spotify.github.io/scio/api/org/apache/beam/sdk/extensions/smb/?org/apache/beam/sdk/extensions/smb/AvroSortedBucketIO.html" title="org.apache.beam.sdk.extensions.smb.AvroSortedBucketIO"><code>AvroSortedBucketIO</code></a></li>
    </ul>
  </li>
  <li>JSON
    <ul>
      <li><a href="https://spotify.github.io/scio/api/org/apache/beam/sdk/extensions/smb/?org/apache/beam/sdk/extensions/smb/JsonSortedBucketIO.html" title="org.apache.beam.sdk.extensions.smb.JsonSortedBucketIO"><code>JsonSortedBucketIO</code></a></li>
    </ul>
  </li>
  <li>Parquet when also depending on <code>scio-parquet</code>
    <ul>
      <li><a href="https://spotify.github.io/scio/api/org/apache/beam/sdk/extensions/smb/?org/apache/beam/sdk/extensions/smb/ParquetAvroSortedBucketIO.html" title="org.apache.beam.sdk.extensions.smb.ParquetAvroSortedBucketIO"><code>ParquetAvroSortedBucketIO</code></a></li>
      <li><a href="https://spotify.github.io/scio/api/org/apache/beam/sdk/extensions/smb/?org/apache/beam/sdk/extensions/smb/ParquetTypesSortedBucketIO.html" title="org.apache.beam.sdk.extensions.smb.ParquetTypesSortedBucketIO"><code>ParquetTypesSortedBucketIO</code></a></li>
    </ul>
  </li>
  <li>Tensorflow when also depending on <code>scio-tensorflow</code>
    <ul>
      <li><a href="https://spotify.github.io/scio/api/org/apache/beam/sdk/extensions/smb/?org/apache/beam/sdk/extensions/smb/TensorFlowBucketIO.html" title="org.apache.beam.sdk.extensions.smb.TensorFlowBucketIO"><code>TensorFlowBucketIO</code></a></li>
    </ul>
  </li>
</ul>
<h2><a href="#secondary-keys" name="secondary-keys" class="anchor"><span class="anchor-link"></span></a>Secondary keys</h2>
<p><em>Since Scio 0.12.0</em>.</p>
<p>A single key group may be very large and the implementation of SMB requires either handling the elements of the key group iteratively or loading the entire key group into memory. In the case where a secondary grouping or sorting is required, this can be prohibitive in terms of memory and/or wasteful when multiple downstream pipelines do the same grouping. For example, a SMB dataset might be keyed by <code>user_id</code> but many downstreams want to group by the tuple of <code>(user_id, artist_id)</code>.</p>
<p>Secondary SMB keys enable this use-case by sorting pipeline output by the hashed primary SMB key as described above, then additionally sorting the output for each key by the secondary SMB key. When key groups are read by a downstream pipeline it may read either the entire (primary) key group or the subset of elements belonging to the (primary key, secondary key) tuple.</p>
<p><em>A dataset may therefore add a secondary key and remain compatible with any downstream readers which expect only a primary key.</em></p>
<p>To write with a secondary key, the additional key class and path must be provided:</p>
<pre class="prettyprint"><button class="snippet-button copy-snippet" title="Copy snippet to clipboard">copy</button><a class="snippet-button go-to-source" href="https://github.com/spotify/scio/tree/master/scio-examples/src/main/scala/com/spotify/scio/examples/extra/SortMergeBucketExample.scala#L139-L151" target="_blank" title="Go to snippet source">source</a><code class="language-scala">.saveAsSortedBucket(
  ParquetAvroSortedBucketIO
    .write[Integer, String, Account](
      // primary key class and field
      classOf[Integer],
      &quot;id&quot;,
      // secondary key class and field
      classOf[String],
      &quot;type&quot;,
      classOf[Account]
    )
    .to(args(&quot;accounts&quot;))
)</code></pre>
<p>To read with a secondary key, the additional key class must be provided:</p>
<pre class="prettyprint"><button class="snippet-button copy-snippet" title="Copy snippet to clipboard">copy</button><a class="snippet-button go-to-source" href="https://github.com/spotify/scio/tree/master/scio-examples/src/main/scala/com/spotify/scio/examples/extra/SortMergeBucketExample.scala#L265-L273" target="_blank" title="Go to snippet source">source</a><code class="language-scala">sc.sortMergeGroupByKey(
  classOf[String], // primary key class
  classOf[String], // secondary key class
  ParquetAvroSortedBucketIO
    .read(new TupleTag[Account](&quot;account&quot;), classOf[Account])
    .from(args(&quot;accounts&quot;))
).map { case ((primaryKey, secondaryKey), elements) =&gt;
// ...
}</code></pre>
<p>Corresponding secondary-key-enabled variants of <code>sortMergeJoin</code>, <code>sortMergeCogroup</code>, and <code>sortMergeTransform</code> are also included.</p>
<h2><a href="#null-keys" name="null-keys" class="anchor"><span class="anchor-link"></span></a>Null keys</h2>
<p>If the key field of one or more PCollection elements is null, those elements will be diverted into a special bucket file, <code>bucket-null-keys.avro</code>. This file will be ignored in SMB reads and transforms and must be manually read by a downstream user.</p>
<h2><a href="#avro-string-keys" name="avro-string-keys" class="anchor"><span class="anchor-link"></span></a>Avro String keys</h2>
<p>As of <strong>Scio 0.14.0</strong>, Avro <code>CharSequence</code> are backed by <code>String</code> instead of default <code>Utf8</code>. With previous versions you may encounter the following when using Avro <code>CharSequence</code> keys:</p>
<pre class="prettyprint"><code class="language-bash">Cause: java.lang.ClassCastException: class org.apache.avro.util.Utf8 cannot be cast to class java.lang.String
[info]   at org.apache.beam.sdk.coders.StringUtf8Coder.encode(StringUtf8Coder.java:37)
[info]   at org.apache.beam.sdk.extensions.smb.BucketMetadata.encodeKeyBytes(BucketMetadata.java:222)
</code></pre>
<p>You&rsquo;ll have to either recompile your avro schema using <code>String</code> type, or add the <code>GenericData.StringType.String</code> property to your Avro schema with <a href="https://avro.apache.org/docs/1.11.1/api/java/org/apache/avro/generic/GenericData.html#setStringType-org.apache.avro.Schema-org.apache.avro.generic.GenericData.StringType-">setStringType</a></p>
<h2><a href="#parquet" name="parquet" class="anchor"><span class="anchor-link"></span></a>Parquet</h2>
<p>SMB supports Parquet reads and writes in both Avro and case class formats.</p>
<p>As of <strong>Scio 0.14.0</strong> and above, Scio supports specific record logical types in parquet-avro out of the box.</p>
<p>When using generic record, you have to manually supply a <em>data supplier</em> in your Parquet <code>Configuration</code> parameter. See <a href="../io/Parquet.html#logical-types">Logical Types in Parquet</a> for more information.</p>
<h2><a href="#tuning-parameters-for-smb-transforms" name="tuning-parameters-for-smb-transforms" class="anchor"><span class="anchor-link"></span></a>Tuning parameters for SMB transforms</h2>
<h3><a href="#numbuckets-numshards" name="numbuckets-numshards" class="anchor"><span class="anchor-link"></span></a>numBuckets/numShards</h3>
<p>SMB reads should be more performant and less resource-intensive than regular joins or groupBys. However, SMB writes are more expensive than their regular counterparts, since they involve an extra group-by (bucketing) and sorting step. Additionally, non-SMB writes (i.e. implementations of <a href="https://beam.apache.org/releases/javadoc/2.66.0/?org/apache/beam/sdk/io/FileBasedSink.html" title="org.apache.beam.sdk.io.FileBasedSink"><code>FileBasedSink</code></a>) use hints from the runner to determine an optimal number of output files. Unfortunately, SMB doesn&rsquo;t have access to those runtime hints; you must specify the number of buckets and shards as static values up front.</p>
<p>In SMB, <em>buckets</em> correspond to the hashed value of the SMB key % a given power of 2. A record with a given key will <em>always</em> be hashed into the same bucket. On the file system, buckets consist of one or more <em>sharded files</em> in which records are randomly assigned per-bundle. Two records with the same key may end up in different shard files within a bucket.</p>
<ul>
  <li>A good starting point is to look at your output data as it has been written by a non-SMB sink,  and pick the closest power of 2 as your initial <code>numBuckets</code> and set <code>numShards</code> to 1.</li>
  <li>If you anticipate having hot keys, try increasing <code>numShards</code> to randomly split data within a bucket.</li>
  <li><code>numBuckets</code> * <code>numShards</code> = total # of files written to disk.</li>
</ul>
<h3><a href="#sortermemorymb" name="sortermemorymb" class="anchor"><span class="anchor-link"></span></a>sorterMemoryMb</h3>
<p>If your job gets stuck in the sorting phase (since the <code>GroupByKey</code> and <code>SortValues</code> transforms  may get fused&ndash;you can reference the <a href="https://beam.apache.org/releases/javadoc/2.66.0/?org/apache/beam/sdk/metrics/Counter.html" title="org.apache.beam.sdk.metrics.Counter"><code>Counter</code></a>s  <code>SortedBucketSink-bucketsInitiatedSorting</code> and <code>SortedBucketSink-bucketsCompletedSorting</code>  to get an idea of where your job fails), you can increase sorter memory  (default is 1024MB, or 128MB for Scio &lt;= 0.9.0):</p>
<pre class="prettyprint"><code class="language-scala">data.saveAsSortedBucket(
  AvroSortedBucketIO
    .write[K, V](classOf[K], &quot;keyField&quot;, classOf[V])
    .to(...)
    .withSorterMemoryMb(256)
)
</code></pre>
<p>The amount of data each external sorter instance needs to handle is <code>total output size / numBuckets
/ numShards</code>, and when this exceeds sorter memory, the sorter will spill to disk. <code>n1-standard</code> workers has 3.75GB RAM per CPU, so 1GB sorter memory is a decent default, especially if the output files are kept under that size. If you have to spill to disk, note that worker disk IO depends on disk type, size, and worker number of CPUs.</p>
<p>See <a href="https://cloud.google.com/dataflow/docs/guides/specifying-exec-params">specifying pipeline execution parameters</a> for more details, e.g. <code>--workerMachineType</code>, <code>--workerDiskType</code>, and <code>--diskSizeGb</code>. Also read more about <a href="https://cloud.google.com/compute/docs/machine-types">machine types</a> and <a href="https://cloud.google.com/compute/docs/disks/performance">block storage performance</a></p>
<h3><a href="#parallelism" name="parallelism" class="anchor"><span class="anchor-link"></span></a>Parallelism</h3>
<p>The <code>SortedBucketSource</code> API accepts an optional <a href="https://spotify.github.io/scio/api/org/apache/beam/sdk/extensions/smb/?org/apache/beam/sdk/extensions/smb/TargetParallelism.html" title="org.apache.beam.sdk.extensions.smb.TargetParallelism"><code>TargetParallelism</code></a> parameter to set the desired parallelism of the SMB read operation. For a given set of sources, <code>targetParallelism</code> can be set to any number between the least and greatest numbers of buckets among sources. This can be dynamically configured using <code>TargetParallelism.min()</code> or <code>TargetParallelism.max()</code>, which at graph construction time will determine the least or greatest amount of parallelism based on sources.</p>
<p>Alternately, <code>TargetParallelism.of(Integer value)</code> can be used to statically configure a custom value, or <code>{@link TargetParallelism#auto()}</code> can be used to let the runner decide how to split the SMB read at runtime based on the combined byte size of the inputs&ndash;this is also the default behavior if <code>TargetPallelism</code> is left unspecified.</p>
<p>If no value is specified, SMB read operations will use Auto parallelism.</p>
<p>When selecting a target parallelism for your SMB operation, there are tradeoffs to consider:</p>
<ul>
  <li>Minimal parallelism means a fewer number of workers merging data from potentially many buckets. For example, if source A has 4 buckets and source B has 64, a minimally parallel SMB read would have 4 workers, each one merging 1 bucket from source A and 16 buckets from source B. This read may have low throughput.</li>
  <li>Maximal parallelism means that each bucket is read by at least one worker. For example, if source A has 4 buckets and source B has 64, a maximally parallel SMB read would have 64 workers, each one merging 1 bucket from source B and 1 bucket from source A, replicated 16 times. This may have better throughput than the minimal example, but more expensive because every key group from the replicated sources must be re-hashed to avoid emitting duplicate records.</li>
  <li>A custom parallelism in the middle of these bounds may be the best balance of speed and computing cost.</li>
  <li>Auto parallelism is more likely to pick an ideal value for most use cases. If its performance is worse than expected, you can look up the parallelism value it has computed and try a manual adjustment. Unfortunately, since it&rsquo;s determined at runtime, the computed parallelism value can&rsquo;t be added to the pipeline graph through <code>DisplayData</code>. Instead, you&rsquo;ll have to check the worker logs to find out which value was selected. When using Dataflow, you can do this in the UI by clicking on the SMB transform box, and searching the associated logs for the text <code>Parallelism was adjusted</code>. For example, in this case the value is 1024:
    <div style="text-align: center;"><img src="../images/smb_auto_parallelism_example.png" alt="Finding computed parallelism"
style="margin: 10px auto; width: 75%" /></div>
    <p>From there, you can try increasing or decreasing the parallelism by specifying a different <code>TargetParallelism</code> parameter to your SMB read. Often auto-parallelism will select a low value and using <code>TargetParallelism.max()</code> can help.</p>
  </li>
</ul>
<h3><a href="#read-buffering" name="read-buffering" class="anchor"><span class="anchor-link"></span></a>Read buffering</h3>
<p>Performance can suffer when reading an SMB source across many partitions if the total number of files (<code>numBuckets</code> * <code>numShards</code> * <code>numPartitions</code>) is too large (on the order of hundreds of thousands to millions of files). We&rsquo;ve observed errors and timeouts as a result of too many simultaneous filesystem connections. To that end, we&rsquo;ve added two <a href="https://beam.apache.org/releases/javadoc/2.66.0/?org/apache/beam/sdk/options/PipelineOptions.html" title="org.apache.beam.sdk.options.PipelineOptions"><code>PipelineOptions</code></a> to Scio 0.10.3, settable either via command-line args or using <a href="https://spotify.github.io/scio/api/org/apache/beam/sdk/extensions/smb/?org/apache/beam/sdk/extensions/smb/SortedBucketOptions.html" title="org.apache.beam.sdk.extensions.smb.SortedBucketOptions"><code>SortedBucketOptions</code></a> directly.</p>
<ul>
  <li><code>--sortedBucketReadBufferSize</code> (default: 10000): an Integer that determines the number of <em>elements</em> to read and buffer from <em>each file</em> at a time. For example, by default, each file will have 10,000 elements read and buffered into an in-memory array at worker startup. Then, the sort-merge algorithm will request them one at a time as needed. Once 10,000 elements have been requested, the file will buffer the next 10,000.
    <p><em>Note</em>: this can be quite memory-intensive and require bumping the worker memory. If you have a small number of files, or don&rsquo;t need this optimization, you can turn it off by setting <code>--sortedBucketReadBufferSize=0</code>.</p>
  </li>
  <li><code>--sortedBucketReadDiskBufferMb</code> (default: unset): an Integer that, if set, will force each worker to actually copy the specified # of megabytes from the remote filesystem into the worker&rsquo;s local temp directory, rather than streaming directly from FS. This caching is done eagerly: each worker will read as much as it can of each file in the order they&rsquo;re requested, and more space will be freed up once a file is fully read. Note that this is a <em>per worker limit</em>.</li>
</ul>
<h2><a href="#testing" name="testing" class="anchor"><span class="anchor-link"></span></a>Testing</h2>
<p>As of Scio 0.14, mocking data for SMB transforms is supported in the <code>com.spotify.scio.testing.JobTest</code> framework. Prior to Scio 0.14, you can test using real data written to local temp files.</p>
<h3><a href="#testing-smb-in-jobtest" name="testing-smb-in-jobtest" class="anchor"><span class="anchor-link"></span></a>Testing SMB in JobTest</h3>
<p>Scio 0.14 and above support testing SMB reads, writes, and transforms using <a href="http://spotify.github.com/scio/api/?com/spotify/scio/smb/SmbIO.html" title="com.spotify.scio.smb.SmbIO"><code>SmbIO</code></a>.</p>
<p>Consider the following sample job that contains an SMB read and write:</p>
<pre class="prettyprint"><code class="language-scala mdoc">import org.apache.beam.sdk.extensions.smb.ParquetAvroSortedBucketIO
import org.apache.beam.sdk.values.TupleTag
import com.spotify.scio._
import com.spotify.scio.avro.Account
import com.spotify.scio.values.SCollection
import com.spotify.scio.smb._

object SmbJob {
    def main(cmdLineArgs: Array[String]): Unit = {
        val (sc, args) = ContextAndArgs(cmdLineArgs)
        
        // Read
        sc.sortMergeGroupByKey(
            classOf[Integer],
            ParquetAvroSortedBucketIO
                .read(new TupleTag[Account](), classOf[Account])
                .from(args(&quot;input&quot;))
        )
        
        // Write
        val writeData: SCollection[Account] = ???
        writeData.saveAsSortedBucket(
            ParquetAvroSortedBucketIO
              .write(classOf[Integer], &quot;id&quot;, classOf[Account])
              .to(args(&quot;output&quot;))
        )
        
        sc.run().waitUntilDone()
    }
}
</code></pre>
<p>A JobTest can be wired in using <code>SmbIO</code> inputs and outputs. <code>SmbIO</code> is typed according to the record type and the SMB key type, and the SMB key function is required to construct it.</p>
<pre class="prettyprint"><code class="language-scala mdoc">import com.spotify.scio.smb.SmbIO
import com.spotify.scio.testing.PipelineSpec

class SmbJobTest extends PipelineSpec {
    &quot;SmbJob&quot; should &quot;work&quot; in {
        val smbInput: Seq[Account] = ???
        
        JobTest[SmbJob.type]
              .args(&quot;--input=gs://input&quot;, &quot;--output=gs://output&quot;)
             
              // Mock .sortMergeGroupByKey
              .input(SmbIO[Int, Account](&quot;gs://input&quot;, _.getId), smbInput)
              
              // Mock .saveAsSortedBucket
              .output(SmbIO[Int, Account](&quot;gs://output&quot;, _.getId)) { output =&gt;
                // Assert on output
              }
              .run()
    }
}
</code></pre>
<p>SMB Transforms can be mocked by combining input and output <code>SmbIO</code>s:</p>
<pre class="prettyprint"><code class="language-scala mdoc:compile-only">// Scio job
object SmbTransformJob {
    def main(cmdLineArgs: Array[String]): Unit = {
        val (sc, args) = ContextAndArgs(cmdLineArgs)
        sc.sortMergeTransform(
            classOf[Integer],
            ParquetAvroSortedBucketIO
                .read(new TupleTag[Account](), classOf[Account])
                .from(args(&quot;input&quot;))
        ).to(
            ParquetAvroSortedBucketIO
                .transformOutput[Integer, Account](classOf[Integer], &quot;id&quot;, classOf[Account])
                .to(args(&quot;output&quot;))
        ).via { case (key, grouped, outputCollector) =&gt;
          val output: Account = ???
          outputCollector.accept(output)
        }
        sc.run().waitUntilDone()
  }
}

// Job test
class SmbTransformJobTest extends PipelineSpec {
    &quot;SmbTransformJob&quot; should &quot;work&quot; in {
        val smbinput: Seq[Account] = ???
        
        JobTest[SmbTransformJob.type]
              .args(&quot;--input=gs://input&quot;, &quot;--output=gs://output&quot;)
             
              // Mock SMB Transform input
              .input(SmbIO[Int, Account](&quot;gs://input&quot;, _.getId), smbinput)
              
              // Mock SMB Transform output
              .output(SmbIO[Int, Account](&quot;gs://output&quot;, _.getId)) { output =&gt;
                // Assert on output
              }
              .run()
    }
}
</code></pre>
<p>See <a href="https://github.com/spotify/scio/tree/master/scio-examples/src/test/scala/com/spotify/scio/examples/extra/SortMergeBucketExampleTest.scala">SortMergeBucketExampleTest</a> for complete JobTest examples.</p>
<h3><a href="#testing-smb-using-local-file-system" name="testing-smb-using-local-file-system" class="anchor"><span class="anchor-link"></span></a>Testing SMB using local file system</h3>
<p>Using the JobTest framework for SMB reads, writes, and transforms is recommended, as it eliminates the need to manage local files and Taps. However, there are a few cases where performing real reads and writes is advantageous:</p>
<ul>
  <li>If you want to assert on SMB Predicates/Parquet FilterPredicates in reads, as these are skipped in JobTest</li>
  <li>If you want to assert on written metadata</li>
  <li>If you want to test schema evolution compatibility (i.e. writing using an updated record schema and reading using the original schema), or on projected schema compatability (i.e. using a case class projection to read Parquet data written with an Avro schema)</li>
</ul>
<p>Scio 0.14.0 and above automatically return Taps for SMB writes and transforms, and can materialize SMB reads into Taps:</p>
<pre class="prettyprint"><code class="language-scala mdoc">import com.spotify.scio.io.ClosedTap

// Scio job
object SmbRealFilesJob {
    def write(sc: ScioContext, output: String): ClosedTap[Account] = {
        val writeData: SCollection[Account] = ???
        writeData.saveAsSortedBucket(
            ParquetAvroSortedBucketIO
              .write(classOf[Integer], &quot;id&quot;, classOf[Account])
              .to(output)
        )
    }
    
    def read(sc: ScioContext, input: String): SCollection[(Integer, Iterable[Account])] = {
        sc.sortMergeGroupByKey(
            classOf[Integer],
            ParquetAvroSortedBucketIO
                .read(new TupleTag[Account](), classOf[Account])
                .from(input)
        )
    }
}

// Unit test
import java.nio.file.Files

class SmbLocalFilesTest extends PipelineSpec {
    &quot;SmbRealFilesJob&quot; should &quot;write and read data&quot; in {
        val dir = Files.createTempDirectory(&quot;smb&quot;).toString
        
        // Test write
        val (_, writtenData) = runWithOutput { sc =&gt;
            SmbRealFilesJob.write(sc, dir)
        }

        // Assert on actual written output
        writtenData.value should have size 100
        
        // Test read in separate ScioContext
        val (_, groupedData) = runWithLocalOutput { sc =&gt;
            SmbRealFilesJob.read(sc, dir)
        }

        // Assert on actual read result
        groupedData should have size 50
    }
}
</code></pre>
<p>In addition to JobTest examples, see <a href="https://github.com/spotify/scio/tree/master/scio-examples/src/test/scala/com/spotify/scio/examples/extra/SortMergeBucketExampleTest.scala">SortMergeBucketExampleTest</a> for complete SMB Tap examples.</p>
</div>
<div>
<a href="https://github.com/spotify/scio/tree/master/site/src/main/paradox/extras/Sort-Merge-Bucket.md" title="Edit this page" class="md-source-file md-edit">
Edit this page
</a>
</div>
<div class="print-only">
<span class="md-source-file md-version">
0.14.17-31-b648b4a-20250714T202047Z*
</span>
</div>
</article>
</div>
</div>
</main>
<footer class="md-footer">
<div class="md-footer-nav">
<nav class="md-footer-nav__inner md-grid">
<a href="../extras/Sorter.html" title="Sorter" class="md-flex md-footer-nav__link md-footer-nav__link--prev" rel="prev">
<div class="md-flex__cell md-flex__cell--shrink">
<i class="md-icon md-icon--arrow-back md-footer-nav__button"></i>
</div>
<div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
<span class="md-flex__ellipsis">
<span class="md-footer-nav__direction">
Previous
</span>
Sorter
</span>
</div>
</a>
<a href="../extras/Sparkey.html" title="Sparkey" class="md-flex md-footer-nav__link md-footer-nav__link--next" rel="next">
<div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
<span class="md-flex__ellipsis">
<span class="md-footer-nav__direction">
Next
</span>
Sparkey
</span>
</div>
<div class="md-flex__cell md-flex__cell--shrink">
<i class="md-icon md-icon--arrow-forward md-footer-nav__button"></i>
</div>
</a>
</nav>
</div>
<div class="md-footer-meta md-typeset">
<div class="md-footer-meta__inner md-grid">
<div class="md-footer-copyright">
<div class="md-footer-copyright__highlight">
Copyright (C) 2025 Spotify AB
</div>
Powered by
<a href="https://github.com/lightbend/paradox">Paradox</a>
and
<a href="https://sbt.github.io/sbt-paradox-material-theme/">Paradox Material Theme</a>

</div>
<div class="md-footer-social">
<a href="https://github.com/spotify" class="md-footer-social__link fa fa-github"></a><a href="https://twitter.com/spotifyeng" class="md-footer-social__link fa fa-twitter"></a>
</div>

</div>
</div>
</footer>

</div>
<script src="../assets/javascripts/application.583bbe55.js"></script>
<script src="../assets/javascripts/paradox-material-theme.js"></script>
<script>app.initialize({version:"0.17",url:{base:"../."}})</script>
<script type="text/javascript" src="../lib/prettify/prettify.js"></script>
<script type="text/javascript" src="../lib/prettify/lang-scala.js"></script>
<script type="text/javascript">
document.addEventListener("DOMContentLoaded", function(event) {
window.prettyPrint && prettyPrint();
});
</script>
</body>
</html>
